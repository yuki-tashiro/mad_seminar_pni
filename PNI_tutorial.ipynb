{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","machine_shape":"hm","authorship_tag":"ABX9TyM37p11edvtSe9Xggll7VwL"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["**Here is a quick guide of PNI model for fast MRI datase.**\n","\n","\n","PNI : Industrial Anomaly Detection using Position and Neighborhood Information\n","\n","\n","*   https://github.com/wogur110/PNI_anomaly_detection/tree/main\n","*   https://arxiv.org/abs/2211.12634\n","\n","\n"],"metadata":{"id":"DK9yMatkN05p"}},{"cell_type":"markdown","source":["# Code in original PNI"],"metadata":{"id":"Q3uXbOAMFpxQ"}},{"cell_type":"code","source":["# !conda create -y -n anomaly_env\n","# !conda activate anomaly_env"],"metadata":{"id":"9sJAGHO7N0wE"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8qO9OIoQNwiv"},"outputs":[],"source":["# !conda install -y python=3.8\n","# !pip install pytorch-lightning==1.5.9\n","# !pip install pillow==9.0\n","# !pip install faiss-gpu==1.7\n","# !pip install opencv-python==4.5\n","# !pip install scikit-learn==0.24\n","# !pip install scikit-image==0.19\n","# !pip install pymp-pypi==0.5\n","# !pip install numpngw==0.1\n","# !pip install matplotlib==3.7\n","# !pip install timm==0.9"]},{"cell_type":"markdown","source":["3min"],"metadata":{"id":"_WlNNYz-Qmwo"}},{"cell_type":"code","source":["# !pip install torch==1.12.1+cu116 torchvision==0.13.1+cu116 -f https://download.pytorch.org/whl/torch_stable.html"],"metadata":{"id":"6KvxkSQMOtd8"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# !git clone https://github.com/wogur110/PNI_anomaly_detection.git"],"metadata":{"id":"39TmWxCKS8x5"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# %cd PNI_anomaly_detection"],"metadata":{"id":"fFMaf0FhTjgU"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# %ls"],"metadata":{"id":"BwJ3ZfiHfqvO"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# PNI tutorial for FastMRI"],"metadata":{"id":"HbVZQH41FwHz"}},{"cell_type":"markdown","source":["## environment setup"],"metadata":{"id":"JuiHRoRK3F-A"}},{"cell_type":"code","source":["!apt-get update -y\n","!apt-get install python3.8 python3.8-distutils\n","!update-alternatives --install /usr/bin/python3 python3 /usr/bin/python3.8 1\n","!update-alternatives --config python3\n","!apt-get install python3-pip\n","!python3 -m pip install --upgrade pip --user"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7NECl7L7IOR-","executionInfo":{"status":"ok","timestamp":1706572439176,"user_tz":-60,"elapsed":24041,"user":{"displayName":"田代勇希","userId":"12989083184923368445"}},"outputId":"11e733b6-f283-4a80-95b8-19337fba5687"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\r0% [Working]\r            \rGet:1 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease [3,626 B]\n","Get:2 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease [1,581 B]\n","Get:3 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  Packages [665 kB]\n","Get:4 http://security.ubuntu.com/ubuntu jammy-security InRelease [110 kB]\n","Hit:5 http://archive.ubuntu.com/ubuntu jammy InRelease\n","Get:6 http://archive.ubuntu.com/ubuntu jammy-updates InRelease [119 kB]\n","Hit:7 https://ppa.launchpadcontent.net/c2d4u.team/c2d4u4.0+/ubuntu jammy InRelease\n","Hit:8 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\n","Get:9 http://security.ubuntu.com/ubuntu jammy-security/main amd64 Packages [1,398 kB]\n","Hit:10 http://archive.ubuntu.com/ubuntu jammy-backports InRelease\n","Get:11 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease [24.3 kB]\n","Get:12 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease [23.8 kB]\n","Get:13 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 Packages [1,677 kB]\n","Get:14 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy/main amd64 Packages [44.4 kB]\n","Get:15 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy/main amd64 Packages [66.2 kB]\n","Get:16 http://security.ubuntu.com/ubuntu jammy-security/restricted amd64 Packages [1,685 kB]\n","Get:17 http://security.ubuntu.com/ubuntu jammy-security/universe amd64 Packages [1,060 kB]\n","Get:18 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 Packages [1,326 kB]\n","Get:19 http://archive.ubuntu.com/ubuntu jammy-updates/restricted amd64 Packages [1,722 kB]\n","Fetched 9,927 kB in 3s (3,152 kB/s)\n","Reading package lists... Done\n","Reading package lists... Done\n","Building dependency tree... Done\n","Reading state information... Done\n","The following additional packages will be installed:\n","  libpython3.8-minimal libpython3.8-stdlib mailcap mime-support python3.8-lib2to3 python3.8-minimal\n","Suggested packages:\n","  python3.8-venv binfmt-support\n","The following NEW packages will be installed:\n","  libpython3.8-minimal libpython3.8-stdlib mailcap mime-support python3.8 python3.8-distutils\n","  python3.8-lib2to3 python3.8-minimal\n","0 upgraded, 8 newly installed, 0 to remove and 83 not upgraded.\n","Need to get 5,417 kB of archives.\n","After this operation, 20.2 MB of additional disk space will be used.\n","Get:1 http://archive.ubuntu.com/ubuntu jammy/main amd64 mailcap all 3.70+nmu1ubuntu1 [23.8 kB]\n","Get:2 http://archive.ubuntu.com/ubuntu jammy/main amd64 mime-support all 3.66 [3,696 B]\n","Get:3 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy/main amd64 libpython3.8-minimal amd64 3.8.18-1+jammy1 [794 kB]\n","Get:4 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy/main amd64 python3.8-minimal amd64 3.8.18-1+jammy1 [2,024 kB]\n","Get:5 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy/main amd64 libpython3.8-stdlib amd64 3.8.18-1+jammy1 [1,815 kB]\n","Get:6 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy/main amd64 python3.8 amd64 3.8.18-1+jammy1 [438 kB]\n","Get:7 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy/main amd64 python3.8-lib2to3 all 3.8.18-1+jammy1 [126 kB]\n","Get:8 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy/main amd64 python3.8-distutils all 3.8.18-1+jammy1 [193 kB]\n","Fetched 5,417 kB in 3s (2,086 kB/s)\n","Selecting previously unselected package libpython3.8-minimal:amd64.\n","(Reading database ... 121671 files and directories currently installed.)\n","Preparing to unpack .../0-libpython3.8-minimal_3.8.18-1+jammy1_amd64.deb ...\n","Unpacking libpython3.8-minimal:amd64 (3.8.18-1+jammy1) ...\n","Selecting previously unselected package python3.8-minimal.\n","Preparing to unpack .../1-python3.8-minimal_3.8.18-1+jammy1_amd64.deb ...\n","Unpacking python3.8-minimal (3.8.18-1+jammy1) ...\n","Selecting previously unselected package mailcap.\n","Preparing to unpack .../2-mailcap_3.70+nmu1ubuntu1_all.deb ...\n","Unpacking mailcap (3.70+nmu1ubuntu1) ...\n","Selecting previously unselected package mime-support.\n","Preparing to unpack .../3-mime-support_3.66_all.deb ...\n","Unpacking mime-support (3.66) ...\n","Selecting previously unselected package libpython3.8-stdlib:amd64.\n","Preparing to unpack .../4-libpython3.8-stdlib_3.8.18-1+jammy1_amd64.deb ...\n","Unpacking libpython3.8-stdlib:amd64 (3.8.18-1+jammy1) ...\n","Selecting previously unselected package python3.8.\n","Preparing to unpack .../5-python3.8_3.8.18-1+jammy1_amd64.deb ...\n","Unpacking python3.8 (3.8.18-1+jammy1) ...\n","Selecting previously unselected package python3.8-lib2to3.\n","Preparing to unpack .../6-python3.8-lib2to3_3.8.18-1+jammy1_all.deb ...\n","Unpacking python3.8-lib2to3 (3.8.18-1+jammy1) ...\n","Selecting previously unselected package python3.8-distutils.\n","Preparing to unpack .../7-python3.8-distutils_3.8.18-1+jammy1_all.deb ...\n","Unpacking python3.8-distutils (3.8.18-1+jammy1) ...\n","Setting up libpython3.8-minimal:amd64 (3.8.18-1+jammy1) ...\n","Setting up python3.8-lib2to3 (3.8.18-1+jammy1) ...\n","Setting up python3.8-minimal (3.8.18-1+jammy1) ...\n","Setting up python3.8-distutils (3.8.18-1+jammy1) ...\n","Setting up mailcap (3.70+nmu1ubuntu1) ...\n","Setting up mime-support (3.66) ...\n","Setting up libpython3.8-stdlib:amd64 (3.8.18-1+jammy1) ...\n","Setting up python3.8 (3.8.18-1+jammy1) ...\n","Processing triggers for man-db (2.10.2-1) ...\n","update-alternatives: using /usr/bin/python3.8 to provide /usr/bin/python3 (python3) in auto mode\n","There is only one alternative in link group python3 (providing /usr/bin/python3): /usr/bin/python3.8\n","Nothing to configure.\n","Reading package lists... Done\n","Building dependency tree... Done\n","Reading state information... Done\n","The following additional packages will be installed:\n","  python3-setuptools python3-wheel\n","Suggested packages:\n","  python-setuptools-doc\n","The following NEW packages will be installed:\n","  python3-pip python3-setuptools python3-wheel\n","0 upgraded, 3 newly installed, 0 to remove and 83 not upgraded.\n","Need to get 1,677 kB of archives.\n","After this operation, 8,967 kB of additional disk space will be used.\n","Get:1 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 python3-setuptools all 59.6.0-1.2ubuntu0.22.04.1 [339 kB]\n","Get:2 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 python3-wheel all 0.37.1-2ubuntu0.22.04.1 [32.0 kB]\n","Get:3 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 python3-pip all 22.0.2+dfsg-1ubuntu0.4 [1,305 kB]\n","Fetched 1,677 kB in 2s (802 kB/s)\n","Selecting previously unselected package python3-setuptools.\n","(Reading database ... 122461 files and directories currently installed.)\n","Preparing to unpack .../python3-setuptools_59.6.0-1.2ubuntu0.22.04.1_all.deb ...\n","Unpacking python3-setuptools (59.6.0-1.2ubuntu0.22.04.1) ...\n","Selecting previously unselected package python3-wheel.\n","Preparing to unpack .../python3-wheel_0.37.1-2ubuntu0.22.04.1_all.deb ...\n","Unpacking python3-wheel (0.37.1-2ubuntu0.22.04.1) ...\n","Selecting previously unselected package python3-pip.\n","Preparing to unpack .../python3-pip_22.0.2+dfsg-1ubuntu0.4_all.deb ...\n","Unpacking python3-pip (22.0.2+dfsg-1ubuntu0.4) ...\n","Setting up python3-setuptools (59.6.0-1.2ubuntu0.22.04.1) ...\n","Setting up python3-wheel (0.37.1-2ubuntu0.22.04.1) ...\n","Setting up python3-pip (22.0.2+dfsg-1ubuntu0.4) ...\n","Processing triggers for man-db (2.10.2-1) ...\n","Requirement already satisfied: pip in /usr/lib/python3/dist-packages (22.0.2)\n","Collecting pip\n","  Downloading pip-23.3.2-py3-none-any.whl (2.1 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m10.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: pip\n","\u001b[33m  WARNING: The scripts pip, pip3, pip3.11 and pip3.8 are installed in '/root/.local/bin' which is not on PATH.\n","  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n","\u001b[0mSuccessfully installed pip-23.3.2\n","\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n","\u001b[0m"]}]},{"cell_type":"code","source":["# !sudo apt install python3.8\n","# !sudo update-alternatives --install /usr/bin/python3 python3 /usr/bin/python3.8 1"],"metadata":{"id":"x46APYFpiBmg"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!python --version"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"01ELJD_EiGMM","executionInfo":{"status":"ok","timestamp":1706572439176,"user_tz":-60,"elapsed":7,"user":{"displayName":"田代勇希","userId":"12989083184923368445"}},"outputId":"2e409ed1-f6e3-4ec8-bcd9-aa42c3552c5c"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Python 3.8.18\n"]}]},{"cell_type":"code","source":["# !pip install torch==1.7.1+cu110 torchvision==0.8.2+cu110 torchaudio==0.7.2 -f https://download.pytorch.org/whl/torch_stable.html\n","# !pip install torch==1.12.1+cu116 torchvision==0.13.1+cu116 -f https://download.pytorch.org/whl/torch_stable.html"],"metadata":{"id":"fr60te6FjlZF"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!python3 -V\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"VbIOd_n6gl5d","executionInfo":{"status":"ok","timestamp":1706572439177,"user_tz":-60,"elapsed":7,"user":{"displayName":"田代勇希","userId":"12989083184923368445"}},"outputId":"d4d3bb45-1216-4900-9b72-09c80b00ee5d"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Python 3.8.18\n"]}]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"zUoMFwX1Fv4-","executionInfo":{"status":"ok","timestamp":1706572463254,"user_tz":-60,"elapsed":24083,"user":{"displayName":"田代勇希","userId":"12989083184923368445"}},"outputId":"8ca32b01-c57c-4ae6-d22c-302c3dd75e47"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","source":["%cd /directory_path/PNI_anomaly_detection-main"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"rVOFiqAOGqYU","executionInfo":{"status":"ok","timestamp":1706572467970,"user_tz":-60,"elapsed":4721,"user":{"displayName":"田代勇希","userId":"12989083184923368445"}},"outputId":"135080f7-9d8f-46b3-e287-3191b8bb10c0"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/MyDrive/TUM/Unsupervised_Anomaly_Detection/PNI_anomaly_detection-main\n"]}]},{"cell_type":"code","source":["%ls"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"8aSNNqNIGwBc","executionInfo":{"status":"ok","timestamp":1706572467971,"user_tz":-60,"elapsed":7,"user":{"displayName":"田代勇希","userId":"12989083184923368445"}},"outputId":"18e95ea5-0c12-4ce3-810c-ce899c95a7c4"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[0m\u001b[01;34manalysis_code\u001b[0m/                                          \u001b[01;34mrefinement\u001b[0m/\n","\u001b[01;34membeddings_layer2+layer3\u001b[0m/                               \u001b[01;34mresult\u001b[0m/\n","\u001b[01;34membeddings_layer2+layer3_01+02\u001b[0m/                         \u001b[01;34mresult_01\u001b[0m/\n","\u001b[01;34membeddings_layer2+layer3_04のやつ\u001b[0m/                      \u001b[01;34mresult_02\u001b[0m/\n","evaluate_amap_on_MVTecAD_bottle_ensemble_pretrained.sh  \u001b[01;34mresult_03\u001b[0m/\n","evaluate_amap_on_MVTecAD_bottle_refinement.sh           \u001b[01;34mresult_04\u001b[0m/\n","evaluate_amap_on_MVTecAD_ensemble.sh                    \u001b[01;34mresult_05_FirstTrueTry\u001b[0m/\n","evaluate_amap_on_MVTecAD_WR101.sh                       train_coreset_distribution.py\n","pixelwise_refinement.py                                 \u001b[01;34mutils\u001b[0m/\n","README.md\n"]}]},{"cell_type":"code","source":["# !chmod +x ./evaluate_amap_on_MVTecAD_WR101.sh"],"metadata":{"id":"c27tQFZ4Ou7h"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!pip install pytorch-lightning==1.5.9\n","!pip install pillow==9.0\n","!pip install faiss-gpu==1.7\n","!pip install opencv-python==4.5\n","!pip install scikit-learn==0.24\n","!pip install scikit-image==0.19\n","!pip install pymp-pypi==0.5\n","!pip install numpngw==0.1\n","!pip install matplotlib==3.7\n","!pip install timm==0.9"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"1xQrZaH_gE4G","executionInfo":{"status":"ok","timestamp":1706572676468,"user_tz":-60,"elapsed":208501,"user":{"displayName":"田代勇希","userId":"12989083184923368445"}},"outputId":"15b32a99-2ca9-4c43-cd99-ce85ba1982d7"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting pytorch-lightning==1.5.9\n","  Downloading pytorch_lightning-1.5.9-py3-none-any.whl (527 kB)\n","\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/527.2 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m143.4/527.2 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m522.2/527.2 kB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m527.2/527.2 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting numpy>=1.17.2 (from pytorch-lightning==1.5.9)\n","  Downloading numpy-1.24.4-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.6 kB)\n","Collecting torch>=1.7.* (from pytorch-lightning==1.5.9)\n","  Downloading torch-2.1.2-cp38-cp38-manylinux1_x86_64.whl.metadata (25 kB)\n","Collecting future>=0.17.1 (from pytorch-lightning==1.5.9)\n","  Downloading future-0.18.3.tar.gz (840 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m840.9/840.9 kB\u001b[0m \u001b[31m11.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Collecting tqdm>=4.41.0 (from pytorch-lightning==1.5.9)\n","  Downloading tqdm-4.66.1-py3-none-any.whl.metadata (57 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.6/57.6 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting PyYAML>=5.1 (from pytorch-lightning==1.5.9)\n","  Downloading PyYAML-6.0.1-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.1 kB)\n","Collecting fsspec!=2021.06.0,>=2021.05.0 (from fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning==1.5.9)\n","  Downloading fsspec-2023.12.2-py3-none-any.whl.metadata (6.8 kB)\n","Collecting tensorboard>=2.2.0 (from pytorch-lightning==1.5.9)\n","  Downloading tensorboard-2.14.0-py3-none-any.whl.metadata (1.8 kB)\n","Collecting torchmetrics>=0.4.1 (from pytorch-lightning==1.5.9)\n","  Downloading torchmetrics-1.3.0.post0-py3-none-any.whl.metadata (20 kB)\n","Collecting pyDeprecate==0.3.1 (from pytorch-lightning==1.5.9)\n","  Downloading pyDeprecate-0.3.1-py3-none-any.whl (10 kB)\n","Collecting packaging>=17.0 (from pytorch-lightning==1.5.9)\n","  Downloading packaging-23.2-py3-none-any.whl.metadata (3.2 kB)\n","Collecting typing-extensions (from pytorch-lightning==1.5.9)\n","  Downloading typing_extensions-4.9.0-py3-none-any.whl.metadata (3.0 kB)\n","Collecting setuptools==59.5.0 (from pytorch-lightning==1.5.9)\n","  Downloading setuptools-59.5.0-py3-none-any.whl (952 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m952.4/952.4 kB\u001b[0m \u001b[31m14.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting requests (from fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning==1.5.9)\n","  Downloading requests-2.31.0-py3-none-any.whl.metadata (4.6 kB)\n","Collecting aiohttp!=4.0.0a0,!=4.0.0a1 (from fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning==1.5.9)\n","  Downloading aiohttp-3.9.3-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.4 kB)\n","Collecting absl-py>=0.4 (from tensorboard>=2.2.0->pytorch-lightning==1.5.9)\n","  Downloading absl_py-2.1.0-py3-none-any.whl.metadata (2.3 kB)\n","Collecting grpcio>=1.48.2 (from tensorboard>=2.2.0->pytorch-lightning==1.5.9)\n","  Downloading grpcio-1.60.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.0 kB)\n","Collecting google-auth<3,>=1.6.3 (from tensorboard>=2.2.0->pytorch-lightning==1.5.9)\n","  Downloading google_auth-2.27.0-py2.py3-none-any.whl.metadata (4.7 kB)\n","Collecting google-auth-oauthlib<1.1,>=0.5 (from tensorboard>=2.2.0->pytorch-lightning==1.5.9)\n","  Downloading google_auth_oauthlib-1.0.0-py2.py3-none-any.whl (18 kB)\n","Collecting markdown>=2.6.8 (from tensorboard>=2.2.0->pytorch-lightning==1.5.9)\n","  Downloading Markdown-3.5.2-py3-none-any.whl.metadata (7.0 kB)\n","Collecting protobuf>=3.19.6 (from tensorboard>=2.2.0->pytorch-lightning==1.5.9)\n","  Downloading protobuf-4.25.2-cp37-abi3-manylinux2014_x86_64.whl.metadata (541 bytes)\n","Collecting tensorboard-data-server<0.8.0,>=0.7.0 (from tensorboard>=2.2.0->pytorch-lightning==1.5.9)\n","  Downloading tensorboard_data_server-0.7.2-py3-none-manylinux_2_31_x86_64.whl.metadata (1.1 kB)\n","Collecting werkzeug>=1.0.1 (from tensorboard>=2.2.0->pytorch-lightning==1.5.9)\n","  Downloading werkzeug-3.0.1-py3-none-any.whl.metadata (4.1 kB)\n","Requirement already satisfied: wheel>=0.26 in /usr/lib/python3/dist-packages (from tensorboard>=2.2.0->pytorch-lightning==1.5.9) (0.37.1)\n","Collecting filelock (from torch>=1.7.*->pytorch-lightning==1.5.9)\n","  Downloading filelock-3.13.1-py3-none-any.whl.metadata (2.8 kB)\n","Collecting sympy (from torch>=1.7.*->pytorch-lightning==1.5.9)\n","  Downloading sympy-1.12-py3-none-any.whl (5.7 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.7/5.7 MB\u001b[0m \u001b[31m24.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting networkx (from torch>=1.7.*->pytorch-lightning==1.5.9)\n","  Downloading networkx-3.1-py3-none-any.whl (2.1 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m32.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting jinja2 (from torch>=1.7.*->pytorch-lightning==1.5.9)\n","  Downloading Jinja2-3.1.3-py3-none-any.whl.metadata (3.3 kB)\n","Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch>=1.7.*->pytorch-lightning==1.5.9)\n","  Downloading nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.7/23.7 MB\u001b[0m \u001b[31m63.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting nvidia-cuda-runtime-cu12==12.1.105 (from torch>=1.7.*->pytorch-lightning==1.5.9)\n","  Downloading nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m823.6/823.6 kB\u001b[0m \u001b[31m50.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting nvidia-cuda-cupti-cu12==12.1.105 (from torch>=1.7.*->pytorch-lightning==1.5.9)\n","  Downloading nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.1/14.1 MB\u001b[0m \u001b[31m95.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting nvidia-cudnn-cu12==8.9.2.26 (from torch>=1.7.*->pytorch-lightning==1.5.9)\n","  Downloading nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n","Collecting nvidia-cublas-cu12==12.1.3.1 (from torch>=1.7.*->pytorch-lightning==1.5.9)\n","  Downloading nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m410.6/410.6 MB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting nvidia-cufft-cu12==11.0.2.54 (from torch>=1.7.*->pytorch-lightning==1.5.9)\n","  Downloading nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.6/121.6 MB\u001b[0m \u001b[31m17.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting nvidia-curand-cu12==10.3.2.106 (from torch>=1.7.*->pytorch-lightning==1.5.9)\n","  Downloading nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.5/56.5 MB\u001b[0m \u001b[31m37.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting nvidia-cusolver-cu12==11.4.5.107 (from torch>=1.7.*->pytorch-lightning==1.5.9)\n","  Downloading nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.2/124.2 MB\u001b[0m \u001b[31m9.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting nvidia-cusparse-cu12==12.1.0.106 (from torch>=1.7.*->pytorch-lightning==1.5.9)\n","  Downloading nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m196.0/196.0 MB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting nvidia-nccl-cu12==2.18.1 (from torch>=1.7.*->pytorch-lightning==1.5.9)\n","  Downloading nvidia_nccl_cu12-2.18.1-py3-none-manylinux1_x86_64.whl (209.8 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m209.8/209.8 MB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting nvidia-nvtx-cu12==12.1.105 (from torch>=1.7.*->pytorch-lightning==1.5.9)\n","  Downloading nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.1/99.1 kB\u001b[0m \u001b[31m9.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting triton==2.1.0 (from torch>=1.7.*->pytorch-lightning==1.5.9)\n","  Downloading triton-2.1.0-0-cp38-cp38-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.3 kB)\n","Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.7.*->pytorch-lightning==1.5.9)\n","  Downloading nvidia_nvjitlink_cu12-12.3.101-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n","Collecting lightning-utilities>=0.8.0 (from torchmetrics>=0.4.1->pytorch-lightning==1.5.9)\n","  Downloading lightning_utilities-0.10.1-py3-none-any.whl.metadata (4.8 kB)\n","Collecting aiosignal>=1.1.2 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning==1.5.9)\n","  Downloading aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\n","Collecting attrs>=17.3.0 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning==1.5.9)\n","  Downloading attrs-23.2.0-py3-none-any.whl.metadata (9.5 kB)\n","Collecting frozenlist>=1.1.1 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning==1.5.9)\n","  Downloading frozenlist-1.4.1-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n","Collecting multidict<7.0,>=4.5 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning==1.5.9)\n","  Downloading multidict-6.0.4-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (121 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.3/121.3 kB\u001b[0m \u001b[31m10.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting yarl<2.0,>=1.0 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning==1.5.9)\n","  Downloading yarl-1.9.4-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (31 kB)\n","Collecting async-timeout<5.0,>=4.0 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning==1.5.9)\n","  Downloading async_timeout-4.0.3-py3-none-any.whl.metadata (4.2 kB)\n","Collecting cachetools<6.0,>=2.0.0 (from google-auth<3,>=1.6.3->tensorboard>=2.2.0->pytorch-lightning==1.5.9)\n","  Downloading cachetools-5.3.2-py3-none-any.whl.metadata (5.2 kB)\n","Collecting pyasn1-modules>=0.2.1 (from google-auth<3,>=1.6.3->tensorboard>=2.2.0->pytorch-lightning==1.5.9)\n","  Downloading pyasn1_modules-0.3.0-py2.py3-none-any.whl (181 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m181.3/181.3 kB\u001b[0m \u001b[31m15.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting rsa<5,>=3.1.4 (from google-auth<3,>=1.6.3->tensorboard>=2.2.0->pytorch-lightning==1.5.9)\n","  Downloading rsa-4.9-py3-none-any.whl (34 kB)\n","Collecting requests-oauthlib>=0.7.0 (from google-auth-oauthlib<1.1,>=0.5->tensorboard>=2.2.0->pytorch-lightning==1.5.9)\n","  Downloading requests_oauthlib-1.3.1-py2.py3-none-any.whl (23 kB)\n","Collecting importlib-metadata>=4.4 (from markdown>=2.6.8->tensorboard>=2.2.0->pytorch-lightning==1.5.9)\n","  Downloading importlib_metadata-7.0.1-py3-none-any.whl.metadata (4.9 kB)\n","Collecting charset-normalizer<4,>=2 (from requests->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning==1.5.9)\n","  Downloading charset_normalizer-3.3.2-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (33 kB)\n","Collecting idna<4,>=2.5 (from requests->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning==1.5.9)\n","  Downloading idna-3.6-py3-none-any.whl.metadata (9.9 kB)\n","Collecting urllib3<3,>=1.21.1 (from requests->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning==1.5.9)\n","  Downloading urllib3-2.1.0-py3-none-any.whl.metadata (6.4 kB)\n","Collecting certifi>=2017.4.17 (from requests->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning==1.5.9)\n","  Downloading certifi-2023.11.17-py3-none-any.whl.metadata (2.2 kB)\n","Collecting MarkupSafe>=2.1.1 (from werkzeug>=1.0.1->tensorboard>=2.2.0->pytorch-lightning==1.5.9)\n","  Downloading MarkupSafe-2.1.4-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.0 kB)\n","Collecting mpmath>=0.19 (from sympy->torch>=1.7.*->pytorch-lightning==1.5.9)\n","  Downloading mpmath-1.3.0-py3-none-any.whl (536 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m536.2/536.2 kB\u001b[0m \u001b[31m38.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting zipp>=0.5 (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard>=2.2.0->pytorch-lightning==1.5.9)\n","  Downloading zipp-3.17.0-py3-none-any.whl.metadata (3.7 kB)\n","Collecting pyasn1<0.6.0,>=0.4.6 (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard>=2.2.0->pytorch-lightning==1.5.9)\n","  Downloading pyasn1-0.5.1-py2.py3-none-any.whl.metadata (8.6 kB)\n","Collecting oauthlib>=3.0.0 (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard>=2.2.0->pytorch-lightning==1.5.9)\n","  Downloading oauthlib-3.2.2-py3-none-any.whl (151 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m151.7/151.7 kB\u001b[0m \u001b[31m14.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading fsspec-2023.12.2-py3-none-any.whl (168 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m169.0/169.0 kB\u001b[0m \u001b[31m15.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading numpy-1.24.4-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.3 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.3/17.3 MB\u001b[0m \u001b[31m100.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading packaging-23.2-py3-none-any.whl (53 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.0/53.0 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading PyYAML-6.0.1-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (736 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m736.6/736.6 kB\u001b[0m \u001b[31m49.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading tensorboard-2.14.0-py3-none-any.whl (5.5 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.5/5.5 MB\u001b[0m \u001b[31m113.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading torch-2.1.2-cp38-cp38-manylinux1_x86_64.whl (670.2 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m670.2/670.2 MB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m731.7/731.7 MB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading triton-2.1.0-0-cp38-cp38-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (89.2 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m89.2/89.2 MB\u001b[0m \u001b[31m24.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading torchmetrics-1.3.0.post0-py3-none-any.whl (840 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m840.2/840.2 kB\u001b[0m \u001b[31m48.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading tqdm-4.66.1-py3-none-any.whl (78 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.3/78.3 kB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading typing_extensions-4.9.0-py3-none-any.whl (32 kB)\n","Downloading absl_py-2.1.0-py3-none-any.whl (133 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m133.7/133.7 kB\u001b[0m \u001b[31m12.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading aiohttp-3.9.3-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m65.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading google_auth-2.27.0-py2.py3-none-any.whl (186 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m186.8/186.8 kB\u001b[0m \u001b[31m15.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading grpcio-1.60.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.4 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.4/5.4 MB\u001b[0m \u001b[31m109.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading lightning_utilities-0.10.1-py3-none-any.whl (24 kB)\n","Downloading Markdown-3.5.2-py3-none-any.whl (103 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m103.9/103.9 kB\u001b[0m \u001b[31m9.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading protobuf-4.25.2-cp37-abi3-manylinux2014_x86_64.whl (294 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m294.6/294.6 kB\u001b[0m \u001b[31m23.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading requests-2.31.0-py3-none-any.whl (62 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.6/62.6 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading tensorboard_data_server-0.7.2-py3-none-manylinux_2_31_x86_64.whl (6.6 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.6/6.6 MB\u001b[0m \u001b[31m107.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading werkzeug-3.0.1-py3-none-any.whl (226 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m226.7/226.7 kB\u001b[0m \u001b[31m19.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading filelock-3.13.1-py3-none-any.whl (11 kB)\n","Downloading Jinja2-3.1.3-py3-none-any.whl (133 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m133.2/133.2 kB\u001b[0m \u001b[31m11.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading async_timeout-4.0.3-py3-none-any.whl (5.7 kB)\n","Downloading attrs-23.2.0-py3-none-any.whl (60 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.8/60.8 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading cachetools-5.3.2-py3-none-any.whl (9.3 kB)\n","Downloading certifi-2023.11.17-py3-none-any.whl (162 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m162.5/162.5 kB\u001b[0m \u001b[31m14.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading charset_normalizer-3.3.2-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (141 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m141.1/141.1 kB\u001b[0m \u001b[31m13.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading frozenlist-1.4.1-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (240 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m240.9/240.9 kB\u001b[0m \u001b[31m21.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading idna-3.6-py3-none-any.whl (61 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.6/61.6 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading importlib_metadata-7.0.1-py3-none-any.whl (23 kB)\n","Downloading MarkupSafe-2.1.4-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (26 kB)\n","Downloading urllib3-2.1.0-py3-none-any.whl (104 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m104.6/104.6 kB\u001b[0m \u001b[31m9.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading yarl-1.9.4-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (308 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m308.8/308.8 kB\u001b[0m \u001b[31m25.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.3.101-py3-none-manylinux1_x86_64.whl (20.5 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m20.5/20.5 MB\u001b[0m \u001b[31m85.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading pyasn1-0.5.1-py2.py3-none-any.whl (84 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.9/84.9 kB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading zipp-3.17.0-py3-none-any.whl (7.4 kB)\n","Building wheels for collected packages: future\n","  Building wheel for future (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for future: filename=future-0.18.3-py3-none-any.whl size=492037 sha256=778c2ee57dd7e5a7501a8a5a380388e54a448990eefd88ec67ae883b6a07ee2b\n","  Stored in directory: /root/.cache/pip/wheels/a0/0b/ee/e6994fadb42c1354dcccb139b0bf2795271bddfe6253ccdf11\n","Successfully built future\n","\u001b[33mDEPRECATION: pytorch-lightning 1.5.9 has a non-standard dependency specifier torch>=1.7.*. pip 24.0 will enforce this behaviour change. A possible replacement is to upgrade to a newer version of pytorch-lightning or contact the author to suggest that they release a version with a conforming dependency specifiers. Discussion can be found at https://github.com/pypa/pip/issues/12063\u001b[0m\u001b[33m\n","\u001b[0mInstalling collected packages: mpmath, zipp, urllib3, typing-extensions, tqdm, tensorboard-data-server, sympy, setuptools, PyYAML, pyDeprecate, pyasn1, protobuf, packaging, oauthlib, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, numpy, networkx, multidict, MarkupSafe, idna, grpcio, future, fsspec, frozenlist, filelock, charset-normalizer, certifi, cachetools, attrs, async-timeout, absl-py, yarl, werkzeug, triton, rsa, requests, pyasn1-modules, nvidia-cusparse-cu12, nvidia-cudnn-cu12, lightning-utilities, jinja2, importlib-metadata, aiosignal, requests-oauthlib, nvidia-cusolver-cu12, markdown, google-auth, aiohttp, torch, google-auth-oauthlib, torchmetrics, tensorboard, pytorch-lightning\n","  Attempting uninstall: setuptools\n","    Found existing installation: setuptools 59.6.0\n","    Uninstalling setuptools-59.6.0:\n","      Successfully uninstalled setuptools-59.6.0\n","Successfully installed MarkupSafe-2.1.4 PyYAML-6.0.1 absl-py-2.1.0 aiohttp-3.9.3 aiosignal-1.3.1 async-timeout-4.0.3 attrs-23.2.0 cachetools-5.3.2 certifi-2023.11.17 charset-normalizer-3.3.2 filelock-3.13.1 frozenlist-1.4.1 fsspec-2023.12.2 future-0.18.3 google-auth-2.27.0 google-auth-oauthlib-1.0.0 grpcio-1.60.0 idna-3.6 importlib-metadata-7.0.1 jinja2-3.1.3 lightning-utilities-0.10.1 markdown-3.5.2 mpmath-1.3.0 multidict-6.0.4 networkx-3.1 numpy-1.24.4 nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.18.1 nvidia-nvjitlink-cu12-12.3.101 nvidia-nvtx-cu12-12.1.105 oauthlib-3.2.2 packaging-23.2 protobuf-4.25.2 pyDeprecate-0.3.1 pyasn1-0.5.1 pyasn1-modules-0.3.0 pytorch-lightning-1.5.9 requests-2.31.0 requests-oauthlib-1.3.1 rsa-4.9 setuptools-59.5.0 sympy-1.12 tensorboard-2.14.0 tensorboard-data-server-0.7.2 torch-2.1.2 torchmetrics-1.3.0.post0 tqdm-4.66.1 triton-2.1.0 typing-extensions-4.9.0 urllib3-2.1.0 werkzeug-3.0.1 yarl-1.9.4 zipp-3.17.0\n","\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n","\u001b[0m"]},{"output_type":"display_data","data":{"application/vnd.colab-display-data+json":{"pip_warning":{"packages":["certifi"]}}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Collecting pillow==9.0\n","  Downloading Pillow-9.0.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.3 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.3/4.3 MB\u001b[0m \u001b[31m37.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h\u001b[33mDEPRECATION: pytorch-lightning 1.5.9 has a non-standard dependency specifier torch>=1.7.*. pip 24.0 will enforce this behaviour change. A possible replacement is to upgrade to a newer version of pytorch-lightning or contact the author to suggest that they release a version with a conforming dependency specifiers. Discussion can be found at https://github.com/pypa/pip/issues/12063\u001b[0m\u001b[33m\n","\u001b[0mInstalling collected packages: pillow\n","Successfully installed pillow-9.0.0\n","\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n","\u001b[0mCollecting faiss-gpu==1.7\n","  Downloading faiss_gpu-1.7.0-cp38-cp38-manylinux2014_x86_64.whl (89.4 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m89.4/89.4 MB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h\u001b[33mDEPRECATION: pytorch-lightning 1.5.9 has a non-standard dependency specifier torch>=1.7.*. pip 24.0 will enforce this behaviour change. A possible replacement is to upgrade to a newer version of pytorch-lightning or contact the author to suggest that they release a version with a conforming dependency specifiers. Discussion can be found at https://github.com/pypa/pip/issues/12063\u001b[0m\u001b[33m\n","\u001b[0mInstalling collected packages: faiss-gpu\n","Successfully installed faiss-gpu-1.7.0\n","\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n","\u001b[0m\u001b[31mERROR: Ignored the following yanked versions: 3.4.9.31, 3.4.10.35, 3.4.11.39\u001b[0m\u001b[31m\n","\u001b[0m\u001b[31mERROR: Could not find a version that satisfies the requirement opencv-python==4.5 (from versions: 3.4.0.14, 3.4.8.29, 3.4.9.33, 3.4.10.37, 3.4.11.41, 3.4.11.43, 3.4.11.45, 3.4.13.47, 3.4.14.51, 3.4.14.53, 3.4.15.55, 3.4.16.57, 3.4.16.59, 3.4.17.61, 3.4.17.63, 3.4.18.65, 4.1.2.30, 4.2.0.32, 4.2.0.34, 4.3.0.36, 4.3.0.38, 4.4.0.40, 4.4.0.42, 4.4.0.44, 4.4.0.46, 4.5.1.48, 4.5.2.52, 4.5.2.54, 4.5.3.56, 4.5.4.58, 4.5.4.60, 4.5.5.62, 4.5.5.64, 4.6.0.66, 4.7.0.68, 4.7.0.72, 4.8.0.74, 4.8.0.76, 4.8.1.78, 4.9.0.80)\u001b[0m\u001b[31m\n","\u001b[0m\u001b[31mERROR: No matching distribution found for opencv-python==4.5\u001b[0m\u001b[31m\n","\u001b[0mCollecting scikit-learn==0.24\n","  Downloading scikit_learn-0.24.0-cp38-cp38-manylinux2010_x86_64.whl (24.9 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.9/24.9 MB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.8/dist-packages (from scikit-learn==0.24) (1.24.4)\n","Collecting scipy>=0.19.1 (from scikit-learn==0.24)\n","  Downloading scipy-1.10.1-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (34.5 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m34.5/34.5 MB\u001b[0m \u001b[31m61.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting joblib>=0.11 (from scikit-learn==0.24)\n","  Downloading joblib-1.3.2-py3-none-any.whl.metadata (5.4 kB)\n","Collecting threadpoolctl>=2.0.0 (from scikit-learn==0.24)\n","  Downloading threadpoolctl-3.2.0-py3-none-any.whl.metadata (10.0 kB)\n","Downloading joblib-1.3.2-py3-none-any.whl (302 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m302.2/302.2 kB\u001b[0m \u001b[31m24.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading threadpoolctl-3.2.0-py3-none-any.whl (15 kB)\n","\u001b[33mDEPRECATION: pytorch-lightning 1.5.9 has a non-standard dependency specifier torch>=1.7.*. pip 24.0 will enforce this behaviour change. A possible replacement is to upgrade to a newer version of pytorch-lightning or contact the author to suggest that they release a version with a conforming dependency specifiers. Discussion can be found at https://github.com/pypa/pip/issues/12063\u001b[0m\u001b[33m\n","\u001b[0mInstalling collected packages: threadpoolctl, scipy, joblib, scikit-learn\n","Successfully installed joblib-1.3.2 scikit-learn-0.24.0 scipy-1.10.1 threadpoolctl-3.2.0\n","\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n","\u001b[0mCollecting scikit-image==0.19\n","  Downloading scikit_image-0.19.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (60.7 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.7/60.7 MB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: numpy>=1.17.0 in /usr/local/lib/python3.8/dist-packages (from scikit-image==0.19) (1.24.4)\n","Requirement already satisfied: scipy>=1.4.1 in /usr/local/lib/python3.8/dist-packages (from scikit-image==0.19) (1.10.1)\n","Requirement already satisfied: networkx>=2.2 in /usr/local/lib/python3.8/dist-packages (from scikit-image==0.19) (3.1)\n","Requirement already satisfied: pillow!=7.1.0,!=7.1.1,!=8.3.0,>=6.1.0 in /usr/local/lib/python3.8/dist-packages (from scikit-image==0.19) (9.0.0)\n","Collecting imageio>=2.4.1 (from scikit-image==0.19)\n","  Downloading imageio-2.33.1-py3-none-any.whl.metadata (4.9 kB)\n","Collecting tifffile>=2019.7.26 (from scikit-image==0.19)\n","  Downloading tifffile-2023.7.10-py3-none-any.whl.metadata (31 kB)\n","Collecting PyWavelets>=1.1.1 (from scikit-image==0.19)\n","  Downloading PyWavelets-1.4.1-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (6.9 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.9/6.9 MB\u001b[0m \u001b[31m110.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.8/dist-packages (from scikit-image==0.19) (23.2)\n","Downloading imageio-2.33.1-py3-none-any.whl (313 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m313.3/313.3 kB\u001b[0m \u001b[31m26.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading tifffile-2023.7.10-py3-none-any.whl (220 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m220.9/220.9 kB\u001b[0m \u001b[31m20.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h\u001b[33mDEPRECATION: pytorch-lightning 1.5.9 has a non-standard dependency specifier torch>=1.7.*. pip 24.0 will enforce this behaviour change. A possible replacement is to upgrade to a newer version of pytorch-lightning or contact the author to suggest that they release a version with a conforming dependency specifiers. Discussion can be found at https://github.com/pypa/pip/issues/12063\u001b[0m\u001b[33m\n","\u001b[0mInstalling collected packages: tifffile, PyWavelets, imageio, scikit-image\n","Successfully installed PyWavelets-1.4.1 imageio-2.33.1 scikit-image-0.19.0 tifffile-2023.7.10\n","\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n","\u001b[0mCollecting pymp-pypi==0.5\n","  Downloading pymp-pypi-0.5.0.tar.gz (12 kB)\n","  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Building wheels for collected packages: pymp-pypi\n","  Building wheel for pymp-pypi (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for pymp-pypi: filename=pymp_pypi-0.5.0-py3-none-any.whl size=10339 sha256=cfc9435660c403e40feed524bc70b2a31ccb1417a6581ecaa19bb36ccaca606a\n","  Stored in directory: /root/.cache/pip/wheels/98/1d/32/d3b1174e02ae373cde389cd6fccc24c8f2530509b138282702\n","Successfully built pymp-pypi\n","\u001b[33mDEPRECATION: pytorch-lightning 1.5.9 has a non-standard dependency specifier torch>=1.7.*. pip 24.0 will enforce this behaviour change. A possible replacement is to upgrade to a newer version of pytorch-lightning or contact the author to suggest that they release a version with a conforming dependency specifiers. Discussion can be found at https://github.com/pypa/pip/issues/12063\u001b[0m\u001b[33m\n","\u001b[0mInstalling collected packages: pymp-pypi\n","Successfully installed pymp-pypi-0.5.0\n","\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n","\u001b[0mCollecting numpngw==0.1\n","  Downloading numpngw-0.1.0-py3-none-any.whl (21 kB)\n","Requirement already satisfied: numpy>=1.6.0 in /usr/local/lib/python3.8/dist-packages (from numpngw==0.1) (1.24.4)\n","\u001b[33mDEPRECATION: pytorch-lightning 1.5.9 has a non-standard dependency specifier torch>=1.7.*. pip 24.0 will enforce this behaviour change. A possible replacement is to upgrade to a newer version of pytorch-lightning or contact the author to suggest that they release a version with a conforming dependency specifiers. Discussion can be found at https://github.com/pypa/pip/issues/12063\u001b[0m\u001b[33m\n","\u001b[0mInstalling collected packages: numpngw\n","Successfully installed numpngw-0.1.0\n","\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n","\u001b[0mCollecting matplotlib==3.7\n","  Downloading matplotlib-3.7.0-cp38-cp38-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (9.2 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.2/9.2 MB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting contourpy>=1.0.1 (from matplotlib==3.7)\n","  Downloading contourpy-1.1.1-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.9 kB)\n","Collecting cycler>=0.10 (from matplotlib==3.7)\n","  Downloading cycler-0.12.1-py3-none-any.whl.metadata (3.8 kB)\n","Collecting fonttools>=4.22.0 (from matplotlib==3.7)\n","  Downloading fonttools-4.47.2-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (157 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m157.6/157.6 kB\u001b[0m \u001b[31m11.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting kiwisolver>=1.0.1 (from matplotlib==3.7)\n","  Downloading kiwisolver-1.4.5-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.whl.metadata (6.4 kB)\n","Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.8/dist-packages (from matplotlib==3.7) (1.24.4)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.8/dist-packages (from matplotlib==3.7) (23.2)\n","Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.8/dist-packages (from matplotlib==3.7) (9.0.0)\n","Collecting pyparsing>=2.3.1 (from matplotlib==3.7)\n","  Downloading pyparsing-3.1.1-py3-none-any.whl.metadata (5.1 kB)\n","Collecting python-dateutil>=2.7 (from matplotlib==3.7)\n","  Downloading python_dateutil-2.8.2-py2.py3-none-any.whl (247 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m247.7/247.7 kB\u001b[0m \u001b[31m21.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting importlib-resources>=3.2.0 (from matplotlib==3.7)\n","  Downloading importlib_resources-6.1.1-py3-none-any.whl.metadata (4.1 kB)\n","Requirement already satisfied: zipp>=3.1.0 in /usr/local/lib/python3.8/dist-packages (from importlib-resources>=3.2.0->matplotlib==3.7) (3.17.0)\n","Requirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil>=2.7->matplotlib==3.7) (1.16.0)\n","Downloading contourpy-1.1.1-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (301 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m301.1/301.1 kB\u001b[0m \u001b[31m23.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading cycler-0.12.1-py3-none-any.whl (8.3 kB)\n","Downloading fonttools-4.47.2-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.7 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.7/4.7 MB\u001b[0m \u001b[31m42.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading importlib_resources-6.1.1-py3-none-any.whl (33 kB)\n","Downloading kiwisolver-1.4.5-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.whl (1.2 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m40.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading pyparsing-3.1.1-py3-none-any.whl (103 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m103.1/103.1 kB\u001b[0m \u001b[31m10.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h\u001b[33mDEPRECATION: pytorch-lightning 1.5.9 has a non-standard dependency specifier torch>=1.7.*. pip 24.0 will enforce this behaviour change. A possible replacement is to upgrade to a newer version of pytorch-lightning or contact the author to suggest that they release a version with a conforming dependency specifiers. Discussion can be found at https://github.com/pypa/pip/issues/12063\u001b[0m\u001b[33m\n","\u001b[0mInstalling collected packages: python-dateutil, pyparsing, kiwisolver, importlib-resources, fonttools, cycler, contourpy, matplotlib\n","Successfully installed contourpy-1.1.1 cycler-0.12.1 fonttools-4.47.2 importlib-resources-6.1.1 kiwisolver-1.4.5 matplotlib-3.7.0 pyparsing-3.1.1 python-dateutil-2.8.2\n","\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n","\u001b[0m"]},{"output_type":"display_data","data":{"application/vnd.colab-display-data+json":{"pip_warning":{"packages":["cycler","dateutil","kiwisolver"]}}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Collecting timm==0.9\n","  Downloading timm-0.9.0-py3-none-any.whl.metadata (68 kB)\n","\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/68.5 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m68.5/68.5 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: torch>=1.7 in /usr/local/lib/python3.8/dist-packages (from timm==0.9) (2.1.2)\n","Collecting torchvision (from timm==0.9)\n","  Downloading torchvision-0.16.2-cp38-cp38-manylinux1_x86_64.whl.metadata (6.6 kB)\n","Requirement already satisfied: pyyaml in /usr/local/lib/python3.8/dist-packages (from timm==0.9) (6.0.1)\n","Collecting huggingface-hub (from timm==0.9)\n","  Downloading huggingface_hub-0.20.3-py3-none-any.whl.metadata (12 kB)\n","Collecting safetensors (from timm==0.9)\n","  Downloading safetensors-0.4.2-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.8 kB)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.8/dist-packages (from torch>=1.7->timm==0.9) (3.13.1)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.8/dist-packages (from torch>=1.7->timm==0.9) (4.9.0)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.8/dist-packages (from torch>=1.7->timm==0.9) (1.12)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.8/dist-packages (from torch>=1.7->timm==0.9) (3.1)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.8/dist-packages (from torch>=1.7->timm==0.9) (3.1.3)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.8/dist-packages (from torch>=1.7->timm==0.9) (2023.12.2)\n","Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.8/dist-packages (from torch>=1.7->timm==0.9) (12.1.105)\n","Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.8/dist-packages (from torch>=1.7->timm==0.9) (12.1.105)\n","Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.8/dist-packages (from torch>=1.7->timm==0.9) (12.1.105)\n","Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.8/dist-packages (from torch>=1.7->timm==0.9) (8.9.2.26)\n","Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.8/dist-packages (from torch>=1.7->timm==0.9) (12.1.3.1)\n","Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.8/dist-packages (from torch>=1.7->timm==0.9) (11.0.2.54)\n","Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.8/dist-packages (from torch>=1.7->timm==0.9) (10.3.2.106)\n","Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.8/dist-packages (from torch>=1.7->timm==0.9) (11.4.5.107)\n","Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.8/dist-packages (from torch>=1.7->timm==0.9) (12.1.0.106)\n","Requirement already satisfied: nvidia-nccl-cu12==2.18.1 in /usr/local/lib/python3.8/dist-packages (from torch>=1.7->timm==0.9) (2.18.1)\n","Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.8/dist-packages (from torch>=1.7->timm==0.9) (12.1.105)\n","Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.8/dist-packages (from torch>=1.7->timm==0.9) (2.1.0)\n","Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.8/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.7->timm==0.9) (12.3.101)\n","Requirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from huggingface-hub->timm==0.9) (2.31.0)\n","Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.8/dist-packages (from huggingface-hub->timm==0.9) (4.66.1)\n","Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.8/dist-packages (from huggingface-hub->timm==0.9) (23.2)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.8/dist-packages (from torchvision->timm==0.9) (1.24.4)\n","Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.8/dist-packages (from torchvision->timm==0.9) (9.0.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.8/dist-packages (from jinja2->torch>=1.7->timm==0.9) (2.1.4)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.8/dist-packages (from requests->huggingface-hub->timm==0.9) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->huggingface-hub->timm==0.9) (3.6)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->huggingface-hub->timm==0.9) (2.1.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->huggingface-hub->timm==0.9) (2023.11.17)\n","Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.8/dist-packages (from sympy->torch>=1.7->timm==0.9) (1.3.0)\n","Downloading timm-0.9.0-py3-none-any.whl (2.2 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading huggingface_hub-0.20.3-py3-none-any.whl (330 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m330.1/330.1 kB\u001b[0m \u001b[31m27.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading safetensors-0.4.2-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m69.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading torchvision-0.16.2-cp38-cp38-manylinux1_x86_64.whl (6.8 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.8/6.8 MB\u001b[0m \u001b[31m117.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h\u001b[33mDEPRECATION: pytorch-lightning 1.5.9 has a non-standard dependency specifier torch>=1.7.*. pip 24.0 will enforce this behaviour change. A possible replacement is to upgrade to a newer version of pytorch-lightning or contact the author to suggest that they release a version with a conforming dependency specifiers. Discussion can be found at https://github.com/pypa/pip/issues/12063\u001b[0m\u001b[33m\n","\u001b[0mInstalling collected packages: safetensors, huggingface-hub, torchvision, timm\n","Successfully installed huggingface-hub-0.20.3 safetensors-0.4.2 timm-0.9.0 torchvision-0.16.2\n","\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n","\u001b[0m"]}]},{"cell_type":"code","source":["!pip install torch==1.12.1+cu116 torchvision==0.13.1+cu116 -f https://download.pytorch.org/whl/torch_stable.html"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_r0v968PLl10","executionInfo":{"status":"ok","timestamp":1706572752623,"user_tz":-60,"elapsed":76171,"user":{"displayName":"田代勇希","userId":"12989083184923368445"}},"outputId":"1d3d2bad-3b42-4293-bd78-4c88c19f024f"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in links: https://download.pytorch.org/whl/torch_stable.html\n","Collecting torch==1.12.1+cu116\n","  Downloading https://download.pytorch.org/whl/cu116/torch-1.12.1%2Bcu116-cp38-cp38-linux_x86_64.whl (1904.8 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 GB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting torchvision==0.13.1+cu116\n","  Downloading https://download.pytorch.org/whl/cu116/torchvision-0.13.1%2Bcu116-cp38-cp38-linux_x86_64.whl (23.5 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.5/23.5 MB\u001b[0m \u001b[31m56.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: typing-extensions in /usr/local/lib/python3.8/dist-packages (from torch==1.12.1+cu116) (4.9.0)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.8/dist-packages (from torchvision==0.13.1+cu116) (1.24.4)\n","Requirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from torchvision==0.13.1+cu116) (2.31.0)\n","Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.8/dist-packages (from torchvision==0.13.1+cu116) (9.0.0)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.8/dist-packages (from requests->torchvision==0.13.1+cu116) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->torchvision==0.13.1+cu116) (3.6)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->torchvision==0.13.1+cu116) (2.1.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->torchvision==0.13.1+cu116) (2023.11.17)\n","\u001b[33mDEPRECATION: pytorch-lightning 1.5.9 has a non-standard dependency specifier torch>=1.7.*. pip 24.0 will enforce this behaviour change. A possible replacement is to upgrade to a newer version of pytorch-lightning or contact the author to suggest that they release a version with a conforming dependency specifiers. Discussion can be found at https://github.com/pypa/pip/issues/12063\u001b[0m\u001b[33m\n","\u001b[0mInstalling collected packages: torch, torchvision\n","  Attempting uninstall: torch\n","    Found existing installation: torch 2.1.2\n","    Uninstalling torch-2.1.2:\n","      Successfully uninstalled torch-2.1.2\n","  Attempting uninstall: torchvision\n","    Found existing installation: torchvision 0.16.2\n","    Uninstalling torchvision-0.16.2:\n","      Successfully uninstalled torchvision-0.16.2\n","Successfully installed torch-1.12.1+cu116 torchvision-0.13.1+cu116\n","\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n","\u001b[0m"]}]},{"cell_type":"code","source":["!pip install opencv-python\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"f9IGwjkONiEK","executionInfo":{"status":"ok","timestamp":1706572756427,"user_tz":-60,"elapsed":3812,"user":{"displayName":"田代勇希","userId":"12989083184923368445"}},"outputId":"bfb69f49-b907-4d2d-8a7d-6ba0814b26af"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting opencv-python\n","  Downloading opencv_python-4.9.0.80-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (20 kB)\n","Requirement already satisfied: numpy>=1.17.0 in /usr/local/lib/python3.8/dist-packages (from opencv-python) (1.24.4)\n","Downloading opencv_python-4.9.0.80-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (62.2 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.2/62.2 MB\u001b[0m \u001b[31m32.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h\u001b[33mDEPRECATION: pytorch-lightning 1.5.9 has a non-standard dependency specifier torch>=1.7.*. pip 24.0 will enforce this behaviour change. A possible replacement is to upgrade to a newer version of pytorch-lightning or contact the author to suggest that they release a version with a conforming dependency specifiers. Discussion can be found at https://github.com/pypa/pip/issues/12063\u001b[0m\u001b[33m\n","\u001b[0mInstalling collected packages: opencv-python\n","Successfully installed opencv-python-4.9.0.80\n","\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n","\u001b[0m"]}]},{"cell_type":"markdown","source":["## transfering dataset for FastMRI"],"metadata":{"id":"AxK8aNbSmnIH"}},{"cell_type":"code","source":["import os\n","import shutil\n","import glob\n","from PIL import Image\n","import pandas as pd\n","import numpy as np\n","\n","\n","# Copy images and masks\n","def transfer_images(data_dir, train_dataset_names, pathologies):\n","\n","    # Run a loop for each pathology\n","    for pathology in pathologies:\n","\n","        # Run a loop for each train dataset\n","        for train_name in train_dataset_names:\n","            # Set the path of the CSV file\n","            csv_file = os.path.join(data_dir, 'data', 'splits', f'{train_name}.csv')\n","\n","            # Read the CSV file and get the list of image files\n","            files = pd.read_csv(csv_file)['filename'].tolist()\n","\n","            # List of directory structure\n","            dir_structure = [\n","                [data_dir, 'UAD', f'{pathology}', 'train', 'good']\n","            ]\n","\n","            # make directories\n","            for dirs in dir_structure:\n","                full_path = os.path.join(*dirs)\n","                os.makedirs(full_path, exist_ok=True)\n","\n","            # Copy images\n","            for file in files:\n","                shutil.copy(os.path.join(data_dir, file.replace(\"./\", \"\")), os.path.join(data_dir, 'UAD', f'{pathology}', 'train', 'good', os.path.basename(file)))\n","\n","\n","        # Set the path of the CSV file\n","        img_csv = os.path.join(data_dir, 'data', 'splits', f'{pathology}.csv')\n","        mask_csv = os.path.join(data_dir, 'data', 'splits', f'{pathology}_ann.csv')\n","\n","        # Read the CSV file and get the list of image files and mask files\n","        train_files_img = pd.read_csv(img_csv)['filename'].tolist()\n","        train_files_mask = pd.read_csv(mask_csv)['filename'].tolist()\n","\n","        # List of directory structure\n","        dir_structure = [\n","            [data_dir, 'UAD', f'{pathology}', 'test', 'anomaly_images'],\n","            [data_dir, 'UAD', f'{pathology}', 'ground_truth', 'anomaly_images'],\n","        ]\n","\n","        # make directories\n","        for dirs in dir_structure:\n","            full_path = os.path.join(*dirs)\n","            os.makedirs(full_path, exist_ok=True)\n","\n","        # Copy images and masks\n","        for file in train_files_img:\n","            shutil.copy(os.path.join(data_dir, file.replace(\"./\", \"\")), os.path.join(data_dir, 'UAD', f'{pathology}', 'test', 'anomaly_images', os.path.basename(file)))\n","\n","        for file in train_files_mask:\n","            shutil.copy(os.path.join(data_dir, file.replace(\"./\", \"\")), os.path.join(data_dir, 'UAD', f'{pathology}', 'ground_truth', 'anomaly_images', os.path.basename(file)))\n","\n","\n","def combined_images(data_dir, pathologies):\n","\n","    for pathology in pathologies:\n","\n","        # Specify the ground_truth directory\n","        ground_truth_dir = os.path.join(data_dir, 'UAD', f'{pathology}', 'ground_truth', 'anomaly_images')\n","\n","        # Get the image paths\n","        image_paths = glob.glob(os.path.join(ground_truth_dir, \"*.png\"))\n","\n","        # Generate DataFrame from image paths\n","        data = []\n","        for image_path in image_paths:\n","            # Extract a part of the image path to use as a key\n","            image_path_idx = \"_\".join(os.path.basename(image_path).split(\"_\")[:5])\n","\n","            # Add the image path and its part to the data list\n","            data.append([image_path_idx, image_path])\n","\n","        # Generate DataFrame from data list\n","        df = pd.DataFrame(data, columns=['Index', 'ImagePath'])\n","\n","        # Create a dictionary with a list of ImagePaths as values, using Index as the key\n","        image_dict = {}\n","        for idx, group in df.groupby('Index'):\n","            image_dict[idx] = group['ImagePath'].tolist()\n","\n","        # Delete entries with only one image path\n","        keys_to_delete = [key for key, paths in image_dict.items() if len(paths) == 1]\n","\n","        # Delete\n","        for key in keys_to_delete:\n","            del image_dict[key]\n","\n","        for key, ImagePath in image_dict.items():\n","            # Load the images and store their arrays in a list\n","            images = [Image.open(p) for p in ImagePath]\n","\n","            # Convert the images to numpy arrays and calculate the average\n","            images_array = [np.array(image) for image in images]\n","            # 画像を足し合わせる\n","            combined_image = np.clip(np.sum(images_array, axis=0), 0, 255).astype(np.uint8)\n","\n","            # Convert the averaged image to a PIL image and save\n","            result_image = Image.fromarray(combined_image)\n","            result_image_path = os.path.join(os.path.dirname(ImagePath[0]), key + '_combined.png')\n","            result_image.save(result_image_path)\n","\n","            # Delete the original image files\n","            for p in ImagePath:\n","                os.remove(p)\n","\n","\n","# Set the names of the datasets and the directory structure\n","train_dataset_names = ['ixi_normal_train', 'normal_train', 'normal_val']\n","\n","# Set pathologies\n","pathologies = [\n","    'absent_septum',\n","    'artefacts',\n","    'craniatomy',\n","    'dural',\n","    'ea_mass',\n","    'edema',\n","    'encephalomalacia',\n","    'enlarged_ventricles',\n","    'intraventricular',\n","    'lesions',\n","    'mass',\n","    'posttreatment',\n","    'resection',\n","    'sinus',\n","    'wml',\n","    'other'\n","]\n","\n","data_dir = '../dataset/'  # Set the path of the data directory here\n","\n","# Copy images and masks\n","transfer_images(data_dir, train_dataset_names, pathologies)\n","\n","combined_images(data_dir, pathologies)"],"metadata":{"id":"cqU0OMBXmtol"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["5min"],"metadata":{"id":"T97ZzNjv9U6O"}},{"cell_type":"markdown","source":["## 実行"],"metadata":{"id":"xK_MbQkDXL4M"}},{"cell_type":"code","source":["# train our proposed model and evaluates anomaly map for each category\n","# You can change dataset with \"--dataset_category\" argument, and category with \"--category\" path.\n","# Note that dataset should be in directory of \"--dataset_path\" argument.\n","# If you want other pretrained network rather than WideResNet101-2, change \"--backbone\" argument.\n","\n","!python3 train_coreset_distribution.py --category absent_septum --dataset_category UAD --seed 23 --train_coreset --train_nb_dist --train_coor_dist --dataset_path ../dataset/UAD  --num_epochs 15 --num_layers 10 --gpu 1 --subsampling_percentage 0.01 --resize 128 --imagesize 128 # --num_workers 1\n","!python3 train_coreset_distribution.py --category artefacts --dataset_category UAD --seed 23 --train_coreset --train_nb_dist --train_coor_dist --dataset_path ../dataset/UAD  --num_epochs 15 --num_layers 10 --gpu 1 --subsampling_percentage 0.01 --resize 128 --imagesize 128 # --num_workers 1\n","!python3 train_coreset_distribution.py --category craniatomy --dataset_category UAD --seed 23 --train_coreset --train_nb_dist --train_coor_dist --dataset_path ../dataset/UAD  --num_epochs 15 --num_layers 10 --gpu 1 --subsampling_percentage 0.01 --resize 128 --imagesize 128 # --num_workers 1\n","!python3 train_coreset_distribution.py --category dural --dataset_category UAD --seed 23 --train_coreset --train_nb_dist --train_coor_dist --dataset_path ../dataset/UAD  --num_epochs 15 --num_layers 10 --gpu 1 --subsampling_percentage 0.01 --resize 128 --imagesize 128 # --num_workers 1\n","!python3 train_coreset_distribution.py --category ea_mass --dataset_category UAD --seed 23 --train_coreset --train_nb_dist --train_coor_dist --dataset_path ../dataset/UAD  --num_epochs 15 --num_layers 10 --gpu 1 --subsampling_percentage 0.01 --resize 128 --imagesize 128 # --num_workers 1\n","!python3 train_coreset_distribution.py --category edema --dataset_category UAD --seed 23 --train_coreset --train_nb_dist --train_coor_dist --dataset_path ../dataset/UAD  --num_epochs 15 --num_layers 10 --gpu 1 --subsampling_percentage 0.01 --resize 128 --imagesize 128 # --num_workers 1\n","!python3 train_coreset_distribution.py --category encephalomalacia --dataset_category UAD --seed 23 --train_coreset --train_nb_dist --train_coor_dist --dataset_path ../dataset/UAD  --num_epochs 15 --num_layers 10 --gpu 1 --subsampling_percentage 0.01 --resize 128 --imagesize 128 # --num_workers 1\n","!python3 train_coreset_distribution.py --category enlarged_ventricles --dataset_category UAD --seed 23 --train_coreset --train_nb_dist --train_coor_dist --dataset_path ../dataset/UAD  --num_epochs 15 --num_layers 10 --gpu 1 --subsampling_percentage 0.01 --resize 128 --imagesize 128 # --num_workers 1\n","!python3 train_coreset_distribution.py --category intraventricular --dataset_category UAD --seed 23 --train_coreset --train_nb_dist --train_coor_dist --dataset_path ../dataset/UAD  --num_epochs 15 --num_layers 10 --gpu 1 --subsampling_percentage 0.01 --resize 128 --imagesize 128 # --num_workers 1\n","!python3 train_coreset_distribution.py --category lesions --dataset_category UAD --seed 23 --train_coreset --train_nb_dist --train_coor_dist --dataset_path ../dataset/UAD  --num_epochs 15 --num_layers 10 --gpu 1 --subsampling_percentage 0.01 --resize 128 --imagesize 128 # --num_workers 1\n","!python3 train_coreset_distribution.py --category mass --dataset_category UAD --seed 23 --train_coreset --train_nb_dist --train_coor_dist --dataset_path ../dataset/UAD  --num_epochs 15 --num_layers 10 --gpu 1 --subsampling_percentage 0.01 --resize 128 --imagesize 128 # --num_workers 1\n","!python3 train_coreset_distribution.py --category posttreatment --dataset_category UAD --seed 23 --train_coreset --train_nb_dist --train_coor_dist --dataset_path ../dataset/UAD  --num_epochs 15 --num_layers 10 --gpu 1 --subsampling_percentage 0.01 --resize 128 --imagesize 128 # --num_workers 1\n","!python3 train_coreset_distribution.py --category resection --dataset_category UAD --seed 23 --train_coreset --train_nb_dist --train_coor_dist --dataset_path ../dataset/UAD  --num_epochs 15 --num_layers 10 --gpu 1 --subsampling_percentage 0.01 --resize 128 --imagesize 128 # --num_workers 1\n","!python3 train_coreset_distribution.py --category sinus --dataset_category UAD --seed 23 --train_coreset --train_nb_dist --train_coor_dist --dataset_path ../dataset/UAD  --num_epochs 15 --num_layers 10 --gpu 1 --subsampling_percentage 0.01 --resize 128 --imagesize 128 # --num_workers 1\n","!python3 train_coreset_distribution.py --category wml --dataset_category UAD --seed 23 --train_coreset --train_nb_dist --train_coor_dist --dataset_path ../dataset/UAD  --num_epochs 15 --num_layers 10 --gpu 1 --subsampling_percentage 0.01 --resize 128 --imagesize 128 # --num_workers 1\n","!python3 train_coreset_distribution.py --category other --dataset_category UAD --seed 23 --train_coreset --train_nb_dist --train_coor_dist --dataset_path ../dataset/UAD  --num_epochs 15 --num_layers 10 --gpu 1 --subsampling_percentage 0.01 --resize 128 --imagesize 128 # --num_workers 1\n"],"metadata":{"id":"y4_1Bd7tLJR6","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1706432794647,"user_tz":-60,"elapsed":22849019,"user":{"displayName":"田代勇希","userId":"12989083184923368445"}},"outputId":"4b7866a5-f7d0-40f6-dab3-17dcfb267159"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Global seed set to 23\n","1\n","1\n","['../dataset/UAD/absent_septum/test/anomaly_images/file_brain_AXT1_202_6000392.png']\n","['../dataset/UAD/absent_septum/ground_truth/anomaly_images/file_brain_AXT1_202_6000392_absent_septum_pellucidum_0.png']\n","Start generating embeeding coreset and distribution coreset\n","GPU available: True, used: True\n","TPU available: False, using: 0 TPU cores\n","IPU available: False, using: 0 IPUs\n","coreset_generator_trainer完了\n","Downloading: \"https://github.com/pytorch/vision/zipball/v0.10.0\" to /root/.cache/torch/hub/v0.10.0.zip\n","/usr/local/lib/python3.8/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and will be removed in 0.15, please use 'weights' instead.\n","  warnings.warn(\n","/usr/local/lib/python3.8/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and will be removed in 0.15. The current behavior is equivalent to passing `weights=Wide_ResNet101_2_Weights.IMAGENET1K_V1`. You can also use `weights=Wide_ResNet101_2_Weights.DEFAULT` to get the most up-to-date weights.\n","  warnings.warn(msg)\n","Downloading: \"https://download.pytorch.org/models/wide_resnet101_2-32ee1156.pth\" to /root/.cache/torch/hub/checkpoints/wide_resnet101_2-32ee1156.pth\n","100% 243M/243M [00:06<00:00, 36.9MB/s]\n","coreset_generator完了\n","LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n","/usr/local/lib/python3.8/dist-packages/pytorch_lightning/trainer/optimizers.py:37: UserWarning: `LightningModule.configure_optimizers` returned `None`, this fit will run with no optimizer\n","  rank_zero_warn(\n","Missing logger folder: ./result/absent_septum/WR101/coreset/lightning_logs\n","\n","  | Name     | Type   | Params\n","------------------------------------\n","0 | backbone | ResNet | 126 M \n","------------------------------------\n","126 M     Trainable params\n","0         Non-trainable params\n","126 M     Total params\n","507.547   Total estimated model params size (MB)\n","/usr/local/lib/python3.8/dist-packages/pytorch_lightning/trainer/data_loading.py:132: UserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n","  rank_zero_warn(\n","Epoch 0:   0% 0/726 [00:00<?, ?it/s] /usr/local/lib/python3.8/dist-packages/pytorch_lightning/loops/optimization/optimizer_loop.py:145: UserWarning: `training_step` returned `None`. If this was on purpose, ignore this warning...\n","  self.warning_cache.warn(\n","Epoch 0:  99% 720/726 [00:34<00:00, 20.87it/s, loss=nan, v_num=0]Size of embedding coreset without edge feature :  (1045, 1024)\n","Size of distribution coreset without edge feature :  (2048, 1024)\n","Epoch 0:  99% 720/726 [00:48<00:00, 14.98it/s, loss=nan, v_num=0]Size of embedding coreset with edge feature :  (1858, 1024)\n","Epoch 0: 100% 726/726 [00:58<00:00, 12.38it/s, loss=nan, v_num=0]\n","End generating embeeding coreset and distribution coreset\n","Using cache found in /root/.cache/torch/hub/pytorch_vision_v0.10.0\n","/usr/local/lib/python3.8/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and will be removed in 0.15, please use 'weights' instead.\n","  warnings.warn(\n","/usr/local/lib/python3.8/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and will be removed in 0.15. The current behavior is equivalent to passing `weights=Wide_ResNet101_2_Weights.IMAGENET1K_V1`. You can also use `weights=Wide_ResNet101_2_Weights.DEFAULT` to get the most up-to-date weights.\n","  warnings.warn(msg)\n","Start training normal feature distribution from neighbrohood information\n","GPU available: True, used: True\n","TPU available: False, using: 0 TPU cores\n","IPU available: False, using: 0 IPUs\n","LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n","Missing logger folder: ./result/absent_septum/WR101/distribution\n","\n","  | Name  | Type               | Params\n","---------------------------------------------\n","0 | model | Distribution_Model | 205 M \n","---------------------------------------------\n","205 M     Trainable params\n","0         Non-trainable params\n","205 M     Total params\n","822.313   Total estimated model params size (MB)\n","Validation sanity check: 0it [00:00, ?it/s]/usr/local/lib/python3.8/dist-packages/pytorch_lightning/trainer/data_loading.py:132: UserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n","  rank_zero_warn(\n","Global seed set to 23\n","/usr/local/lib/python3.8/dist-packages/pytorch_lightning/trainer/data_loading.py:132: UserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n","  rank_zero_warn(\n","Epoch 0: 100% 52/52 [01:06<00:00,  1.28s/it, loss=6.32, v_num=0, train_loss=5.890]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/6 [00:00<?, ?it/s]\u001b[A\n","Epoch 0: 100% 52/52 [01:18<00:00,  1.51s/it, loss=6.04, v_num=0, train_loss=5.670, valid_loss=5.780]\n","Epoch 1: 100% 52/52 [01:07<00:00,  1.30s/it, loss=4.68, v_num=0, train_loss=4.470, valid_loss=5.780]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 1: 100% 52/52 [01:17<00:00,  1.50s/it, loss=4.68, v_num=0, train_loss=4.470, valid_loss=5.780]\n","Epoch 1: 100% 52/52 [01:19<00:00,  1.53s/it, loss=4.53, v_num=0, train_loss=4.410, valid_loss=4.090]\n","Epoch 2: 100% 52/52 [01:08<00:00,  1.32s/it, loss=3.93, v_num=0, train_loss=3.830, valid_loss=4.090]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/6 [00:00<?, ?it/s]\u001b[A\n","Epoch 2: 100% 52/52 [01:22<00:00,  1.58s/it, loss=3.86, v_num=0, train_loss=3.790, valid_loss=3.530]\n","Epoch 3: 100% 52/52 [01:07<00:00,  1.30s/it, loss=3.55, v_num=0, train_loss=3.520, valid_loss=3.530]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/6 [00:00<?, ?it/s]\u001b[A\n","Epoch 3: 100% 52/52 [01:19<00:00,  1.53s/it, loss=3.51, v_num=0, train_loss=3.500, valid_loss=3.160]\n","Epoch 4: 100% 52/52 [01:08<00:00,  1.32s/it, loss=3.29, v_num=0, train_loss=3.250, valid_loss=3.160]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/6 [00:00<?, ?it/s]\u001b[A\n","Epoch 4: 100% 52/52 [01:19<00:00,  1.54s/it, loss=3.27, v_num=0, train_loss=3.260, valid_loss=2.840]\n","Epoch 5: 100% 52/52 [01:07<00:00,  1.30s/it, loss=3.04, v_num=0, train_loss=2.970, valid_loss=2.840]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/6 [00:00<?, ?it/s]\u001b[A\n","Epoch 5: 100% 52/52 [01:19<00:00,  1.52s/it, loss=3.04, v_num=0, train_loss=3.020, valid_loss=2.660]\n","Epoch 6: 100% 52/52 [01:07<00:00,  1.30s/it, loss=2.99, v_num=0, train_loss=2.950, valid_loss=2.660]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 6: 100% 52/52 [01:17<00:00,  1.49s/it, loss=2.99, v_num=0, train_loss=2.950, valid_loss=2.660]\n","Epoch 6: 100% 52/52 [01:18<00:00,  1.52s/it, loss=2.99, v_num=0, train_loss=3.020, valid_loss=2.640]\n","Epoch 7: 100% 52/52 [01:07<00:00,  1.29s/it, loss=2.97, v_num=0, train_loss=2.990, valid_loss=2.640]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 7: 100% 52/52 [01:17<00:00,  1.49s/it, loss=2.97, v_num=0, train_loss=2.990, valid_loss=2.640]\n","Epoch 7: 100% 52/52 [01:19<00:00,  1.53s/it, loss=2.97, v_num=0, train_loss=2.990, valid_loss=2.600]\n","Epoch 8: 100% 52/52 [01:06<00:00,  1.28s/it, loss=2.95, v_num=0, train_loss=2.900, valid_loss=2.600]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 8: 100% 52/52 [01:17<00:00,  1.50s/it, loss=2.95, v_num=0, train_loss=2.900, valid_loss=2.600]\n","Epoch 8: 100% 52/52 [01:19<00:00,  1.53s/it, loss=2.94, v_num=0, train_loss=2.880, valid_loss=2.580]\n","Epoch 9: 100% 52/52 [01:06<00:00,  1.28s/it, loss=2.91, v_num=0, train_loss=2.900, valid_loss=2.580]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 9: 100% 52/52 [01:18<00:00,  1.50s/it, loss=2.91, v_num=0, train_loss=2.900, valid_loss=2.580]\n","Epoch 9: 100% 52/52 [01:18<00:00,  1.51s/it, loss=2.92, v_num=0, train_loss=2.890, valid_loss=2.560]\n","Epoch 10: 100% 52/52 [01:06<00:00,  1.28s/it, loss=2.88, v_num=0, train_loss=2.900, valid_loss=2.560]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/6 [00:00<?, ?it/s]\u001b[A\n","Epoch 10: 100% 52/52 [01:18<00:00,  1.51s/it, loss=2.89, v_num=0, train_loss=2.970, valid_loss=2.550]\n","Epoch 11: 100% 52/52 [01:06<00:00,  1.28s/it, loss=2.88, v_num=0, train_loss=2.850, valid_loss=2.550]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/6 [00:00<?, ?it/s]\u001b[A\n","Epoch 11: 100% 52/52 [01:18<00:00,  1.51s/it, loss=2.88, v_num=0, train_loss=2.880, valid_loss=2.550]\n","Epoch 12: 100% 52/52 [01:06<00:00,  1.29s/it, loss=2.88, v_num=0, train_loss=2.860, valid_loss=2.550]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/6 [00:00<?, ?it/s]\u001b[A\n","Epoch 12: 100% 52/52 [01:19<00:00,  1.54s/it, loss=2.89, v_num=0, train_loss=2.920, valid_loss=2.550]\n","Epoch 13: 100% 52/52 [01:08<00:00,  1.31s/it, loss=2.87, v_num=0, train_loss=2.840, valid_loss=2.550]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/6 [00:00<?, ?it/s]\u001b[A\n","Epoch 13: 100% 52/52 [01:19<00:00,  1.53s/it, loss=2.88, v_num=0, train_loss=2.940, valid_loss=2.540]\n","Epoch 14: 100% 52/52 [01:08<00:00,  1.31s/it, loss=2.88, v_num=0, train_loss=2.880, valid_loss=2.540]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/6 [00:00<?, ?it/s]\u001b[A\n","Epoch 14: 100% 52/52 [01:19<00:00,  1.53s/it, loss=2.88, v_num=0, train_loss=2.830, valid_loss=2.540]\n","Epoch 14: 100% 52/52 [01:19<00:00,  1.53s/it, loss=2.88, v_num=0, train_loss=2.830, valid_loss=2.540]\n","End training normal feature distribution from neighbrohood information\n","Start training normal feature distribution from position information\n","Using cache found in /root/.cache/torch/hub/pytorch_vision_v0.10.0\n","Using cache found in /root/.cache/torch/hub/pytorch_vision_v0.10.0\n","End training normal feature distribution from position information\n","Start evaluating anomaly score\n","GPU available: True, used: True\n","TPU available: False, using: 0 TPU cores\n","IPU available: False, using: 0 IPUs\n","Using cache found in /root/.cache/torch/hub/pytorch_vision_v0.10.0\n","LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n","/usr/local/lib/python3.8/dist-packages/pytorch_lightning/trainer/data_loading.py:132: UserWarning: The dataloader, test_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n","  rank_zero_warn(\n","Testing: 100% 1/1 [00:29<00:00, 29.37s/it]--------------------------------------------------------------------------------\n","DATALOADER:0 TEST RESULTS\n","{'pixel_auc_coor': 0.9797993328549601,\n"," 'pixel_auc_nb': 0.9826324312130356,\n"," 'pixel_auc_nb_coor': 0.9841887173517075,\n"," 'pixel_auc_patchcore': 0.9850289657364366}\n","--------------------------------------------------------------------------------\n","Testing: 100% 1/1 [00:29<00:00, 29.37s/it]\n","End evaluating anomaly score\n","Global seed set to 23\n","12\n","12\n","['../dataset/UAD/artefacts/test/anomaly_images/file_brain_AXT1_201_6002783.png', '../dataset/UAD/artefacts/test/anomaly_images/file_brain_AXT1_201_6002796.png', '../dataset/UAD/artefacts/test/anomaly_images/file_brain_AXT1_201_6002811.png', '../dataset/UAD/artefacts/test/anomaly_images/file_brain_AXT1_201_6002847.png', '../dataset/UAD/artefacts/test/anomaly_images/file_brain_AXT1_201_6002897.png', '../dataset/UAD/artefacts/test/anomaly_images/file_brain_AXT1_202_2020086.png', '../dataset/UAD/artefacts/test/anomaly_images/file_brain_AXT1_202_2020111.png', '../dataset/UAD/artefacts/test/anomaly_images/file_brain_AXT1_202_2020129.png', '../dataset/UAD/artefacts/test/anomaly_images/file_brain_AXT1_202_2020156.png', '../dataset/UAD/artefacts/test/anomaly_images/file_brain_AXT1_202_2020264.png', '../dataset/UAD/artefacts/test/anomaly_images/file_brain_AXT1_202_2020570.png', '../dataset/UAD/artefacts/test/anomaly_images/file_brain_AXT1_202_6000416.png']\n","['../dataset/UAD/artefacts/ground_truth/anomaly_images/file_brain_AXT1_201_6002783_possible_artifact_0.png', '../dataset/UAD/artefacts/ground_truth/anomaly_images/file_brain_AXT1_201_6002796_possible_artifact_1.png', '../dataset/UAD/artefacts/ground_truth/anomaly_images/file_brain_AXT1_201_6002811_combined.png', '../dataset/UAD/artefacts/ground_truth/anomaly_images/file_brain_AXT1_201_6002847_possible_artifact_0.png', '../dataset/UAD/artefacts/ground_truth/anomaly_images/file_brain_AXT1_201_6002897_combined.png', '../dataset/UAD/artefacts/ground_truth/anomaly_images/file_brain_AXT1_202_2020086_possible_artifact_0.png', '../dataset/UAD/artefacts/ground_truth/anomaly_images/file_brain_AXT1_202_2020111_possible_artifact_1.png', '../dataset/UAD/artefacts/ground_truth/anomaly_images/file_brain_AXT1_202_2020129_combined.png', '../dataset/UAD/artefacts/ground_truth/anomaly_images/file_brain_AXT1_202_2020156_possible_artifact_1.png', '../dataset/UAD/artefacts/ground_truth/anomaly_images/file_brain_AXT1_202_2020264_possible_artifact_1.png', '../dataset/UAD/artefacts/ground_truth/anomaly_images/file_brain_AXT1_202_2020570_possible_artifact_1.png', '../dataset/UAD/artefacts/ground_truth/anomaly_images/file_brain_AXT1_202_6000416_possible_artifact_1.png']\n","Start generating embeeding coreset and distribution coreset\n","GPU available: True, used: True\n","TPU available: False, using: 0 TPU cores\n","IPU available: False, using: 0 IPUs\n","coreset_generator_trainer完了\n","Using cache found in /root/.cache/torch/hub/pytorch_vision_v0.10.0\n","/usr/local/lib/python3.8/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and will be removed in 0.15, please use 'weights' instead.\n","  warnings.warn(\n","/usr/local/lib/python3.8/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and will be removed in 0.15. The current behavior is equivalent to passing `weights=Wide_ResNet101_2_Weights.IMAGENET1K_V1`. You can also use `weights=Wide_ResNet101_2_Weights.DEFAULT` to get the most up-to-date weights.\n","  warnings.warn(msg)\n","coreset_generator完了\n","LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n","/usr/local/lib/python3.8/dist-packages/pytorch_lightning/trainer/optimizers.py:37: UserWarning: `LightningModule.configure_optimizers` returned `None`, this fit will run with no optimizer\n","  rank_zero_warn(\n","Missing logger folder: ./result/artefacts/WR101/coreset/lightning_logs\n","\n","  | Name     | Type   | Params\n","------------------------------------\n","0 | backbone | ResNet | 126 M \n","------------------------------------\n","126 M     Trainable params\n","0         Non-trainable params\n","126 M     Total params\n","507.547   Total estimated model params size (MB)\n","/usr/local/lib/python3.8/dist-packages/pytorch_lightning/trainer/data_loading.py:132: UserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n","  rank_zero_warn(\n","Epoch 0:   0% 0/726 [00:00<?, ?it/s] /usr/local/lib/python3.8/dist-packages/pytorch_lightning/loops/optimization/optimizer_loop.py:145: UserWarning: `training_step` returned `None`. If this was on purpose, ignore this warning...\n","  self.warning_cache.warn(\n","Epoch 0:  99% 720/726 [00:33<00:00, 21.44it/s, loss=nan, v_num=0]Size of embedding coreset without edge feature :  (1045, 1024)\n","Size of distribution coreset without edge feature :  (2048, 1024)\n","Epoch 0:  99% 720/726 [00:50<00:00, 14.40it/s, loss=nan, v_num=0]Size of embedding coreset with edge feature :  (1858, 1024)\n","Epoch 0: 100% 726/726 [00:57<00:00, 12.55it/s, loss=nan, v_num=0]\n","End generating embeeding coreset and distribution coreset\n","Using cache found in /root/.cache/torch/hub/pytorch_vision_v0.10.0\n","/usr/local/lib/python3.8/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and will be removed in 0.15, please use 'weights' instead.\n","  warnings.warn(\n","/usr/local/lib/python3.8/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and will be removed in 0.15. The current behavior is equivalent to passing `weights=Wide_ResNet101_2_Weights.IMAGENET1K_V1`. You can also use `weights=Wide_ResNet101_2_Weights.DEFAULT` to get the most up-to-date weights.\n","  warnings.warn(msg)\n","Start training normal feature distribution from neighbrohood information\n","GPU available: True, used: True\n","TPU available: False, using: 0 TPU cores\n","IPU available: False, using: 0 IPUs\n","LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n","Missing logger folder: ./result/artefacts/WR101/distribution\n","\n","  | Name  | Type               | Params\n","---------------------------------------------\n","0 | model | Distribution_Model | 205 M \n","---------------------------------------------\n","205 M     Trainable params\n","0         Non-trainable params\n","205 M     Total params\n","822.313   Total estimated model params size (MB)\n","Validation sanity check: 0it [00:00, ?it/s]/usr/local/lib/python3.8/dist-packages/pytorch_lightning/trainer/data_loading.py:132: UserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n","  rank_zero_warn(\n","Global seed set to 23\n","/usr/local/lib/python3.8/dist-packages/pytorch_lightning/trainer/data_loading.py:132: UserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n","  rank_zero_warn(\n","Epoch 0: 100% 52/52 [00:56<00:00,  1.09s/it, loss=6.32, v_num=0, train_loss=5.890]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 0: 100% 52/52 [01:07<00:00,  1.30s/it, loss=6.32, v_num=0, train_loss=5.890]\n","Epoch 0: 100% 52/52 [01:07<00:00,  1.30s/it, loss=6.04, v_num=0, train_loss=5.670, valid_loss=5.780]\n","Epoch 1: 100% 52/52 [00:55<00:00,  1.06s/it, loss=4.68, v_num=0, train_loss=4.470, valid_loss=5.780]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/6 [00:00<?, ?it/s]\u001b[A\n","Epoch 1: 100% 52/52 [01:05<00:00,  1.26s/it, loss=4.53, v_num=0, train_loss=4.410, valid_loss=4.090]\n","Epoch 2: 100% 52/52 [00:55<00:00,  1.06s/it, loss=3.93, v_num=0, train_loss=3.830, valid_loss=4.090]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/6 [00:00<?, ?it/s]\u001b[A\n","Epoch 2: 100% 52/52 [01:06<00:00,  1.28s/it, loss=3.86, v_num=0, train_loss=3.790, valid_loss=3.530]\n","Epoch 3: 100% 52/52 [00:55<00:00,  1.06s/it, loss=3.55, v_num=0, train_loss=3.520, valid_loss=3.530]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/6 [00:00<?, ?it/s]\u001b[A\n","Epoch 3: 100% 52/52 [01:05<00:00,  1.26s/it, loss=3.51, v_num=0, train_loss=3.500, valid_loss=3.160]\n","Epoch 4: 100% 52/52 [00:55<00:00,  1.07s/it, loss=3.29, v_num=0, train_loss=3.250, valid_loss=3.160]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/6 [00:00<?, ?it/s]\u001b[A\n","Epoch 4: 100% 52/52 [01:05<00:00,  1.26s/it, loss=3.27, v_num=0, train_loss=3.260, valid_loss=2.840]\n","Epoch 5: 100% 52/52 [00:54<00:00,  1.06s/it, loss=3.04, v_num=0, train_loss=2.970, valid_loss=2.840]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/6 [00:00<?, ?it/s]\u001b[A\n","Epoch 5: 100% 52/52 [01:04<00:00,  1.25s/it, loss=3.04, v_num=0, train_loss=3.020, valid_loss=2.660]\n","Epoch 6: 100% 52/52 [00:56<00:00,  1.08s/it, loss=2.99, v_num=0, train_loss=2.950, valid_loss=2.660]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/6 [00:00<?, ?it/s]\u001b[A\n","Epoch 6: 100% 52/52 [01:07<00:00,  1.31s/it, loss=2.99, v_num=0, train_loss=3.020, valid_loss=2.640]\n","Epoch 7: 100% 52/52 [00:55<00:00,  1.08s/it, loss=2.97, v_num=0, train_loss=2.990, valid_loss=2.640]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/6 [00:00<?, ?it/s]\u001b[A\n","Epoch 7: 100% 52/52 [01:06<00:00,  1.27s/it, loss=2.97, v_num=0, train_loss=2.990, valid_loss=2.600]\n","Epoch 8: 100% 52/52 [00:55<00:00,  1.06s/it, loss=2.95, v_num=0, train_loss=2.900, valid_loss=2.600]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/6 [00:00<?, ?it/s]\u001b[A\n","Epoch 8: 100% 52/52 [01:05<00:00,  1.26s/it, loss=2.94, v_num=0, train_loss=2.880, valid_loss=2.580]\n","Epoch 9: 100% 52/52 [00:55<00:00,  1.07s/it, loss=2.91, v_num=0, train_loss=2.900, valid_loss=2.580]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/6 [00:00<?, ?it/s]\u001b[A\n","Epoch 9: 100% 52/52 [01:06<00:00,  1.28s/it, loss=2.92, v_num=0, train_loss=2.890, valid_loss=2.560]\n","Epoch 10: 100% 52/52 [00:55<00:00,  1.07s/it, loss=2.88, v_num=0, train_loss=2.900, valid_loss=2.560]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/6 [00:00<?, ?it/s]\u001b[A\n","Epoch 10: 100% 52/52 [01:05<00:00,  1.26s/it, loss=2.89, v_num=0, train_loss=2.970, valid_loss=2.550]\n","Epoch 11: 100% 52/52 [00:55<00:00,  1.07s/it, loss=2.88, v_num=0, train_loss=2.850, valid_loss=2.550]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/6 [00:00<?, ?it/s]\u001b[A\n","Epoch 11: 100% 52/52 [01:05<00:00,  1.25s/it, loss=2.88, v_num=0, train_loss=2.880, valid_loss=2.550]\n","Epoch 12: 100% 52/52 [00:55<00:00,  1.06s/it, loss=2.88, v_num=0, train_loss=2.860, valid_loss=2.550]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/6 [00:00<?, ?it/s]\u001b[A\n","Epoch 12: 100% 52/52 [01:05<00:00,  1.25s/it, loss=2.89, v_num=0, train_loss=2.920, valid_loss=2.550]\n","Epoch 13: 100% 52/52 [00:55<00:00,  1.07s/it, loss=2.87, v_num=0, train_loss=2.840, valid_loss=2.550]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/6 [00:00<?, ?it/s]\u001b[A\n","Epoch 13: 100% 52/52 [01:05<00:00,  1.26s/it, loss=2.88, v_num=0, train_loss=2.940, valid_loss=2.540]\n","Epoch 14: 100% 52/52 [00:55<00:00,  1.07s/it, loss=2.88, v_num=0, train_loss=2.880, valid_loss=2.540]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/6 [00:00<?, ?it/s]\u001b[A\n","Epoch 14: 100% 52/52 [01:05<00:00,  1.26s/it, loss=2.88, v_num=0, train_loss=2.830, valid_loss=2.540]\n","Epoch 14: 100% 52/52 [01:05<00:00,  1.26s/it, loss=2.88, v_num=0, train_loss=2.830, valid_loss=2.540]\n","End training normal feature distribution from neighbrohood information\n","Start training normal feature distribution from position information\n","Using cache found in /root/.cache/torch/hub/pytorch_vision_v0.10.0\n","Using cache found in /root/.cache/torch/hub/pytorch_vision_v0.10.0\n","End training normal feature distribution from position information\n","Start evaluating anomaly score\n","GPU available: True, used: True\n","TPU available: False, using: 0 TPU cores\n","IPU available: False, using: 0 IPUs\n","Using cache found in /root/.cache/torch/hub/pytorch_vision_v0.10.0\n","LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n","/usr/local/lib/python3.8/dist-packages/pytorch_lightning/trainer/data_loading.py:132: UserWarning: The dataloader, test_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n","  rank_zero_warn(\n","Testing: 100% 12/12 [01:03<00:00,  5.28s/it]--------------------------------------------------------------------------------\n","DATALOADER:0 TEST RESULTS\n","{'pixel_auc_coor': 0.8857217711290912,\n"," 'pixel_auc_nb': 0.8743403942812733,\n"," 'pixel_auc_nb_coor': 0.8710149567258754,\n"," 'pixel_auc_patchcore': 0.8829701167517545}\n","--------------------------------------------------------------------------------\n","Testing: 100% 12/12 [01:03<00:00,  5.28s/it]\n","End evaluating anomaly score\n","Global seed set to 23\n","14\n","14\n","['../dataset/UAD/craniatomy/test/anomaly_images/file_brain_AXT1_201_6002796.png', '../dataset/UAD/craniatomy/test/anomaly_images/file_brain_AXT1_201_6002885.png', '../dataset/UAD/craniatomy/test/anomaly_images/file_brain_AXT1_202_2020009.png', '../dataset/UAD/craniatomy/test/anomaly_images/file_brain_AXT1_202_2020026.png', '../dataset/UAD/craniatomy/test/anomaly_images/file_brain_AXT1_202_2020066.png', '../dataset/UAD/craniatomy/test/anomaly_images/file_brain_AXT1_202_2020203.png', '../dataset/UAD/craniatomy/test/anomaly_images/file_brain_AXT1_202_2020334.png', '../dataset/UAD/craniatomy/test/anomaly_images/file_brain_AXT1_202_2020486.png', '../dataset/UAD/craniatomy/test/anomaly_images/file_brain_AXT1_202_2020518.png', '../dataset/UAD/craniatomy/test/anomaly_images/file_brain_AXT1_202_6000382.png', '../dataset/UAD/craniatomy/test/anomaly_images/file_brain_AXT1_202_6000448.png', '../dataset/UAD/craniatomy/test/anomaly_images/file_brain_AXT1_202_6000505.png', '../dataset/UAD/craniatomy/test/anomaly_images/file_brain_AXT1_202_6000518.png', '../dataset/UAD/craniatomy/test/anomaly_images/file_brain_AXT1_206_2060085.png']\n","['../dataset/UAD/craniatomy/ground_truth/anomaly_images/file_brain_AXT1_201_6002796_craniotomy_0.png', '../dataset/UAD/craniatomy/ground_truth/anomaly_images/file_brain_AXT1_201_6002885_craniotomy_0.png', '../dataset/UAD/craniatomy/ground_truth/anomaly_images/file_brain_AXT1_202_2020009_craniectomy_0.png', '../dataset/UAD/craniatomy/ground_truth/anomaly_images/file_brain_AXT1_202_2020026_craniotomy_0.png', '../dataset/UAD/craniatomy/ground_truth/anomaly_images/file_brain_AXT1_202_2020066_craniotomy_0.png', '../dataset/UAD/craniatomy/ground_truth/anomaly_images/file_brain_AXT1_202_2020203_craniotomy_0.png', '../dataset/UAD/craniatomy/ground_truth/anomaly_images/file_brain_AXT1_202_2020334_craniotomy_0.png', '../dataset/UAD/craniatomy/ground_truth/anomaly_images/file_brain_AXT1_202_2020486_craniotomy_0.png', '../dataset/UAD/craniatomy/ground_truth/anomaly_images/file_brain_AXT1_202_2020518_craniotomy_0.png', '../dataset/UAD/craniatomy/ground_truth/anomaly_images/file_brain_AXT1_202_6000382_craniotomy_0.png', '../dataset/UAD/craniatomy/ground_truth/anomaly_images/file_brain_AXT1_202_6000448_craniotomy_0.png', '../dataset/UAD/craniatomy/ground_truth/anomaly_images/file_brain_AXT1_202_6000505_craniotomy_0.png', '../dataset/UAD/craniatomy/ground_truth/anomaly_images/file_brain_AXT1_202_6000518_combined.png', '../dataset/UAD/craniatomy/ground_truth/anomaly_images/file_brain_AXT1_206_2060085_craniotomy_0.png']\n","Start generating embeeding coreset and distribution coreset\n","GPU available: True, used: True\n","TPU available: False, using: 0 TPU cores\n","IPU available: False, using: 0 IPUs\n","coreset_generator_trainer完了\n","Using cache found in /root/.cache/torch/hub/pytorch_vision_v0.10.0\n","/usr/local/lib/python3.8/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and will be removed in 0.15, please use 'weights' instead.\n","  warnings.warn(\n","/usr/local/lib/python3.8/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and will be removed in 0.15. The current behavior is equivalent to passing `weights=Wide_ResNet101_2_Weights.IMAGENET1K_V1`. You can also use `weights=Wide_ResNet101_2_Weights.DEFAULT` to get the most up-to-date weights.\n","  warnings.warn(msg)\n","coreset_generator完了\n","LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n","/usr/local/lib/python3.8/dist-packages/pytorch_lightning/trainer/optimizers.py:37: UserWarning: `LightningModule.configure_optimizers` returned `None`, this fit will run with no optimizer\n","  rank_zero_warn(\n","Missing logger folder: ./result/craniatomy/WR101/coreset/lightning_logs\n","\n","  | Name     | Type   | Params\n","------------------------------------\n","0 | backbone | ResNet | 126 M \n","------------------------------------\n","126 M     Trainable params\n","0         Non-trainable params\n","126 M     Total params\n","507.547   Total estimated model params size (MB)\n","/usr/local/lib/python3.8/dist-packages/pytorch_lightning/trainer/data_loading.py:132: UserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n","  rank_zero_warn(\n","Epoch 0:   0% 0/726 [00:00<?, ?it/s] /usr/local/lib/python3.8/dist-packages/pytorch_lightning/loops/optimization/optimizer_loop.py:145: UserWarning: `training_step` returned `None`. If this was on purpose, ignore this warning...\n","  self.warning_cache.warn(\n","Epoch 0:  99% 720/726 [00:35<00:00, 20.55it/s, loss=nan, v_num=0]Size of embedding coreset without edge feature :  (1045, 1024)\n","Size of distribution coreset without edge feature :  (2048, 1024)\n","Epoch 0:  99% 720/726 [00:49<00:00, 14.40it/s, loss=nan, v_num=0]Size of embedding coreset with edge feature :  (1858, 1024)\n","Epoch 0: 100% 726/726 [00:59<00:00, 12.18it/s, loss=nan, v_num=0]\n","End generating embeeding coreset and distribution coreset\n","Using cache found in /root/.cache/torch/hub/pytorch_vision_v0.10.0\n","/usr/local/lib/python3.8/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and will be removed in 0.15, please use 'weights' instead.\n","  warnings.warn(\n","/usr/local/lib/python3.8/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and will be removed in 0.15. The current behavior is equivalent to passing `weights=Wide_ResNet101_2_Weights.IMAGENET1K_V1`. You can also use `weights=Wide_ResNet101_2_Weights.DEFAULT` to get the most up-to-date weights.\n","  warnings.warn(msg)\n","Start training normal feature distribution from neighbrohood information\n","GPU available: True, used: True\n","TPU available: False, using: 0 TPU cores\n","IPU available: False, using: 0 IPUs\n","LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n","Missing logger folder: ./result/craniatomy/WR101/distribution\n","\n","  | Name  | Type               | Params\n","---------------------------------------------\n","0 | model | Distribution_Model | 205 M \n","---------------------------------------------\n","205 M     Trainable params\n","0         Non-trainable params\n","205 M     Total params\n","822.313   Total estimated model params size (MB)\n","Validation sanity check: 0it [00:00, ?it/s]/usr/local/lib/python3.8/dist-packages/pytorch_lightning/trainer/data_loading.py:132: UserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n","  rank_zero_warn(\n","Global seed set to 23\n","/usr/local/lib/python3.8/dist-packages/pytorch_lightning/trainer/data_loading.py:132: UserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n","  rank_zero_warn(\n","Epoch 0: 100% 52/52 [01:08<00:00,  1.32s/it, loss=6.32, v_num=0, train_loss=5.890]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/6 [00:00<?, ?it/s]\u001b[A\n","Epoch 0: 100% 52/52 [01:20<00:00,  1.54s/it, loss=6.04, v_num=0, train_loss=5.670, valid_loss=5.780]\n","Epoch 1: 100% 52/52 [01:08<00:00,  1.31s/it, loss=4.68, v_num=0, train_loss=4.470, valid_loss=5.780]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/6 [00:00<?, ?it/s]\u001b[A\n","Epoch 1: 100% 52/52 [01:19<00:00,  1.53s/it, loss=4.53, v_num=0, train_loss=4.410, valid_loss=4.090]\n","Epoch 2: 100% 52/52 [01:07<00:00,  1.31s/it, loss=3.93, v_num=0, train_loss=3.830, valid_loss=4.090]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/6 [00:00<?, ?it/s]\u001b[A\n","Epoch 2: 100% 52/52 [01:19<00:00,  1.53s/it, loss=3.86, v_num=0, train_loss=3.790, valid_loss=3.530]\n","Epoch 3: 100% 52/52 [01:08<00:00,  1.32s/it, loss=3.55, v_num=0, train_loss=3.520, valid_loss=3.530]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/6 [00:00<?, ?it/s]\u001b[A\n","Epoch 3: 100% 52/52 [01:20<00:00,  1.54s/it, loss=3.51, v_num=0, train_loss=3.500, valid_loss=3.160]\n","Epoch 4: 100% 52/52 [01:08<00:00,  1.31s/it, loss=3.29, v_num=0, train_loss=3.250, valid_loss=3.160]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/6 [00:00<?, ?it/s]\u001b[A\n","Epoch 4: 100% 52/52 [01:19<00:00,  1.53s/it, loss=3.27, v_num=0, train_loss=3.260, valid_loss=2.840]\n","Epoch 5: 100% 52/52 [01:08<00:00,  1.32s/it, loss=3.04, v_num=0, train_loss=2.970, valid_loss=2.840]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/6 [00:00<?, ?it/s]\u001b[A\n","Epoch 5: 100% 52/52 [01:19<00:00,  1.53s/it, loss=3.04, v_num=0, train_loss=3.020, valid_loss=2.660]\n","Epoch 6: 100% 52/52 [01:08<00:00,  1.32s/it, loss=2.99, v_num=0, train_loss=2.950, valid_loss=2.660]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/6 [00:00<?, ?it/s]\u001b[A\n","Epoch 6: 100% 52/52 [01:20<00:00,  1.54s/it, loss=2.99, v_num=0, train_loss=3.020, valid_loss=2.640]\n","Epoch 7: 100% 52/52 [01:08<00:00,  1.31s/it, loss=2.97, v_num=0, train_loss=2.990, valid_loss=2.640]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/6 [00:00<?, ?it/s]\u001b[A\n","Epoch 7: 100% 52/52 [01:20<00:00,  1.55s/it, loss=2.97, v_num=0, train_loss=2.990, valid_loss=2.600]\n","Epoch 8: 100% 52/52 [01:08<00:00,  1.31s/it, loss=2.95, v_num=0, train_loss=2.900, valid_loss=2.600]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/6 [00:00<?, ?it/s]\u001b[A\n","Epoch 8: 100% 52/52 [01:19<00:00,  1.52s/it, loss=2.94, v_num=0, train_loss=2.880, valid_loss=2.580]\n","Epoch 9: 100% 52/52 [01:08<00:00,  1.32s/it, loss=2.91, v_num=0, train_loss=2.900, valid_loss=2.580]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/6 [00:00<?, ?it/s]\u001b[A\n","Epoch 9: 100% 52/52 [01:20<00:00,  1.56s/it, loss=2.92, v_num=0, train_loss=2.890, valid_loss=2.560]\n","Epoch 10: 100% 52/52 [01:08<00:00,  1.32s/it, loss=2.88, v_num=0, train_loss=2.900, valid_loss=2.560]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/6 [00:00<?, ?it/s]\u001b[A\n","Epoch 10: 100% 52/52 [01:21<00:00,  1.57s/it, loss=2.89, v_num=0, train_loss=2.970, valid_loss=2.550]\n","Epoch 11: 100% 52/52 [01:08<00:00,  1.31s/it, loss=2.88, v_num=0, train_loss=2.850, valid_loss=2.550]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/6 [00:00<?, ?it/s]\u001b[A\n","Epoch 11: 100% 52/52 [01:19<00:00,  1.53s/it, loss=2.88, v_num=0, train_loss=2.880, valid_loss=2.550]\n","Epoch 12: 100% 52/52 [01:07<00:00,  1.31s/it, loss=2.88, v_num=0, train_loss=2.860, valid_loss=2.550]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/6 [00:00<?, ?it/s]\u001b[A\n","Epoch 12: 100% 52/52 [01:21<00:00,  1.56s/it, loss=2.89, v_num=0, train_loss=2.920, valid_loss=2.550]\n","Epoch 13: 100% 52/52 [01:07<00:00,  1.30s/it, loss=2.87, v_num=0, train_loss=2.840, valid_loss=2.550]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/6 [00:00<?, ?it/s]\u001b[A\n","Epoch 13: 100% 52/52 [01:19<00:00,  1.54s/it, loss=2.88, v_num=0, train_loss=2.940, valid_loss=2.540]\n","Epoch 14: 100% 52/52 [01:08<00:00,  1.31s/it, loss=2.88, v_num=0, train_loss=2.880, valid_loss=2.540]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/6 [00:00<?, ?it/s]\u001b[A\n","Epoch 14: 100% 52/52 [01:19<00:00,  1.54s/it, loss=2.88, v_num=0, train_loss=2.830, valid_loss=2.540]\n","Epoch 14: 100% 52/52 [01:19<00:00,  1.54s/it, loss=2.88, v_num=0, train_loss=2.830, valid_loss=2.540]\n","End training normal feature distribution from neighbrohood information\n","Start training normal feature distribution from position information\n","Using cache found in /root/.cache/torch/hub/pytorch_vision_v0.10.0\n","Using cache found in /root/.cache/torch/hub/pytorch_vision_v0.10.0\n","End training normal feature distribution from position information\n","Start evaluating anomaly score\n","GPU available: True, used: True\n","TPU available: False, using: 0 TPU cores\n","IPU available: False, using: 0 IPUs\n","Using cache found in /root/.cache/torch/hub/pytorch_vision_v0.10.0\n","LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n","/usr/local/lib/python3.8/dist-packages/pytorch_lightning/trainer/data_loading.py:132: UserWarning: The dataloader, test_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n","  rank_zero_warn(\n","Testing: 100% 14/14 [01:17<00:00,  5.56s/it]--------------------------------------------------------------------------------\n","DATALOADER:0 TEST RESULTS\n","{'pixel_auc_coor': 0.8513396490319627,\n"," 'pixel_auc_nb': 0.8364726848531984,\n"," 'pixel_auc_nb_coor': 0.8336168778996507,\n"," 'pixel_auc_patchcore': 0.8621621433201231}\n","--------------------------------------------------------------------------------\n","Testing: 100% 14/14 [01:17<00:00,  5.56s/it]\n","End evaluating anomaly score\n","Global seed set to 23\n","6\n","6\n","['../dataset/UAD/dural/test/anomaly_images/file_brain_AXT1_202_2020156.png', '../dataset/UAD/dural/test/anomaly_images/file_brain_AXT1_202_2020256.png', '../dataset/UAD/dural/test/anomaly_images/file_brain_AXT1_202_2020264.png', '../dataset/UAD/dural/test/anomaly_images/file_brain_AXT1_202_2020334.png', '../dataset/UAD/dural/test/anomaly_images/file_brain_AXT1_206_2060066.png', '../dataset/UAD/dural/test/anomaly_images/file_brain_AXT1_206_2060085.png']\n","['../dataset/UAD/dural/ground_truth/anomaly_images/file_brain_AXT1_202_2020156_dural_thickening_0.png', '../dataset/UAD/dural/ground_truth/anomaly_images/file_brain_AXT1_202_2020256_dural_thickening_0.png', '../dataset/UAD/dural/ground_truth/anomaly_images/file_brain_AXT1_202_2020264_dural_thickening_0.png', '../dataset/UAD/dural/ground_truth/anomaly_images/file_brain_AXT1_202_2020334_dural_thickening_1.png', '../dataset/UAD/dural/ground_truth/anomaly_images/file_brain_AXT1_206_2060066_combined.png', '../dataset/UAD/dural/ground_truth/anomaly_images/file_brain_AXT1_206_2060085_dural_thickening_1.png']\n","Start generating embeeding coreset and distribution coreset\n","GPU available: True, used: True\n","TPU available: False, using: 0 TPU cores\n","IPU available: False, using: 0 IPUs\n","coreset_generator_trainer完了\n","Using cache found in /root/.cache/torch/hub/pytorch_vision_v0.10.0\n","/usr/local/lib/python3.8/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and will be removed in 0.15, please use 'weights' instead.\n","  warnings.warn(\n","/usr/local/lib/python3.8/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and will be removed in 0.15. The current behavior is equivalent to passing `weights=Wide_ResNet101_2_Weights.IMAGENET1K_V1`. You can also use `weights=Wide_ResNet101_2_Weights.DEFAULT` to get the most up-to-date weights.\n","  warnings.warn(msg)\n","coreset_generator完了\n","LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n","/usr/local/lib/python3.8/dist-packages/pytorch_lightning/trainer/optimizers.py:37: UserWarning: `LightningModule.configure_optimizers` returned `None`, this fit will run with no optimizer\n","  rank_zero_warn(\n","Missing logger folder: ./result/dural/WR101/coreset/lightning_logs\n","\n","  | Name     | Type   | Params\n","------------------------------------\n","0 | backbone | ResNet | 126 M \n","------------------------------------\n","126 M     Trainable params\n","0         Non-trainable params\n","126 M     Total params\n","507.547   Total estimated model params size (MB)\n","/usr/local/lib/python3.8/dist-packages/pytorch_lightning/trainer/data_loading.py:132: UserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n","  rank_zero_warn(\n","Epoch 0:   0% 0/726 [00:00<?, ?it/s] /usr/local/lib/python3.8/dist-packages/pytorch_lightning/loops/optimization/optimizer_loop.py:145: UserWarning: `training_step` returned `None`. If this was on purpose, ignore this warning...\n","  self.warning_cache.warn(\n","Epoch 0:  99% 720/726 [00:34<00:00, 20.71it/s, loss=nan, v_num=0]Size of embedding coreset without edge feature :  (1045, 1024)\n","Size of distribution coreset without edge feature :  (2048, 1024)\n","Epoch 0:  99% 720/726 [00:49<00:00, 14.40it/s, loss=nan, v_num=0]Size of embedding coreset with edge feature :  (1858, 1024)\n","Epoch 0: 100% 726/726 [00:59<00:00, 12.23it/s, loss=nan, v_num=0]\n","End generating embeeding coreset and distribution coreset\n","Using cache found in /root/.cache/torch/hub/pytorch_vision_v0.10.0\n","/usr/local/lib/python3.8/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and will be removed in 0.15, please use 'weights' instead.\n","  warnings.warn(\n","/usr/local/lib/python3.8/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and will be removed in 0.15. The current behavior is equivalent to passing `weights=Wide_ResNet101_2_Weights.IMAGENET1K_V1`. You can also use `weights=Wide_ResNet101_2_Weights.DEFAULT` to get the most up-to-date weights.\n","  warnings.warn(msg)\n","Start training normal feature distribution from neighbrohood information\n","GPU available: True, used: True\n","TPU available: False, using: 0 TPU cores\n","IPU available: False, using: 0 IPUs\n","LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n","Missing logger folder: ./result/dural/WR101/distribution\n","\n","  | Name  | Type               | Params\n","---------------------------------------------\n","0 | model | Distribution_Model | 205 M \n","---------------------------------------------\n","205 M     Trainable params\n","0         Non-trainable params\n","205 M     Total params\n","822.313   Total estimated model params size (MB)\n","Validation sanity check: 0it [00:00, ?it/s]/usr/local/lib/python3.8/dist-packages/pytorch_lightning/trainer/data_loading.py:132: UserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n","  rank_zero_warn(\n","Global seed set to 23\n","/usr/local/lib/python3.8/dist-packages/pytorch_lightning/trainer/data_loading.py:132: UserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n","  rank_zero_warn(\n","Epoch 0: 100% 52/52 [01:12<00:00,  1.39s/it, loss=6.32, v_num=0, train_loss=5.890]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/6 [00:00<?, ?it/s]\u001b[A\n","Epoch 0: 100% 52/52 [01:24<00:00,  1.62s/it, loss=6.04, v_num=0, train_loss=5.670, valid_loss=5.780]\n","Epoch 1: 100% 52/52 [01:10<00:00,  1.36s/it, loss=4.68, v_num=0, train_loss=4.470, valid_loss=5.780]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 1: 100% 52/52 [01:20<00:00,  1.56s/it, loss=4.68, v_num=0, train_loss=4.470, valid_loss=5.780]\n","Epoch 1: 100% 52/52 [01:22<00:00,  1.58s/it, loss=4.53, v_num=0, train_loss=4.410, valid_loss=4.090]\n","Epoch 2: 100% 52/52 [01:10<00:00,  1.35s/it, loss=3.93, v_num=0, train_loss=3.830, valid_loss=4.090]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/6 [00:00<?, ?it/s]\u001b[A\n","Epoch 2: 100% 52/52 [01:23<00:00,  1.60s/it, loss=3.86, v_num=0, train_loss=3.790, valid_loss=3.530]\n","Epoch 3: 100% 52/52 [01:10<00:00,  1.36s/it, loss=3.55, v_num=0, train_loss=3.520, valid_loss=3.530]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/6 [00:00<?, ?it/s]\u001b[A\n","Epoch 3: 100% 52/52 [01:23<00:00,  1.61s/it, loss=3.51, v_num=0, train_loss=3.500, valid_loss=3.160]\n","Epoch 4: 100% 52/52 [01:10<00:00,  1.37s/it, loss=3.29, v_num=0, train_loss=3.250, valid_loss=3.160]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 4: 100% 52/52 [01:22<00:00,  1.59s/it, loss=3.29, v_num=0, train_loss=3.250, valid_loss=3.160]\n","Epoch 4: 100% 52/52 [01:22<00:00,  1.59s/it, loss=3.27, v_num=0, train_loss=3.260, valid_loss=2.840]\n","Epoch 5: 100% 52/52 [01:11<00:00,  1.37s/it, loss=3.04, v_num=0, train_loss=2.970, valid_loss=2.840]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/6 [00:00<?, ?it/s]\u001b[A\n","Epoch 5: 100% 52/52 [01:24<00:00,  1.62s/it, loss=3.04, v_num=0, train_loss=3.020, valid_loss=2.660]\n","Epoch 6: 100% 52/52 [01:11<00:00,  1.37s/it, loss=2.99, v_num=0, train_loss=2.950, valid_loss=2.660]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/6 [00:00<?, ?it/s]\u001b[A\n","Epoch 6: 100% 52/52 [01:22<00:00,  1.59s/it, loss=2.99, v_num=0, train_loss=3.020, valid_loss=2.640]\n","Epoch 7: 100% 52/52 [01:09<00:00,  1.34s/it, loss=2.97, v_num=0, train_loss=2.990, valid_loss=2.640]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/6 [00:00<?, ?it/s]\u001b[A\n","Epoch 7: 100% 52/52 [01:22<00:00,  1.58s/it, loss=2.97, v_num=0, train_loss=2.990, valid_loss=2.600]\n","Epoch 8: 100% 52/52 [01:10<00:00,  1.35s/it, loss=2.95, v_num=0, train_loss=2.900, valid_loss=2.600]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 8: 100% 52/52 [01:21<00:00,  1.57s/it, loss=2.95, v_num=0, train_loss=2.900, valid_loss=2.600]\n","Epoch 8: 100% 52/52 [01:25<00:00,  1.64s/it, loss=2.94, v_num=0, train_loss=2.880, valid_loss=2.580]\n","Epoch 9: 100% 52/52 [01:11<00:00,  1.38s/it, loss=2.91, v_num=0, train_loss=2.900, valid_loss=2.580]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/6 [00:00<?, ?it/s]\u001b[A\n","Epoch 9: 100% 52/52 [01:23<00:00,  1.60s/it, loss=2.92, v_num=0, train_loss=2.890, valid_loss=2.560]\n","Epoch 10: 100% 52/52 [01:10<00:00,  1.35s/it, loss=2.88, v_num=0, train_loss=2.900, valid_loss=2.560]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 10: 100% 52/52 [01:22<00:00,  1.59s/it, loss=2.88, v_num=0, train_loss=2.900, valid_loss=2.560]\n","Epoch 10: 100% 52/52 [01:25<00:00,  1.65s/it, loss=2.89, v_num=0, train_loss=2.970, valid_loss=2.550]\n","Epoch 11: 100% 52/52 [01:10<00:00,  1.35s/it, loss=2.88, v_num=0, train_loss=2.850, valid_loss=2.550]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/6 [00:00<?, ?it/s]\u001b[A\n","Epoch 11: 100% 52/52 [01:22<00:00,  1.58s/it, loss=2.88, v_num=0, train_loss=2.880, valid_loss=2.550]\n","Epoch 12: 100% 52/52 [01:10<00:00,  1.36s/it, loss=2.88, v_num=0, train_loss=2.860, valid_loss=2.550]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/6 [00:00<?, ?it/s]\u001b[A\n","Epoch 12: 100% 52/52 [01:22<00:00,  1.59s/it, loss=2.89, v_num=0, train_loss=2.920, valid_loss=2.550]\n","Epoch 13: 100% 52/52 [01:10<00:00,  1.36s/it, loss=2.87, v_num=0, train_loss=2.840, valid_loss=2.550]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 13: 100% 52/52 [01:22<00:00,  1.58s/it, loss=2.87, v_num=0, train_loss=2.840, valid_loss=2.550]\n","Epoch 13: 100% 52/52 [01:23<00:00,  1.61s/it, loss=2.88, v_num=0, train_loss=2.940, valid_loss=2.540]\n","Epoch 14: 100% 52/52 [01:10<00:00,  1.35s/it, loss=2.88, v_num=0, train_loss=2.880, valid_loss=2.540]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/6 [00:00<?, ?it/s]\u001b[A\n","Epoch 14: 100% 52/52 [01:23<00:00,  1.61s/it, loss=2.88, v_num=0, train_loss=2.830, valid_loss=2.540]\n","Epoch 14: 100% 52/52 [01:23<00:00,  1.61s/it, loss=2.88, v_num=0, train_loss=2.830, valid_loss=2.540]\n","End training normal feature distribution from neighbrohood information\n","Start training normal feature distribution from position information\n","Using cache found in /root/.cache/torch/hub/pytorch_vision_v0.10.0\n","Using cache found in /root/.cache/torch/hub/pytorch_vision_v0.10.0\n","End training normal feature distribution from position information\n","Start evaluating anomaly score\n","GPU available: True, used: True\n","TPU available: False, using: 0 TPU cores\n","IPU available: False, using: 0 IPUs\n","Using cache found in /root/.cache/torch/hub/pytorch_vision_v0.10.0\n","LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n","/usr/local/lib/python3.8/dist-packages/pytorch_lightning/trainer/data_loading.py:132: UserWarning: The dataloader, test_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n","  rank_zero_warn(\n","Testing: 100% 6/6 [00:37<00:00,  6.26s/it]--------------------------------------------------------------------------------\n","DATALOADER:0 TEST RESULTS\n","{'pixel_auc_coor': 0.7613520961589979,\n"," 'pixel_auc_nb': 0.6795744746169748,\n"," 'pixel_auc_nb_coor': 0.6750079128644474,\n"," 'pixel_auc_patchcore': 0.7674043568366045}\n","--------------------------------------------------------------------------------\n","Testing: 100% 6/6 [00:37<00:00,  6.26s/it]\n","End evaluating anomaly score\n","Global seed set to 23\n","3\n","3\n","['../dataset/UAD/ea_mass/test/anomaly_images/file_brain_AXT1_201_6002841.png', '../dataset/UAD/ea_mass/test/anomaly_images/file_brain_AXT1_202_2020129.png', '../dataset/UAD/ea_mass/test/anomaly_images/file_brain_AXT1_206_2060066.png']\n","['../dataset/UAD/ea_mass/ground_truth/anomaly_images/file_brain_AXT1_201_6002841_extra-axial_mass_0.png', '../dataset/UAD/ea_mass/ground_truth/anomaly_images/file_brain_AXT1_202_2020129_extra-axial_mass_0.png', '../dataset/UAD/ea_mass/ground_truth/anomaly_images/file_brain_AXT1_206_2060066_combined.png']\n","Start generating embeeding coreset and distribution coreset\n","GPU available: True, used: True\n","TPU available: False, using: 0 TPU cores\n","IPU available: False, using: 0 IPUs\n","coreset_generator_trainer完了\n","Using cache found in /root/.cache/torch/hub/pytorch_vision_v0.10.0\n","/usr/local/lib/python3.8/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and will be removed in 0.15, please use 'weights' instead.\n","  warnings.warn(\n","/usr/local/lib/python3.8/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and will be removed in 0.15. The current behavior is equivalent to passing `weights=Wide_ResNet101_2_Weights.IMAGENET1K_V1`. You can also use `weights=Wide_ResNet101_2_Weights.DEFAULT` to get the most up-to-date weights.\n","  warnings.warn(msg)\n","coreset_generator完了\n","LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n","/usr/local/lib/python3.8/dist-packages/pytorch_lightning/trainer/optimizers.py:37: UserWarning: `LightningModule.configure_optimizers` returned `None`, this fit will run with no optimizer\n","  rank_zero_warn(\n","Missing logger folder: ./result/ea_mass/WR101/coreset/lightning_logs\n","\n","  | Name     | Type   | Params\n","------------------------------------\n","0 | backbone | ResNet | 126 M \n","------------------------------------\n","126 M     Trainable params\n","0         Non-trainable params\n","126 M     Total params\n","507.547   Total estimated model params size (MB)\n","/usr/local/lib/python3.8/dist-packages/pytorch_lightning/trainer/data_loading.py:132: UserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n","  rank_zero_warn(\n","Epoch 0:   0% 0/726 [00:00<?, ?it/s] /usr/local/lib/python3.8/dist-packages/pytorch_lightning/loops/optimization/optimizer_loop.py:145: UserWarning: `training_step` returned `None`. If this was on purpose, ignore this warning...\n","  self.warning_cache.warn(\n","Epoch 0:  99% 720/726 [00:34<00:00, 20.97it/s, loss=nan, v_num=0]Size of embedding coreset without edge feature :  (1045, 1024)\n","Size of distribution coreset without edge feature :  (2048, 1024)\n","Epoch 0:  99% 720/726 [00:49<00:00, 14.40it/s, loss=nan, v_num=0]Size of embedding coreset with edge feature :  (1858, 1024)\n","Epoch 0: 100% 726/726 [01:00<00:00, 12.10it/s, loss=nan, v_num=0]\n","End generating embeeding coreset and distribution coreset\n","Using cache found in /root/.cache/torch/hub/pytorch_vision_v0.10.0\n","/usr/local/lib/python3.8/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and will be removed in 0.15, please use 'weights' instead.\n","  warnings.warn(\n","/usr/local/lib/python3.8/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and will be removed in 0.15. The current behavior is equivalent to passing `weights=Wide_ResNet101_2_Weights.IMAGENET1K_V1`. You can also use `weights=Wide_ResNet101_2_Weights.DEFAULT` to get the most up-to-date weights.\n","  warnings.warn(msg)\n","Start training normal feature distribution from neighbrohood information\n","GPU available: True, used: True\n","TPU available: False, using: 0 TPU cores\n","IPU available: False, using: 0 IPUs\n","LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n","Missing logger folder: ./result/ea_mass/WR101/distribution\n","\n","  | Name  | Type               | Params\n","---------------------------------------------\n","0 | model | Distribution_Model | 205 M \n","---------------------------------------------\n","205 M     Trainable params\n","0         Non-trainable params\n","205 M     Total params\n","822.313   Total estimated model params size (MB)\n","Validation sanity check: 0it [00:00, ?it/s]/usr/local/lib/python3.8/dist-packages/pytorch_lightning/trainer/data_loading.py:132: UserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n","  rank_zero_warn(\n","Global seed set to 23\n","/usr/local/lib/python3.8/dist-packages/pytorch_lightning/trainer/data_loading.py:132: UserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n","  rank_zero_warn(\n","Epoch 0: 100% 52/52 [01:11<00:00,  1.37s/it, loss=6.32, v_num=0, train_loss=5.890]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/6 [00:00<?, ?it/s]\u001b[A\n","Epoch 0: 100% 52/52 [01:24<00:00,  1.63s/it, loss=6.04, v_num=0, train_loss=5.670, valid_loss=5.780]\n","Epoch 1: 100% 52/52 [01:11<00:00,  1.38s/it, loss=4.68, v_num=0, train_loss=4.470, valid_loss=5.780]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/6 [00:00<?, ?it/s]\u001b[A\n","Epoch 1: 100% 52/52 [01:24<00:00,  1.63s/it, loss=4.53, v_num=0, train_loss=4.410, valid_loss=4.090]\n","Epoch 2: 100% 52/52 [01:11<00:00,  1.37s/it, loss=3.93, v_num=0, train_loss=3.830, valid_loss=4.090]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/6 [00:00<?, ?it/s]\u001b[A\n","Epoch 2: 100% 52/52 [01:22<00:00,  1.59s/it, loss=3.86, v_num=0, train_loss=3.790, valid_loss=3.530]\n","Epoch 3: 100% 52/52 [01:11<00:00,  1.37s/it, loss=3.55, v_num=0, train_loss=3.520, valid_loss=3.530]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 3: 100% 52/52 [01:22<00:00,  1.59s/it, loss=3.55, v_num=0, train_loss=3.520, valid_loss=3.530]\n","Epoch 3: 100% 52/52 [01:22<00:00,  1.59s/it, loss=3.51, v_num=0, train_loss=3.500, valid_loss=3.160]\n","Epoch 4: 100% 52/52 [01:11<00:00,  1.37s/it, loss=3.29, v_num=0, train_loss=3.250, valid_loss=3.160]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/6 [00:00<?, ?it/s]\u001b[A\n","Epoch 4: 100% 52/52 [01:22<00:00,  1.59s/it, loss=3.27, v_num=0, train_loss=3.260, valid_loss=2.840]\n","Epoch 5: 100% 52/52 [01:11<00:00,  1.38s/it, loss=3.04, v_num=0, train_loss=2.970, valid_loss=2.840]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/6 [00:00<?, ?it/s]\u001b[A\n","Epoch 5: 100% 52/52 [01:23<00:00,  1.60s/it, loss=3.04, v_num=0, train_loss=3.020, valid_loss=2.660]\n","Epoch 6: 100% 52/52 [01:11<00:00,  1.38s/it, loss=2.99, v_num=0, train_loss=2.950, valid_loss=2.660]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 6: 100% 52/52 [01:24<00:00,  1.62s/it, loss=2.99, v_num=0, train_loss=2.950, valid_loss=2.660]\n","Epoch 6: 100% 52/52 [01:24<00:00,  1.62s/it, loss=2.99, v_num=0, train_loss=3.020, valid_loss=2.640]\n","Epoch 7: 100% 52/52 [01:12<00:00,  1.39s/it, loss=2.97, v_num=0, train_loss=2.990, valid_loss=2.640]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/6 [00:00<?, ?it/s]\u001b[A\n","Epoch 7: 100% 52/52 [01:23<00:00,  1.61s/it, loss=2.97, v_num=0, train_loss=2.990, valid_loss=2.600]\n","Epoch 8: 100% 52/52 [01:12<00:00,  1.40s/it, loss=2.95, v_num=0, train_loss=2.900, valid_loss=2.600]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/6 [00:00<?, ?it/s]\u001b[A\n","Epoch 8: 100% 52/52 [01:25<00:00,  1.65s/it, loss=2.94, v_num=0, train_loss=2.880, valid_loss=2.580]\n","Epoch 9: 100% 52/52 [01:16<00:00,  1.47s/it, loss=2.91, v_num=0, train_loss=2.900, valid_loss=2.580]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/6 [00:00<?, ?it/s]\u001b[A\n","Epoch 9: 100% 52/52 [01:29<00:00,  1.72s/it, loss=2.92, v_num=0, train_loss=2.890, valid_loss=2.560]\n","Epoch 10: 100% 52/52 [01:14<00:00,  1.44s/it, loss=2.88, v_num=0, train_loss=2.900, valid_loss=2.560]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/6 [00:00<?, ?it/s]\u001b[A\n","Epoch 10: 100% 52/52 [01:26<00:00,  1.66s/it, loss=2.89, v_num=0, train_loss=2.970, valid_loss=2.550]\n","Epoch 11: 100% 52/52 [01:15<00:00,  1.45s/it, loss=2.88, v_num=0, train_loss=2.850, valid_loss=2.550]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/6 [00:00<?, ?it/s]\u001b[A\n","Epoch 11: 100% 52/52 [01:28<00:00,  1.70s/it, loss=2.88, v_num=0, train_loss=2.880, valid_loss=2.550]\n","Epoch 12: 100% 52/52 [01:13<00:00,  1.41s/it, loss=2.88, v_num=0, train_loss=2.860, valid_loss=2.550]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/6 [00:00<?, ?it/s]\u001b[A\n","Epoch 12: 100% 52/52 [01:25<00:00,  1.64s/it, loss=2.89, v_num=0, train_loss=2.920, valid_loss=2.550]\n","Epoch 13: 100% 52/52 [01:16<00:00,  1.46s/it, loss=2.87, v_num=0, train_loss=2.840, valid_loss=2.550]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/6 [00:00<?, ?it/s]\u001b[A\n","Epoch 13: 100% 52/52 [01:27<00:00,  1.69s/it, loss=2.88, v_num=0, train_loss=2.940, valid_loss=2.540]\n","Epoch 14: 100% 52/52 [01:17<00:00,  1.49s/it, loss=2.88, v_num=0, train_loss=2.880, valid_loss=2.540]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/6 [00:00<?, ?it/s]\u001b[A\n","Epoch 14: 100% 52/52 [01:29<00:00,  1.71s/it, loss=2.88, v_num=0, train_loss=2.830, valid_loss=2.540]\n","Epoch 14: 100% 52/52 [01:29<00:00,  1.71s/it, loss=2.88, v_num=0, train_loss=2.830, valid_loss=2.540]\n","End training normal feature distribution from neighbrohood information\n","Start training normal feature distribution from position information\n","Using cache found in /root/.cache/torch/hub/pytorch_vision_v0.10.0\n","Using cache found in /root/.cache/torch/hub/pytorch_vision_v0.10.0\n","End training normal feature distribution from position information\n","Start evaluating anomaly score\n","GPU available: True, used: True\n","TPU available: False, using: 0 TPU cores\n","IPU available: False, using: 0 IPUs\n","Using cache found in /root/.cache/torch/hub/pytorch_vision_v0.10.0\n","LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n","/usr/local/lib/python3.8/dist-packages/pytorch_lightning/trainer/data_loading.py:132: UserWarning: The dataloader, test_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n","  rank_zero_warn(\n","Testing: 100% 3/3 [00:21<00:00,  7.15s/it]--------------------------------------------------------------------------------\n","DATALOADER:0 TEST RESULTS\n","{'pixel_auc_coor': 0.8953323808398903,\n"," 'pixel_auc_nb': 0.8536212089549527,\n"," 'pixel_auc_nb_coor': 0.8494883338481946,\n"," 'pixel_auc_patchcore': 0.9044898102887463}\n","--------------------------------------------------------------------------------\n","Testing: 100% 3/3 [00:21<00:00,  7.15s/it]\n","End evaluating anomaly score\n","Global seed set to 23\n","17\n","17\n","['../dataset/UAD/edema/test/anomaly_images/file_brain_AXT1_202_2020041.png', '../dataset/UAD/edema/test/anomaly_images/file_brain_AXT1_202_2020064.png', '../dataset/UAD/edema/test/anomaly_images/file_brain_AXT1_202_2020103.png', '../dataset/UAD/edema/test/anomaly_images/file_brain_AXT1_202_2020318.png', '../dataset/UAD/edema/test/anomaly_images/file_brain_AXT1_202_2020455.png', '../dataset/UAD/edema/test/anomaly_images/file_brain_AXT1_202_2020486.png', '../dataset/UAD/edema/test/anomaly_images/file_brain_AXT1_202_2020570.png', '../dataset/UAD/edema/test/anomaly_images/file_brain_AXT1_202_2020576.png', '../dataset/UAD/edema/test/anomaly_images/file_brain_AXT1_202_2020581.png', '../dataset/UAD/edema/test/anomaly_images/file_brain_AXT1_202_2020585.png', '../dataset/UAD/edema/test/anomaly_images/file_brain_AXT1_202_2120021.png', '../dataset/UAD/edema/test/anomaly_images/file_brain_AXT1_202_6000264.png', '../dataset/UAD/edema/test/anomaly_images/file_brain_AXT1_202_6000319.png', '../dataset/UAD/edema/test/anomaly_images/file_brain_AXT1_202_6000382.png', '../dataset/UAD/edema/test/anomaly_images/file_brain_AXT1_202_6000465.png', '../dataset/UAD/edema/test/anomaly_images/file_brain_AXT1_202_6000577.png', '../dataset/UAD/edema/test/anomaly_images/file_brain_AXT1_206_2060078.png']\n","['../dataset/UAD/edema/ground_truth/anomaly_images/file_brain_AXT1_202_2020041_edema_0.png', '../dataset/UAD/edema/ground_truth/anomaly_images/file_brain_AXT1_202_2020064_combined.png', '../dataset/UAD/edema/ground_truth/anomaly_images/file_brain_AXT1_202_2020103_edema_0.png', '../dataset/UAD/edema/ground_truth/anomaly_images/file_brain_AXT1_202_2020318_edema_0.png', '../dataset/UAD/edema/ground_truth/anomaly_images/file_brain_AXT1_202_2020455_edema_0.png', '../dataset/UAD/edema/ground_truth/anomaly_images/file_brain_AXT1_202_2020486_edema_1.png', '../dataset/UAD/edema/ground_truth/anomaly_images/file_brain_AXT1_202_2020570_edema_0.png', '../dataset/UAD/edema/ground_truth/anomaly_images/file_brain_AXT1_202_2020576_edema_0.png', '../dataset/UAD/edema/ground_truth/anomaly_images/file_brain_AXT1_202_2020581_edema_0.png', '../dataset/UAD/edema/ground_truth/anomaly_images/file_brain_AXT1_202_2020585_edema_0.png', '../dataset/UAD/edema/ground_truth/anomaly_images/file_brain_AXT1_202_2120021_edema_0.png', '../dataset/UAD/edema/ground_truth/anomaly_images/file_brain_AXT1_202_6000264_edema_0.png', '../dataset/UAD/edema/ground_truth/anomaly_images/file_brain_AXT1_202_6000319_edema_0.png', '../dataset/UAD/edema/ground_truth/anomaly_images/file_brain_AXT1_202_6000382_edema_1.png', '../dataset/UAD/edema/ground_truth/anomaly_images/file_brain_AXT1_202_6000465_edema_0.png', '../dataset/UAD/edema/ground_truth/anomaly_images/file_brain_AXT1_202_6000577_edema_0.png', '../dataset/UAD/edema/ground_truth/anomaly_images/file_brain_AXT1_206_2060078_edema_0.png']\n","Start generating embeeding coreset and distribution coreset\n","GPU available: True, used: True\n","TPU available: False, using: 0 TPU cores\n","IPU available: False, using: 0 IPUs\n","coreset_generator_trainer完了\n","Using cache found in /root/.cache/torch/hub/pytorch_vision_v0.10.0\n","/usr/local/lib/python3.8/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and will be removed in 0.15, please use 'weights' instead.\n","  warnings.warn(\n","/usr/local/lib/python3.8/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and will be removed in 0.15. The current behavior is equivalent to passing `weights=Wide_ResNet101_2_Weights.IMAGENET1K_V1`. You can also use `weights=Wide_ResNet101_2_Weights.DEFAULT` to get the most up-to-date weights.\n","  warnings.warn(msg)\n","coreset_generator完了\n","LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n","/usr/local/lib/python3.8/dist-packages/pytorch_lightning/trainer/optimizers.py:37: UserWarning: `LightningModule.configure_optimizers` returned `None`, this fit will run with no optimizer\n","  rank_zero_warn(\n","Missing logger folder: ./result/edema/WR101/coreset/lightning_logs\n","\n","  | Name     | Type   | Params\n","------------------------------------\n","0 | backbone | ResNet | 126 M \n","------------------------------------\n","126 M     Trainable params\n","0         Non-trainable params\n","126 M     Total params\n","507.547   Total estimated model params size (MB)\n","/usr/local/lib/python3.8/dist-packages/pytorch_lightning/trainer/data_loading.py:132: UserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n","  rank_zero_warn(\n","Epoch 0:   0% 0/726 [00:00<?, ?it/s] /usr/local/lib/python3.8/dist-packages/pytorch_lightning/loops/optimization/optimizer_loop.py:145: UserWarning: `training_step` returned `None`. If this was on purpose, ignore this warning...\n","  self.warning_cache.warn(\n","Epoch 0:  99% 720/726 [00:34<00:00, 20.99it/s, loss=nan, v_num=0]Size of embedding coreset without edge feature :  (1045, 1024)\n","Size of distribution coreset without edge feature :  (2048, 1024)\n","Epoch 0:  99% 720/726 [00:49<00:00, 14.40it/s, loss=nan, v_num=0]Size of embedding coreset with edge feature :  (1858, 1024)\n","Epoch 0: 100% 726/726 [01:00<00:00, 12.09it/s, loss=nan, v_num=0]\n","End generating embeeding coreset and distribution coreset\n","Using cache found in /root/.cache/torch/hub/pytorch_vision_v0.10.0\n","/usr/local/lib/python3.8/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and will be removed in 0.15, please use 'weights' instead.\n","  warnings.warn(\n","/usr/local/lib/python3.8/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and will be removed in 0.15. The current behavior is equivalent to passing `weights=Wide_ResNet101_2_Weights.IMAGENET1K_V1`. You can also use `weights=Wide_ResNet101_2_Weights.DEFAULT` to get the most up-to-date weights.\n","  warnings.warn(msg)\n","Start training normal feature distribution from neighbrohood information\n","GPU available: True, used: True\n","TPU available: False, using: 0 TPU cores\n","IPU available: False, using: 0 IPUs\n","LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n","Missing logger folder: ./result/edema/WR101/distribution\n","\n","  | Name  | Type               | Params\n","---------------------------------------------\n","0 | model | Distribution_Model | 205 M \n","---------------------------------------------\n","205 M     Trainable params\n","0         Non-trainable params\n","205 M     Total params\n","822.313   Total estimated model params size (MB)\n","Validation sanity check: 0it [00:00, ?it/s]/usr/local/lib/python3.8/dist-packages/pytorch_lightning/trainer/data_loading.py:132: UserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n","  rank_zero_warn(\n","Global seed set to 23\n","/usr/local/lib/python3.8/dist-packages/pytorch_lightning/trainer/data_loading.py:132: UserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n","  rank_zero_warn(\n","Epoch 0: 100% 52/52 [01:10<00:00,  1.35s/it, loss=6.32, v_num=0, train_loss=5.890]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/6 [00:00<?, ?it/s]\u001b[A\n","Epoch 0: 100% 52/52 [01:21<00:00,  1.58s/it, loss=6.04, v_num=0, train_loss=5.670, valid_loss=5.780]\n","Epoch 1: 100% 52/52 [01:11<00:00,  1.37s/it, loss=4.68, v_num=0, train_loss=4.470, valid_loss=5.780]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 1: 100% 52/52 [01:23<00:00,  1.61s/it, loss=4.68, v_num=0, train_loss=4.470, valid_loss=5.780]\n","Epoch 1: 100% 52/52 [01:24<00:00,  1.62s/it, loss=4.53, v_num=0, train_loss=4.410, valid_loss=4.090]\n","Epoch 2: 100% 52/52 [01:10<00:00,  1.36s/it, loss=3.93, v_num=0, train_loss=3.830, valid_loss=4.090]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/6 [00:00<?, ?it/s]\u001b[A\n","Epoch 2: 100% 52/52 [01:23<00:00,  1.60s/it, loss=3.86, v_num=0, train_loss=3.790, valid_loss=3.530]\n","Epoch 3: 100% 52/52 [01:49<00:00,  2.10s/it, loss=3.55, v_num=0, train_loss=3.520, valid_loss=3.530]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/6 [00:00<?, ?it/s]\u001b[A\n","Epoch 3: 100% 52/52 [02:04<00:00,  2.40s/it, loss=3.51, v_num=0, train_loss=3.500, valid_loss=3.160]\n","Epoch 4: 100% 52/52 [01:14<00:00,  1.44s/it, loss=3.29, v_num=0, train_loss=3.250, valid_loss=3.160]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/6 [00:00<?, ?it/s]\u001b[A\n","Epoch 4: 100% 52/52 [01:26<00:00,  1.66s/it, loss=3.27, v_num=0, train_loss=3.260, valid_loss=2.840]\n","Epoch 5: 100% 52/52 [01:13<00:00,  1.40s/it, loss=3.04, v_num=0, train_loss=2.970, valid_loss=2.840]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/6 [00:00<?, ?it/s]\u001b[A\n","Epoch 5: 100% 52/52 [01:30<00:00,  1.73s/it, loss=3.04, v_num=0, train_loss=3.020, valid_loss=2.660]\n","Epoch 6: 100% 52/52 [01:27<00:00,  1.68s/it, loss=2.99, v_num=0, train_loss=2.950, valid_loss=2.660]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/6 [00:00<?, ?it/s]\u001b[A\n","Epoch 6: 100% 52/52 [01:42<00:00,  1.98s/it, loss=2.99, v_num=0, train_loss=3.020, valid_loss=2.640]\n","Epoch 7: 100% 52/52 [01:23<00:00,  1.61s/it, loss=2.97, v_num=0, train_loss=2.990, valid_loss=2.640]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 7: 100% 52/52 [01:34<00:00,  1.82s/it, loss=2.97, v_num=0, train_loss=2.990, valid_loss=2.640]\n","Epoch 7: 100% 52/52 [01:35<00:00,  1.83s/it, loss=2.97, v_num=0, train_loss=2.990, valid_loss=2.600]\n","Epoch 8: 100% 52/52 [01:11<00:00,  1.37s/it, loss=2.95, v_num=0, train_loss=2.900, valid_loss=2.600]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/6 [00:00<?, ?it/s]\u001b[A\n","Epoch 8: 100% 52/52 [01:23<00:00,  1.61s/it, loss=2.94, v_num=0, train_loss=2.880, valid_loss=2.580]\n","Epoch 9: 100% 52/52 [01:24<00:00,  1.62s/it, loss=2.91, v_num=0, train_loss=2.900, valid_loss=2.580]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/6 [00:00<?, ?it/s]\u001b[A\n","Epoch 9: 100% 52/52 [01:40<00:00,  1.93s/it, loss=2.92, v_num=0, train_loss=2.890, valid_loss=2.560]\n","Epoch 10: 100% 52/52 [01:18<00:00,  1.50s/it, loss=2.88, v_num=0, train_loss=2.900, valid_loss=2.560]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/6 [00:00<?, ?it/s]\u001b[A\n","Epoch 10: 100% 52/52 [01:29<00:00,  1.72s/it, loss=2.89, v_num=0, train_loss=2.970, valid_loss=2.550]\n","Epoch 11: 100% 52/52 [01:09<00:00,  1.33s/it, loss=2.88, v_num=0, train_loss=2.850, valid_loss=2.550]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/6 [00:00<?, ?it/s]\u001b[A\n","Epoch 11: 100% 52/52 [01:22<00:00,  1.58s/it, loss=2.88, v_num=0, train_loss=2.880, valid_loss=2.550]\n","Epoch 12: 100% 52/52 [01:16<00:00,  1.47s/it, loss=2.88, v_num=0, train_loss=2.860, valid_loss=2.550]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/6 [00:00<?, ?it/s]\u001b[A\n","Epoch 12: 100% 52/52 [01:31<00:00,  1.77s/it, loss=2.89, v_num=0, train_loss=2.920, valid_loss=2.550]\n","Epoch 13: 100% 52/52 [01:26<00:00,  1.67s/it, loss=2.87, v_num=0, train_loss=2.840, valid_loss=2.550]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/6 [00:00<?, ?it/s]\u001b[A\n","Epoch 13: 100% 52/52 [01:38<00:00,  1.89s/it, loss=2.88, v_num=0, train_loss=2.940, valid_loss=2.540]\n","Epoch 14: 100% 52/52 [01:17<00:00,  1.49s/it, loss=2.88, v_num=0, train_loss=2.880, valid_loss=2.540]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/6 [00:00<?, ?it/s]\u001b[A\n","Epoch 14: 100% 52/52 [01:28<00:00,  1.71s/it, loss=2.88, v_num=0, train_loss=2.830, valid_loss=2.540]\n","Epoch 14: 100% 52/52 [01:28<00:00,  1.71s/it, loss=2.88, v_num=0, train_loss=2.830, valid_loss=2.540]\n","End training normal feature distribution from neighbrohood information\n","Start training normal feature distribution from position information\n","Using cache found in /root/.cache/torch/hub/pytorch_vision_v0.10.0\n","Using cache found in /root/.cache/torch/hub/pytorch_vision_v0.10.0\n","End training normal feature distribution from position information\n","Start evaluating anomaly score\n","GPU available: True, used: True\n","TPU available: False, using: 0 TPU cores\n","IPU available: False, using: 0 IPUs\n","Using cache found in /root/.cache/torch/hub/pytorch_vision_v0.10.0\n","LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n","/usr/local/lib/python3.8/dist-packages/pytorch_lightning/trainer/data_loading.py:132: UserWarning: The dataloader, test_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n","  rank_zero_warn(\n","Testing: 100% 17/17 [02:28<00:00,  8.74s/it]--------------------------------------------------------------------------------\n","DATALOADER:0 TEST RESULTS\n","{'pixel_auc_coor': 0.9203062623148736,\n"," 'pixel_auc_nb': 0.8792310168977636,\n"," 'pixel_auc_nb_coor': 0.8798550954106368,\n"," 'pixel_auc_patchcore': 0.9242205687385312}\n","--------------------------------------------------------------------------------\n","Testing: 100% 17/17 [02:28<00:00,  8.74s/it]\n","End evaluating anomaly score\n","Global seed set to 23\n","1\n","1\n","['../dataset/UAD/encephalomalacia/test/anomaly_images/file_brain_AXT1_202_2020377.png']\n","['../dataset/UAD/encephalomalacia/ground_truth/anomaly_images/file_brain_AXT1_202_2020377_encephalomalacia_0.png']\n","Start generating embeeding coreset and distribution coreset\n","GPU available: True, used: True\n","TPU available: False, using: 0 TPU cores\n","IPU available: False, using: 0 IPUs\n","coreset_generator_trainer完了\n","Using cache found in /root/.cache/torch/hub/pytorch_vision_v0.10.0\n","/usr/local/lib/python3.8/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and will be removed in 0.15, please use 'weights' instead.\n","  warnings.warn(\n","/usr/local/lib/python3.8/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and will be removed in 0.15. The current behavior is equivalent to passing `weights=Wide_ResNet101_2_Weights.IMAGENET1K_V1`. You can also use `weights=Wide_ResNet101_2_Weights.DEFAULT` to get the most up-to-date weights.\n","  warnings.warn(msg)\n","coreset_generator完了\n","LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n","/usr/local/lib/python3.8/dist-packages/pytorch_lightning/trainer/optimizers.py:37: UserWarning: `LightningModule.configure_optimizers` returned `None`, this fit will run with no optimizer\n","  rank_zero_warn(\n","Missing logger folder: ./result/encephalomalacia/WR101/coreset/lightning_logs\n","\n","  | Name     | Type   | Params\n","------------------------------------\n","0 | backbone | ResNet | 126 M \n","------------------------------------\n","126 M     Trainable params\n","0         Non-trainable params\n","126 M     Total params\n","507.547   Total estimated model params size (MB)\n","/usr/local/lib/python3.8/dist-packages/pytorch_lightning/trainer/data_loading.py:132: UserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n","  rank_zero_warn(\n","Epoch 0:   0% 0/726 [00:00<?, ?it/s] /usr/local/lib/python3.8/dist-packages/pytorch_lightning/loops/optimization/optimizer_loop.py:145: UserWarning: `training_step` returned `None`. If this was on purpose, ignore this warning...\n","  self.warning_cache.warn(\n","Epoch 0:  99% 720/726 [00:35<00:00, 20.45it/s, loss=nan, v_num=0]Size of embedding coreset without edge feature :  (1045, 1024)\n","Size of distribution coreset without edge feature :  (2048, 1024)\n","Epoch 0:  99% 720/726 [00:49<00:00, 14.40it/s, loss=nan, v_num=0]Size of embedding coreset with edge feature :  (1858, 1024)\n","Epoch 0: 100% 726/726 [01:01<00:00, 11.83it/s, loss=nan, v_num=0]\n","End generating embeeding coreset and distribution coreset\n","Using cache found in /root/.cache/torch/hub/pytorch_vision_v0.10.0\n","/usr/local/lib/python3.8/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and will be removed in 0.15, please use 'weights' instead.\n","  warnings.warn(\n","/usr/local/lib/python3.8/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and will be removed in 0.15. The current behavior is equivalent to passing `weights=Wide_ResNet101_2_Weights.IMAGENET1K_V1`. You can also use `weights=Wide_ResNet101_2_Weights.DEFAULT` to get the most up-to-date weights.\n","  warnings.warn(msg)\n","Start training normal feature distribution from neighbrohood information\n","GPU available: True, used: True\n","TPU available: False, using: 0 TPU cores\n","IPU available: False, using: 0 IPUs\n","LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n","Missing logger folder: ./result/encephalomalacia/WR101/distribution\n","\n","  | Name  | Type               | Params\n","---------------------------------------------\n","0 | model | Distribution_Model | 205 M \n","---------------------------------------------\n","205 M     Trainable params\n","0         Non-trainable params\n","205 M     Total params\n","822.313   Total estimated model params size (MB)\n","Validation sanity check: 0it [00:00, ?it/s]/usr/local/lib/python3.8/dist-packages/pytorch_lightning/trainer/data_loading.py:132: UserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n","  rank_zero_warn(\n","Global seed set to 23\n","/usr/local/lib/python3.8/dist-packages/pytorch_lightning/trainer/data_loading.py:132: UserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n","  rank_zero_warn(\n","Epoch 0: 100% 52/52 [01:05<00:00,  1.27s/it, loss=6.32, v_num=0, train_loss=5.890]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/6 [00:00<?, ?it/s]\u001b[A\n","Epoch 0: 100% 52/52 [01:18<00:00,  1.51s/it, loss=6.04, v_num=0, train_loss=5.670, valid_loss=5.780]\n","Epoch 1: 100% 52/52 [01:05<00:00,  1.26s/it, loss=4.68, v_num=0, train_loss=4.470, valid_loss=5.780]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/6 [00:00<?, ?it/s]\u001b[A\n","Epoch 1: 100% 52/52 [01:16<00:00,  1.48s/it, loss=4.53, v_num=0, train_loss=4.410, valid_loss=4.090]\n","Epoch 2: 100% 52/52 [01:06<00:00,  1.28s/it, loss=3.93, v_num=0, train_loss=3.830, valid_loss=4.090]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 2: 100% 52/52 [01:18<00:00,  1.51s/it, loss=3.93, v_num=0, train_loss=3.830, valid_loss=4.090]\n","Epoch 2: 100% 52/52 [01:18<00:00,  1.52s/it, loss=3.86, v_num=0, train_loss=3.790, valid_loss=3.530]\n","Epoch 3: 100% 52/52 [01:06<00:00,  1.27s/it, loss=3.55, v_num=0, train_loss=3.520, valid_loss=3.530]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/6 [00:00<?, ?it/s]\u001b[A\n","Epoch 3: 100% 52/52 [01:17<00:00,  1.49s/it, loss=3.51, v_num=0, train_loss=3.500, valid_loss=3.160]\n","Epoch 4: 100% 52/52 [01:06<00:00,  1.28s/it, loss=3.29, v_num=0, train_loss=3.250, valid_loss=3.160]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/6 [00:00<?, ?it/s]\u001b[A\n","Epoch 4: 100% 52/52 [01:17<00:00,  1.50s/it, loss=3.27, v_num=0, train_loss=3.260, valid_loss=2.840]\n","Epoch 5: 100% 52/52 [01:06<00:00,  1.28s/it, loss=3.04, v_num=0, train_loss=2.970, valid_loss=2.840]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/6 [00:00<?, ?it/s]\u001b[A\n","Epoch 5: 100% 52/52 [01:17<00:00,  1.49s/it, loss=3.04, v_num=0, train_loss=3.020, valid_loss=2.660]\n","Epoch 6: 100% 52/52 [01:06<00:00,  1.27s/it, loss=2.99, v_num=0, train_loss=2.950, valid_loss=2.660]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 6: 100% 52/52 [01:16<00:00,  1.48s/it, loss=2.99, v_num=0, train_loss=2.950, valid_loss=2.660]\n","Epoch 6: 100% 52/52 [01:17<00:00,  1.49s/it, loss=2.99, v_num=0, train_loss=3.020, valid_loss=2.640]\n","Epoch 7: 100% 52/52 [01:07<00:00,  1.29s/it, loss=2.97, v_num=0, train_loss=2.990, valid_loss=2.640]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/6 [00:00<?, ?it/s]\u001b[A\n","Epoch 7: 100% 52/52 [01:19<00:00,  1.53s/it, loss=2.97, v_num=0, train_loss=2.990, valid_loss=2.600]\n","Epoch 8: 100% 52/52 [01:06<00:00,  1.28s/it, loss=2.95, v_num=0, train_loss=2.900, valid_loss=2.600]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/6 [00:00<?, ?it/s]\u001b[A\n","Epoch 8: 100% 52/52 [01:17<00:00,  1.50s/it, loss=2.94, v_num=0, train_loss=2.880, valid_loss=2.580]\n","Epoch 9: 100% 52/52 [01:06<00:00,  1.27s/it, loss=2.91, v_num=0, train_loss=2.900, valid_loss=2.580]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/6 [00:00<?, ?it/s]\u001b[A\n","Epoch 9: 100% 52/52 [01:18<00:00,  1.52s/it, loss=2.92, v_num=0, train_loss=2.890, valid_loss=2.560]\n","Epoch 10: 100% 52/52 [01:05<00:00,  1.27s/it, loss=2.88, v_num=0, train_loss=2.900, valid_loss=2.560]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/6 [00:00<?, ?it/s]\u001b[A\n","Epoch 10: 100% 52/52 [01:17<00:00,  1.48s/it, loss=2.89, v_num=0, train_loss=2.970, valid_loss=2.550]\n","Epoch 11: 100% 52/52 [01:06<00:00,  1.27s/it, loss=2.88, v_num=0, train_loss=2.850, valid_loss=2.550]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 11: 100% 52/52 [01:16<00:00,  1.47s/it, loss=2.88, v_num=0, train_loss=2.850, valid_loss=2.550]\n","Epoch 11: 100% 52/52 [01:17<00:00,  1.49s/it, loss=2.88, v_num=0, train_loss=2.880, valid_loss=2.550]\n","Epoch 12: 100% 52/52 [01:06<00:00,  1.27s/it, loss=2.88, v_num=0, train_loss=2.860, valid_loss=2.550]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/6 [00:00<?, ?it/s]\u001b[A\n","Epoch 12: 100% 52/52 [01:17<00:00,  1.49s/it, loss=2.89, v_num=0, train_loss=2.920, valid_loss=2.550]\n","Epoch 13: 100% 52/52 [01:06<00:00,  1.27s/it, loss=2.87, v_num=0, train_loss=2.840, valid_loss=2.550]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/6 [00:00<?, ?it/s]\u001b[A\n","Epoch 13: 100% 52/52 [01:17<00:00,  1.49s/it, loss=2.88, v_num=0, train_loss=2.940, valid_loss=2.540]\n","Epoch 14: 100% 52/52 [01:06<00:00,  1.27s/it, loss=2.88, v_num=0, train_loss=2.880, valid_loss=2.540]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/6 [00:00<?, ?it/s]\u001b[A\n","Epoch 14: 100% 52/52 [01:17<00:00,  1.49s/it, loss=2.88, v_num=0, train_loss=2.830, valid_loss=2.540]\n","Epoch 14: 100% 52/52 [01:17<00:00,  1.49s/it, loss=2.88, v_num=0, train_loss=2.830, valid_loss=2.540]\n","End training normal feature distribution from neighbrohood information\n","Start training normal feature distribution from position information\n","Using cache found in /root/.cache/torch/hub/pytorch_vision_v0.10.0\n","Using cache found in /root/.cache/torch/hub/pytorch_vision_v0.10.0\n","End training normal feature distribution from position information\n","Start evaluating anomaly score\n","GPU available: True, used: True\n","TPU available: False, using: 0 TPU cores\n","IPU available: False, using: 0 IPUs\n","Using cache found in /root/.cache/torch/hub/pytorch_vision_v0.10.0\n","LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n","/usr/local/lib/python3.8/dist-packages/pytorch_lightning/trainer/data_loading.py:132: UserWarning: The dataloader, test_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n","  rank_zero_warn(\n","Testing: 100% 1/1 [00:10<00:00, 10.60s/it]--------------------------------------------------------------------------------\n","DATALOADER:0 TEST RESULTS\n","{'pixel_auc_coor': 0.9653819397894458,\n"," 'pixel_auc_nb': 0.9831832854960737,\n"," 'pixel_auc_nb_coor': 0.9787444399521873,\n"," 'pixel_auc_patchcore': 0.949709563264644}\n","--------------------------------------------------------------------------------\n","Testing: 100% 1/1 [00:10<00:00, 10.61s/it]\n","End evaluating anomaly score\n","Global seed set to 23\n","16\n","16\n","['../dataset/UAD/enlarged_ventricles/test/anomaly_images/file_brain_AXT1_201_6002800.png', '../dataset/UAD/enlarged_ventricles/test/anomaly_images/file_brain_AXT1_201_6002836.png', '../dataset/UAD/enlarged_ventricles/test/anomaly_images/file_brain_AXT1_201_6002879.png', '../dataset/UAD/enlarged_ventricles/test/anomaly_images/file_brain_AXT1_202_2020146.png', '../dataset/UAD/enlarged_ventricles/test/anomaly_images/file_brain_AXT1_202_2020256.png', '../dataset/UAD/enlarged_ventricles/test/anomaly_images/file_brain_AXT1_202_2020517.png', '../dataset/UAD/enlarged_ventricles/test/anomaly_images/file_brain_AXT1_202_2020526.png', '../dataset/UAD/enlarged_ventricles/test/anomaly_images/file_brain_AXT1_202_2020559.png', '../dataset/UAD/enlarged_ventricles/test/anomaly_images/file_brain_AXT1_202_2020561.png', '../dataset/UAD/enlarged_ventricles/test/anomaly_images/file_brain_AXT1_202_2020589.png', '../dataset/UAD/enlarged_ventricles/test/anomaly_images/file_brain_AXT1_202_6000264.png', '../dataset/UAD/enlarged_ventricles/test/anomaly_images/file_brain_AXT1_202_6000279.png', '../dataset/UAD/enlarged_ventricles/test/anomaly_images/file_brain_AXT1_202_6000342.png', '../dataset/UAD/enlarged_ventricles/test/anomaly_images/file_brain_AXT1_202_6000398.png', '../dataset/UAD/enlarged_ventricles/test/anomaly_images/file_brain_AXT1_202_6000434.png', '../dataset/UAD/enlarged_ventricles/test/anomaly_images/file_brain_AXT1_206_2060056.png']\n","['../dataset/UAD/enlarged_ventricles/ground_truth/anomaly_images/file_brain_AXT1_201_6002800_combined.png', '../dataset/UAD/enlarged_ventricles/ground_truth/anomaly_images/file_brain_AXT1_201_6002836_enlarged_ventricles_0.png', '../dataset/UAD/enlarged_ventricles/ground_truth/anomaly_images/file_brain_AXT1_201_6002879_combined.png', '../dataset/UAD/enlarged_ventricles/ground_truth/anomaly_images/file_brain_AXT1_202_2020146_enlarged_ventricles_0.png', '../dataset/UAD/enlarged_ventricles/ground_truth/anomaly_images/file_brain_AXT1_202_2020256_enlarged_ventricles_1.png', '../dataset/UAD/enlarged_ventricles/ground_truth/anomaly_images/file_brain_AXT1_202_2020517_enlarged_ventricles_0.png', '../dataset/UAD/enlarged_ventricles/ground_truth/anomaly_images/file_brain_AXT1_202_2020526_enlarged_ventricles_0.png', '../dataset/UAD/enlarged_ventricles/ground_truth/anomaly_images/file_brain_AXT1_202_2020559_enlarged_ventricles_0.png', '../dataset/UAD/enlarged_ventricles/ground_truth/anomaly_images/file_brain_AXT1_202_2020561_enlarged_ventricles_0.png', '../dataset/UAD/enlarged_ventricles/ground_truth/anomaly_images/file_brain_AXT1_202_2020589_enlarged_ventricles_0.png', '../dataset/UAD/enlarged_ventricles/ground_truth/anomaly_images/file_brain_AXT1_202_6000264_enlarged_ventricles_1.png', '../dataset/UAD/enlarged_ventricles/ground_truth/anomaly_images/file_brain_AXT1_202_6000279_enlarged_ventricles_0.png', '../dataset/UAD/enlarged_ventricles/ground_truth/anomaly_images/file_brain_AXT1_202_6000342_enlarged_ventricles_0.png', '../dataset/UAD/enlarged_ventricles/ground_truth/anomaly_images/file_brain_AXT1_202_6000398_enlarged_ventricles_0.png', '../dataset/UAD/enlarged_ventricles/ground_truth/anomaly_images/file_brain_AXT1_202_6000434_combined.png', '../dataset/UAD/enlarged_ventricles/ground_truth/anomaly_images/file_brain_AXT1_206_2060056_enlarged_ventricles_0.png']\n","Start generating embeeding coreset and distribution coreset\n","GPU available: True, used: True\n","TPU available: False, using: 0 TPU cores\n","IPU available: False, using: 0 IPUs\n","coreset_generator_trainer完了\n","Using cache found in /root/.cache/torch/hub/pytorch_vision_v0.10.0\n","/usr/local/lib/python3.8/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and will be removed in 0.15, please use 'weights' instead.\n","  warnings.warn(\n","/usr/local/lib/python3.8/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and will be removed in 0.15. The current behavior is equivalent to passing `weights=Wide_ResNet101_2_Weights.IMAGENET1K_V1`. You can also use `weights=Wide_ResNet101_2_Weights.DEFAULT` to get the most up-to-date weights.\n","  warnings.warn(msg)\n","coreset_generator完了\n","LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n","/usr/local/lib/python3.8/dist-packages/pytorch_lightning/trainer/optimizers.py:37: UserWarning: `LightningModule.configure_optimizers` returned `None`, this fit will run with no optimizer\n","  rank_zero_warn(\n","Missing logger folder: ./result/enlarged_ventricles/WR101/coreset/lightning_logs\n","\n","  | Name     | Type   | Params\n","------------------------------------\n","0 | backbone | ResNet | 126 M \n","------------------------------------\n","126 M     Trainable params\n","0         Non-trainable params\n","126 M     Total params\n","507.547   Total estimated model params size (MB)\n","/usr/local/lib/python3.8/dist-packages/pytorch_lightning/trainer/data_loading.py:132: UserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n","  rank_zero_warn(\n","Epoch 0:   0% 0/726 [00:00<?, ?it/s] /usr/local/lib/python3.8/dist-packages/pytorch_lightning/loops/optimization/optimizer_loop.py:145: UserWarning: `training_step` returned `None`. If this was on purpose, ignore this warning...\n","  self.warning_cache.warn(\n","Epoch 0:  99% 720/726 [00:34<00:00, 20.73it/s, loss=nan, v_num=0]Size of embedding coreset without edge feature :  (1045, 1024)\n","Size of distribution coreset without edge feature :  (2048, 1024)\n","Epoch 0:  99% 720/726 [00:49<00:00, 14.40it/s, loss=nan, v_num=0]Size of embedding coreset with edge feature :  (1858, 1024)\n","Epoch 0: 100% 726/726 [01:00<00:00, 11.99it/s, loss=nan, v_num=0]\n","End generating embeeding coreset and distribution coreset\n","Using cache found in /root/.cache/torch/hub/pytorch_vision_v0.10.0\n","/usr/local/lib/python3.8/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and will be removed in 0.15, please use 'weights' instead.\n","  warnings.warn(\n","/usr/local/lib/python3.8/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and will be removed in 0.15. The current behavior is equivalent to passing `weights=Wide_ResNet101_2_Weights.IMAGENET1K_V1`. You can also use `weights=Wide_ResNet101_2_Weights.DEFAULT` to get the most up-to-date weights.\n","  warnings.warn(msg)\n","Start training normal feature distribution from neighbrohood information\n","GPU available: True, used: True\n","TPU available: False, using: 0 TPU cores\n","IPU available: False, using: 0 IPUs\n","LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n","Missing logger folder: ./result/enlarged_ventricles/WR101/distribution\n","\n","  | Name  | Type               | Params\n","---------------------------------------------\n","0 | model | Distribution_Model | 205 M \n","---------------------------------------------\n","205 M     Trainable params\n","0         Non-trainable params\n","205 M     Total params\n","822.313   Total estimated model params size (MB)\n","Validation sanity check: 0it [00:00, ?it/s]/usr/local/lib/python3.8/dist-packages/pytorch_lightning/trainer/data_loading.py:132: UserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n","  rank_zero_warn(\n","Global seed set to 23\n","/usr/local/lib/python3.8/dist-packages/pytorch_lightning/trainer/data_loading.py:132: UserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n","  rank_zero_warn(\n","Epoch 0: 100% 52/52 [00:59<00:00,  1.15s/it, loss=6.32, v_num=0, train_loss=5.890]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/6 [00:00<?, ?it/s]\u001b[A\n","Epoch 0: 100% 52/52 [01:11<00:00,  1.37s/it, loss=6.04, v_num=0, train_loss=5.670, valid_loss=5.780]\n","Epoch 1: 100% 52/52 [00:59<00:00,  1.14s/it, loss=4.68, v_num=0, train_loss=4.470, valid_loss=5.780]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/6 [00:00<?, ?it/s]\u001b[A\n","Epoch 1: 100% 52/52 [01:11<00:00,  1.37s/it, loss=4.53, v_num=0, train_loss=4.410, valid_loss=4.090]\n","Epoch 2: 100% 52/52 [00:59<00:00,  1.15s/it, loss=3.93, v_num=0, train_loss=3.830, valid_loss=4.090]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/6 [00:00<?, ?it/s]\u001b[A\n","Epoch 2: 100% 52/52 [01:12<00:00,  1.40s/it, loss=3.86, v_num=0, train_loss=3.790, valid_loss=3.530]\n","Epoch 3: 100% 52/52 [01:03<00:00,  1.23s/it, loss=3.55, v_num=0, train_loss=3.520, valid_loss=3.530]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/6 [00:00<?, ?it/s]\u001b[A\n","Epoch 3: 100% 52/52 [01:14<00:00,  1.44s/it, loss=3.51, v_num=0, train_loss=3.500, valid_loss=3.160]\n","Epoch 4: 100% 52/52 [01:00<00:00,  1.16s/it, loss=3.29, v_num=0, train_loss=3.250, valid_loss=3.160]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/6 [00:00<?, ?it/s]\u001b[A\n","Epoch 4: 100% 52/52 [01:13<00:00,  1.41s/it, loss=3.27, v_num=0, train_loss=3.260, valid_loss=2.840]\n","Epoch 5: 100% 52/52 [01:00<00:00,  1.17s/it, loss=3.04, v_num=0, train_loss=2.970, valid_loss=2.840]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/6 [00:00<?, ?it/s]\u001b[A\n","Epoch 5: 100% 52/52 [01:11<00:00,  1.38s/it, loss=3.04, v_num=0, train_loss=3.020, valid_loss=2.660]\n","Epoch 6: 100% 52/52 [01:02<00:00,  1.20s/it, loss=2.99, v_num=0, train_loss=2.950, valid_loss=2.660]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/6 [00:00<?, ?it/s]\u001b[A\n","Epoch 6: 100% 52/52 [01:12<00:00,  1.40s/it, loss=2.99, v_num=0, train_loss=3.020, valid_loss=2.640]\n","Epoch 7: 100% 52/52 [00:59<00:00,  1.14s/it, loss=2.97, v_num=0, train_loss=2.990, valid_loss=2.640]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/6 [00:00<?, ?it/s]\u001b[A\n","Epoch 7: 100% 52/52 [01:10<00:00,  1.35s/it, loss=2.97, v_num=0, train_loss=2.990, valid_loss=2.600]\n","Epoch 8: 100% 52/52 [01:02<00:00,  1.20s/it, loss=2.95, v_num=0, train_loss=2.900, valid_loss=2.600]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/6 [00:00<?, ?it/s]\u001b[A\n","Epoch 8: 100% 52/52 [01:15<00:00,  1.45s/it, loss=2.94, v_num=0, train_loss=2.880, valid_loss=2.580]\n","Epoch 9: 100% 52/52 [01:01<00:00,  1.18s/it, loss=2.91, v_num=0, train_loss=2.900, valid_loss=2.580]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/6 [00:00<?, ?it/s]\u001b[A\n","Epoch 9: 100% 52/52 [01:11<00:00,  1.38s/it, loss=2.92, v_num=0, train_loss=2.890, valid_loss=2.560]\n","Epoch 10: 100% 52/52 [00:59<00:00,  1.14s/it, loss=2.88, v_num=0, train_loss=2.900, valid_loss=2.560]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/6 [00:00<?, ?it/s]\u001b[A\n","Epoch 10: 100% 52/52 [01:10<00:00,  1.36s/it, loss=2.89, v_num=0, train_loss=2.970, valid_loss=2.550]\n","Epoch 11: 100% 52/52 [00:59<00:00,  1.14s/it, loss=2.88, v_num=0, train_loss=2.850, valid_loss=2.550]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/6 [00:00<?, ?it/s]\u001b[A\n","Epoch 11: 100% 52/52 [01:12<00:00,  1.39s/it, loss=2.88, v_num=0, train_loss=2.880, valid_loss=2.550]\n","Epoch 12: 100% 52/52 [00:59<00:00,  1.15s/it, loss=2.88, v_num=0, train_loss=2.860, valid_loss=2.550]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/6 [00:00<?, ?it/s]\u001b[A\n","Epoch 12: 100% 52/52 [01:10<00:00,  1.35s/it, loss=2.89, v_num=0, train_loss=2.920, valid_loss=2.550]\n","Epoch 13: 100% 52/52 [00:59<00:00,  1.14s/it, loss=2.87, v_num=0, train_loss=2.840, valid_loss=2.550]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 13: 100% 52/52 [01:11<00:00,  1.38s/it, loss=2.87, v_num=0, train_loss=2.840, valid_loss=2.550]\n","Epoch 13: 100% 52/52 [01:11<00:00,  1.38s/it, loss=2.88, v_num=0, train_loss=2.940, valid_loss=2.540]\n","Epoch 14: 100% 52/52 [00:59<00:00,  1.14s/it, loss=2.88, v_num=0, train_loss=2.880, valid_loss=2.540]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 14: 100% 52/52 [01:09<00:00,  1.34s/it, loss=2.88, v_num=0, train_loss=2.880, valid_loss=2.540]\n","Epoch 14: 100% 52/52 [01:10<00:00,  1.36s/it, loss=2.88, v_num=0, train_loss=2.830, valid_loss=2.540]\n","Epoch 14: 100% 52/52 [01:10<00:00,  1.36s/it, loss=2.88, v_num=0, train_loss=2.830, valid_loss=2.540]\n","End training normal feature distribution from neighbrohood information\n","Start training normal feature distribution from position information\n","Using cache found in /root/.cache/torch/hub/pytorch_vision_v0.10.0\n","Using cache found in /root/.cache/torch/hub/pytorch_vision_v0.10.0\n","End training normal feature distribution from position information\n","Start evaluating anomaly score\n","GPU available: True, used: True\n","TPU available: False, using: 0 TPU cores\n","IPU available: False, using: 0 IPUs\n","Using cache found in /root/.cache/torch/hub/pytorch_vision_v0.10.0\n","LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n","/usr/local/lib/python3.8/dist-packages/pytorch_lightning/trainer/data_loading.py:132: UserWarning: The dataloader, test_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n","  rank_zero_warn(\n","Testing: 100% 16/16 [01:42<00:00,  6.40s/it]--------------------------------------------------------------------------------\n","DATALOADER:0 TEST RESULTS\n","{'pixel_auc_coor': 0.9456888820367888,\n"," 'pixel_auc_nb': 0.9167918680493197,\n"," 'pixel_auc_nb_coor': 0.9309614624478051,\n"," 'pixel_auc_patchcore': 0.9426132717465987}\n","--------------------------------------------------------------------------------\n","Testing: 100% 16/16 [01:42<00:00,  6.40s/it]\n","End evaluating anomaly score\n","Global seed set to 23\n","1\n","1\n","['../dataset/UAD/intraventricular/test/anomaly_images/file_brain_AXT1_202_6000391.png']\n","['../dataset/UAD/intraventricular/ground_truth/anomaly_images/file_brain_AXT1_202_6000391_intraventricular_substance_0.png']\n","Start generating embeeding coreset and distribution coreset\n","GPU available: True, used: True\n","TPU available: False, using: 0 TPU cores\n","IPU available: False, using: 0 IPUs\n","coreset_generator_trainer完了\n","Using cache found in /root/.cache/torch/hub/pytorch_vision_v0.10.0\n","/usr/local/lib/python3.8/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and will be removed in 0.15, please use 'weights' instead.\n","  warnings.warn(\n","/usr/local/lib/python3.8/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and will be removed in 0.15. The current behavior is equivalent to passing `weights=Wide_ResNet101_2_Weights.IMAGENET1K_V1`. You can also use `weights=Wide_ResNet101_2_Weights.DEFAULT` to get the most up-to-date weights.\n","  warnings.warn(msg)\n","coreset_generator完了\n","LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n","/usr/local/lib/python3.8/dist-packages/pytorch_lightning/trainer/optimizers.py:37: UserWarning: `LightningModule.configure_optimizers` returned `None`, this fit will run with no optimizer\n","  rank_zero_warn(\n","Missing logger folder: ./result/intraventricular/WR101/coreset/lightning_logs\n","\n","  | Name     | Type   | Params\n","------------------------------------\n","0 | backbone | ResNet | 126 M \n","------------------------------------\n","126 M     Trainable params\n","0         Non-trainable params\n","126 M     Total params\n","507.547   Total estimated model params size (MB)\n","/usr/local/lib/python3.8/dist-packages/pytorch_lightning/trainer/data_loading.py:132: UserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n","  rank_zero_warn(\n","Epoch 0:   0% 0/726 [00:00<?, ?it/s] /usr/local/lib/python3.8/dist-packages/pytorch_lightning/loops/optimization/optimizer_loop.py:145: UserWarning: `training_step` returned `None`. If this was on purpose, ignore this warning...\n","  self.warning_cache.warn(\n","Epoch 0:  99% 720/726 [00:35<00:00, 20.38it/s, loss=nan, v_num=0]Size of embedding coreset without edge feature :  (1045, 1024)\n","Size of distribution coreset without edge feature :  (2048, 1024)\n","Epoch 0:  99% 720/726 [00:50<00:00, 14.40it/s, loss=nan, v_num=0]Size of embedding coreset with edge feature :  (1858, 1024)\n","Epoch 0: 100% 726/726 [01:01<00:00, 11.87it/s, loss=nan, v_num=0]\n","End generating embeeding coreset and distribution coreset\n","Using cache found in /root/.cache/torch/hub/pytorch_vision_v0.10.0\n","/usr/local/lib/python3.8/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and will be removed in 0.15, please use 'weights' instead.\n","  warnings.warn(\n","/usr/local/lib/python3.8/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and will be removed in 0.15. The current behavior is equivalent to passing `weights=Wide_ResNet101_2_Weights.IMAGENET1K_V1`. You can also use `weights=Wide_ResNet101_2_Weights.DEFAULT` to get the most up-to-date weights.\n","  warnings.warn(msg)\n","Start training normal feature distribution from neighbrohood information\n","GPU available: True, used: True\n","TPU available: False, using: 0 TPU cores\n","IPU available: False, using: 0 IPUs\n","LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n","Missing logger folder: ./result/intraventricular/WR101/distribution\n","\n","  | Name  | Type               | Params\n","---------------------------------------------\n","0 | model | Distribution_Model | 205 M \n","---------------------------------------------\n","205 M     Trainable params\n","0         Non-trainable params\n","205 M     Total params\n","822.313   Total estimated model params size (MB)\n","Validation sanity check: 0it [00:00, ?it/s]/usr/local/lib/python3.8/dist-packages/pytorch_lightning/trainer/data_loading.py:132: UserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n","  rank_zero_warn(\n","Global seed set to 23\n","/usr/local/lib/python3.8/dist-packages/pytorch_lightning/trainer/data_loading.py:132: UserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n","  rank_zero_warn(\n","Epoch 0: 100% 52/52 [00:56<00:00,  1.09s/it, loss=6.32, v_num=0, train_loss=5.890]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/6 [00:00<?, ?it/s]\u001b[A\n","Epoch 0: 100% 52/52 [01:07<00:00,  1.29s/it, loss=6.04, v_num=0, train_loss=5.670, valid_loss=5.780]\n","Epoch 1: 100% 52/52 [00:57<00:00,  1.10s/it, loss=4.68, v_num=0, train_loss=4.470, valid_loss=5.780]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 1: 100% 52/52 [01:07<00:00,  1.30s/it, loss=4.68, v_num=0, train_loss=4.470, valid_loss=5.780]\n","Epoch 1: 100% 52/52 [01:08<00:00,  1.31s/it, loss=4.53, v_num=0, train_loss=4.410, valid_loss=4.090]\n","Epoch 2: 100% 52/52 [00:56<00:00,  1.09s/it, loss=3.93, v_num=0, train_loss=3.830, valid_loss=4.090]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/6 [00:00<?, ?it/s]\u001b[A\n","Epoch 2: 100% 52/52 [01:06<00:00,  1.29s/it, loss=3.86, v_num=0, train_loss=3.790, valid_loss=3.530]\n","Epoch 3: 100% 52/52 [00:56<00:00,  1.09s/it, loss=3.55, v_num=0, train_loss=3.520, valid_loss=3.530]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/6 [00:00<?, ?it/s]\u001b[A\n","Epoch 3: 100% 52/52 [01:09<00:00,  1.33s/it, loss=3.51, v_num=0, train_loss=3.500, valid_loss=3.160]\n","Epoch 4: 100% 52/52 [00:56<00:00,  1.08s/it, loss=3.29, v_num=0, train_loss=3.250, valid_loss=3.160]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/6 [00:00<?, ?it/s]\u001b[A\n","Epoch 4: 100% 52/52 [01:08<00:00,  1.33s/it, loss=3.27, v_num=0, train_loss=3.260, valid_loss=2.840]\n","Epoch 5: 100% 52/52 [00:56<00:00,  1.09s/it, loss=3.04, v_num=0, train_loss=2.970, valid_loss=2.840]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/6 [00:00<?, ?it/s]\u001b[A\n","Epoch 5: 100% 52/52 [01:10<00:00,  1.35s/it, loss=3.04, v_num=0, train_loss=3.020, valid_loss=2.660]\n","Epoch 6: 100% 52/52 [00:57<00:00,  1.10s/it, loss=2.99, v_num=0, train_loss=2.950, valid_loss=2.660]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/6 [00:00<?, ?it/s]\u001b[A\n","Epoch 6: 100% 52/52 [01:07<00:00,  1.30s/it, loss=2.99, v_num=0, train_loss=3.020, valid_loss=2.640]\n","Epoch 7: 100% 52/52 [00:57<00:00,  1.10s/it, loss=2.97, v_num=0, train_loss=2.990, valid_loss=2.640]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 7: 100% 52/52 [01:07<00:00,  1.29s/it, loss=2.97, v_num=0, train_loss=2.990, valid_loss=2.640]\n","Epoch 7: 100% 52/52 [01:09<00:00,  1.33s/it, loss=2.97, v_num=0, train_loss=2.990, valid_loss=2.600]\n","Epoch 8: 100% 52/52 [00:56<00:00,  1.09s/it, loss=2.95, v_num=0, train_loss=2.900, valid_loss=2.600]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 8: 100% 52/52 [01:08<00:00,  1.31s/it, loss=2.95, v_num=0, train_loss=2.900, valid_loss=2.600]\n","Epoch 8: 100% 52/52 [01:09<00:00,  1.33s/it, loss=2.94, v_num=0, train_loss=2.880, valid_loss=2.580]\n","Epoch 9: 100% 52/52 [00:58<00:00,  1.12s/it, loss=2.91, v_num=0, train_loss=2.900, valid_loss=2.580]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 9: 100% 52/52 [01:08<00:00,  1.33s/it, loss=2.91, v_num=0, train_loss=2.900, valid_loss=2.580]\n","Epoch 9: 100% 52/52 [01:09<00:00,  1.34s/it, loss=2.92, v_num=0, train_loss=2.890, valid_loss=2.560]\n","Epoch 10: 100% 52/52 [00:56<00:00,  1.09s/it, loss=2.88, v_num=0, train_loss=2.900, valid_loss=2.560]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 10: 100% 52/52 [01:09<00:00,  1.34s/it, loss=2.88, v_num=0, train_loss=2.900, valid_loss=2.560]\n","Epoch 10: 100% 52/52 [01:09<00:00,  1.34s/it, loss=2.89, v_num=0, train_loss=2.970, valid_loss=2.550]\n","Epoch 11: 100% 52/52 [00:58<00:00,  1.12s/it, loss=2.88, v_num=0, train_loss=2.850, valid_loss=2.550]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/6 [00:00<?, ?it/s]\u001b[A\n","Epoch 11: 100% 52/52 [01:08<00:00,  1.32s/it, loss=2.88, v_num=0, train_loss=2.880, valid_loss=2.550]\n","Epoch 12: 100% 52/52 [00:57<00:00,  1.11s/it, loss=2.88, v_num=0, train_loss=2.860, valid_loss=2.550]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/6 [00:00<?, ?it/s]\u001b[A\n","Epoch 12: 100% 52/52 [01:07<00:00,  1.31s/it, loss=2.89, v_num=0, train_loss=2.920, valid_loss=2.550]\n","Epoch 13: 100% 52/52 [00:57<00:00,  1.11s/it, loss=2.87, v_num=0, train_loss=2.840, valid_loss=2.550]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/6 [00:00<?, ?it/s]\u001b[A\n","Epoch 13: 100% 52/52 [01:08<00:00,  1.32s/it, loss=2.88, v_num=0, train_loss=2.940, valid_loss=2.540]\n","Epoch 14: 100% 52/52 [00:57<00:00,  1.11s/it, loss=2.88, v_num=0, train_loss=2.880, valid_loss=2.540]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/6 [00:00<?, ?it/s]\u001b[A\n","Epoch 14: 100% 52/52 [01:09<00:00,  1.34s/it, loss=2.88, v_num=0, train_loss=2.830, valid_loss=2.540]\n","Epoch 14: 100% 52/52 [01:09<00:00,  1.34s/it, loss=2.88, v_num=0, train_loss=2.830, valid_loss=2.540]\n","End training normal feature distribution from neighbrohood information\n","Start training normal feature distribution from position information\n","Using cache found in /root/.cache/torch/hub/pytorch_vision_v0.10.0\n","Using cache found in /root/.cache/torch/hub/pytorch_vision_v0.10.0\n","End training normal feature distribution from position information\n","Start evaluating anomaly score\n","GPU available: True, used: True\n","TPU available: False, using: 0 TPU cores\n","IPU available: False, using: 0 IPUs\n","Using cache found in /root/.cache/torch/hub/pytorch_vision_v0.10.0\n","LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n","/usr/local/lib/python3.8/dist-packages/pytorch_lightning/trainer/data_loading.py:132: UserWarning: The dataloader, test_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n","  rank_zero_warn(\n","Testing: 100% 1/1 [00:11<00:00, 11.39s/it]--------------------------------------------------------------------------------\n","DATALOADER:0 TEST RESULTS\n","{'pixel_auc_coor': 0.9636692647931854,\n"," 'pixel_auc_nb': 0.9315988832629225,\n"," 'pixel_auc_nb_coor': 0.9532999485724544,\n"," 'pixel_auc_patchcore': 0.9115404146302739}\n","--------------------------------------------------------------------------------\n","Testing: 100% 1/1 [00:11<00:00, 11.39s/it]\n","End evaluating anomaly score\n","Global seed set to 23\n","19\n","19\n","['../dataset/UAD/lesions/test/anomaly_images/file_brain_AXT1_201_6002761.png', '../dataset/UAD/lesions/test/anomaly_images/file_brain_AXT1_201_6002793.png', '../dataset/UAD/lesions/test/anomaly_images/file_brain_AXT1_201_6002875.png', '../dataset/UAD/lesions/test/anomaly_images/file_brain_AXT1_201_6002900.png', '../dataset/UAD/lesions/test/anomaly_images/file_brain_AXT1_202_2020009.png', '../dataset/UAD/lesions/test/anomaly_images/file_brain_AXT1_202_2020111.png', '../dataset/UAD/lesions/test/anomaly_images/file_brain_AXT1_202_2020143.png', '../dataset/UAD/lesions/test/anomaly_images/file_brain_AXT1_202_2020194.png', '../dataset/UAD/lesions/test/anomaly_images/file_brain_AXT1_202_2020211.png', '../dataset/UAD/lesions/test/anomaly_images/file_brain_AXT1_202_2020320.png', '../dataset/UAD/lesions/test/anomaly_images/file_brain_AXT1_202_2020495.png', '../dataset/UAD/lesions/test/anomaly_images/file_brain_AXT1_202_2020590.png', '../dataset/UAD/lesions/test/anomaly_images/file_brain_AXT1_202_6000319.png', '../dataset/UAD/lesions/test/anomaly_images/file_brain_AXT1_202_6000332.png', '../dataset/UAD/lesions/test/anomaly_images/file_brain_AXT1_202_6000333.png', '../dataset/UAD/lesions/test/anomaly_images/file_brain_AXT1_202_6000416.png', '../dataset/UAD/lesions/test/anomaly_images/file_brain_AXT1_202_6000448.png', '../dataset/UAD/lesions/test/anomaly_images/file_brain_AXT1_206_2060051.png', '../dataset/UAD/lesions/test/anomaly_images/file_brain_AXT1_206_2060078.png']\n","['../dataset/UAD/lesions/ground_truth/anomaly_images/file_brain_AXT1_201_6002761_nonspecific_lesion_0.png', '../dataset/UAD/lesions/ground_truth/anomaly_images/file_brain_AXT1_201_6002793_nonspecific_lesion_0.png', '../dataset/UAD/lesions/ground_truth/anomaly_images/file_brain_AXT1_201_6002875_nonspecific_lesion_0.png', '../dataset/UAD/lesions/ground_truth/anomaly_images/file_brain_AXT1_201_6002900_combined.png', '../dataset/UAD/lesions/ground_truth/anomaly_images/file_brain_AXT1_202_2020009_nonspecific_lesion_1.png', '../dataset/UAD/lesions/ground_truth/anomaly_images/file_brain_AXT1_202_2020111_nonspecific_lesion_0.png', '../dataset/UAD/lesions/ground_truth/anomaly_images/file_brain_AXT1_202_2020143_nonspecific_lesion_0.png', '../dataset/UAD/lesions/ground_truth/anomaly_images/file_brain_AXT1_202_2020194_nonspecific_lesion_0.png', '../dataset/UAD/lesions/ground_truth/anomaly_images/file_brain_AXT1_202_2020211_nonspecific_lesion_0.png', '../dataset/UAD/lesions/ground_truth/anomaly_images/file_brain_AXT1_202_2020320_nonspecific_lesion_0.png', '../dataset/UAD/lesions/ground_truth/anomaly_images/file_brain_AXT1_202_2020495_nonspecific_lesion_0.png', '../dataset/UAD/lesions/ground_truth/anomaly_images/file_brain_AXT1_202_2020590_nonspecific_lesion_0.png', '../dataset/UAD/lesions/ground_truth/anomaly_images/file_brain_AXT1_202_6000319_nonspecific_lesion_1.png', '../dataset/UAD/lesions/ground_truth/anomaly_images/file_brain_AXT1_202_6000332_nonspecific_lesion_0.png', '../dataset/UAD/lesions/ground_truth/anomaly_images/file_brain_AXT1_202_6000333_nonspecific_lesion_0.png', '../dataset/UAD/lesions/ground_truth/anomaly_images/file_brain_AXT1_202_6000416_nonspecific_lesion_0.png', '../dataset/UAD/lesions/ground_truth/anomaly_images/file_brain_AXT1_202_6000448_combined.png', '../dataset/UAD/lesions/ground_truth/anomaly_images/file_brain_AXT1_206_2060051_nonspecific_lesion_0.png', '../dataset/UAD/lesions/ground_truth/anomaly_images/file_brain_AXT1_206_2060078_nonspecific_lesion_1.png']\n","Start generating embeeding coreset and distribution coreset\n","GPU available: True, used: True\n","TPU available: False, using: 0 TPU cores\n","IPU available: False, using: 0 IPUs\n","coreset_generator_trainer完了\n","Using cache found in /root/.cache/torch/hub/pytorch_vision_v0.10.0\n","/usr/local/lib/python3.8/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and will be removed in 0.15, please use 'weights' instead.\n","  warnings.warn(\n","/usr/local/lib/python3.8/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and will be removed in 0.15. The current behavior is equivalent to passing `weights=Wide_ResNet101_2_Weights.IMAGENET1K_V1`. You can also use `weights=Wide_ResNet101_2_Weights.DEFAULT` to get the most up-to-date weights.\n","  warnings.warn(msg)\n","coreset_generator完了\n","LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n","/usr/local/lib/python3.8/dist-packages/pytorch_lightning/trainer/optimizers.py:37: UserWarning: `LightningModule.configure_optimizers` returned `None`, this fit will run with no optimizer\n","  rank_zero_warn(\n","Missing logger folder: ./result/lesions/WR101/coreset/lightning_logs\n","\n","  | Name     | Type   | Params\n","------------------------------------\n","0 | backbone | ResNet | 126 M \n","------------------------------------\n","126 M     Trainable params\n","0         Non-trainable params\n","126 M     Total params\n","507.547   Total estimated model params size (MB)\n","/usr/local/lib/python3.8/dist-packages/pytorch_lightning/trainer/data_loading.py:132: UserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n","  rank_zero_warn(\n","Epoch 0:   0% 0/726 [00:00<?, ?it/s] /usr/local/lib/python3.8/dist-packages/pytorch_lightning/loops/optimization/optimizer_loop.py:145: UserWarning: `training_step` returned `None`. If this was on purpose, ignore this warning...\n","  self.warning_cache.warn(\n","Epoch 0:  99% 720/726 [00:34<00:00, 20.61it/s, loss=nan, v_num=0]Size of embedding coreset without edge feature :  (1045, 1024)\n","Size of distribution coreset without edge feature :  (2048, 1024)\n","Epoch 0:  99% 720/726 [00:49<00:00, 14.40it/s, loss=nan, v_num=0]Size of embedding coreset with edge feature :  (1858, 1024)\n","Epoch 0: 100% 726/726 [01:00<00:00, 11.93it/s, loss=nan, v_num=0]\n","End generating embeeding coreset and distribution coreset\n","Using cache found in /root/.cache/torch/hub/pytorch_vision_v0.10.0\n","/usr/local/lib/python3.8/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and will be removed in 0.15, please use 'weights' instead.\n","  warnings.warn(\n","/usr/local/lib/python3.8/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and will be removed in 0.15. The current behavior is equivalent to passing `weights=Wide_ResNet101_2_Weights.IMAGENET1K_V1`. You can also use `weights=Wide_ResNet101_2_Weights.DEFAULT` to get the most up-to-date weights.\n","  warnings.warn(msg)\n","Start training normal feature distribution from neighbrohood information\n","GPU available: True, used: True\n","TPU available: False, using: 0 TPU cores\n","IPU available: False, using: 0 IPUs\n","LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n","Missing logger folder: ./result/lesions/WR101/distribution\n","\n","  | Name  | Type               | Params\n","---------------------------------------------\n","0 | model | Distribution_Model | 205 M \n","---------------------------------------------\n","205 M     Trainable params\n","0         Non-trainable params\n","205 M     Total params\n","822.313   Total estimated model params size (MB)\n","Validation sanity check: 0it [00:00, ?it/s]/usr/local/lib/python3.8/dist-packages/pytorch_lightning/trainer/data_loading.py:132: UserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n","  rank_zero_warn(\n","Global seed set to 23\n","/usr/local/lib/python3.8/dist-packages/pytorch_lightning/trainer/data_loading.py:132: UserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n","  rank_zero_warn(\n","Epoch 0: 100% 52/52 [00:58<00:00,  1.13s/it, loss=6.32, v_num=0, train_loss=5.890]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/6 [00:00<?, ?it/s]\u001b[A\n","Epoch 0: 100% 52/52 [01:09<00:00,  1.35s/it, loss=6.04, v_num=0, train_loss=5.670, valid_loss=5.780]\n","Epoch 1: 100% 52/52 [01:00<00:00,  1.16s/it, loss=4.68, v_num=0, train_loss=4.470, valid_loss=5.780]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/6 [00:00<?, ?it/s]\u001b[A\n","Epoch 1: 100% 52/52 [01:10<00:00,  1.36s/it, loss=4.53, v_num=0, train_loss=4.410, valid_loss=4.090]\n","Epoch 2: 100% 52/52 [01:00<00:00,  1.17s/it, loss=3.93, v_num=0, train_loss=3.830, valid_loss=4.090]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/6 [00:00<?, ?it/s]\u001b[A\n","Epoch 2: 100% 52/52 [01:12<00:00,  1.40s/it, loss=3.86, v_num=0, train_loss=3.790, valid_loss=3.530]\n","Epoch 3: 100% 52/52 [01:00<00:00,  1.17s/it, loss=3.55, v_num=0, train_loss=3.520, valid_loss=3.530]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/6 [00:00<?, ?it/s]\u001b[A\n","Epoch 3: 100% 52/52 [01:11<00:00,  1.38s/it, loss=3.51, v_num=0, train_loss=3.500, valid_loss=3.160]\n","Epoch 4: 100% 52/52 [00:59<00:00,  1.15s/it, loss=3.29, v_num=0, train_loss=3.250, valid_loss=3.160]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 4: 100% 52/52 [01:10<00:00,  1.36s/it, loss=3.29, v_num=0, train_loss=3.250, valid_loss=3.160]\n","Epoch 4: 100% 52/52 [01:11<00:00,  1.37s/it, loss=3.27, v_num=0, train_loss=3.260, valid_loss=2.840]\n","Epoch 5: 100% 52/52 [01:00<00:00,  1.16s/it, loss=3.04, v_num=0, train_loss=2.970, valid_loss=2.840]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/6 [00:00<?, ?it/s]\u001b[A\n","Epoch 5: 100% 52/52 [01:12<00:00,  1.39s/it, loss=3.04, v_num=0, train_loss=3.020, valid_loss=2.660]\n","Epoch 6: 100% 52/52 [01:00<00:00,  1.16s/it, loss=2.99, v_num=0, train_loss=2.950, valid_loss=2.660]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/6 [00:00<?, ?it/s]\u001b[A\n","Epoch 6: 100% 52/52 [01:11<00:00,  1.38s/it, loss=2.99, v_num=0, train_loss=3.020, valid_loss=2.640]\n","Epoch 7: 100% 52/52 [01:00<00:00,  1.16s/it, loss=2.97, v_num=0, train_loss=2.990, valid_loss=2.640]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/6 [00:00<?, ?it/s]\u001b[A\n","Epoch 7: 100% 52/52 [01:12<00:00,  1.39s/it, loss=2.97, v_num=0, train_loss=2.990, valid_loss=2.600]\n","Epoch 8: 100% 52/52 [01:01<00:00,  1.17s/it, loss=2.95, v_num=0, train_loss=2.900, valid_loss=2.600]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/6 [00:00<?, ?it/s]\u001b[A\n","Epoch 8: 100% 52/52 [01:11<00:00,  1.38s/it, loss=2.94, v_num=0, train_loss=2.880, valid_loss=2.580]\n","Epoch 9: 100% 52/52 [01:00<00:00,  1.17s/it, loss=2.91, v_num=0, train_loss=2.900, valid_loss=2.580]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 9: 100% 52/52 [01:11<00:00,  1.37s/it, loss=2.91, v_num=0, train_loss=2.900, valid_loss=2.580]\n","Epoch 9: 100% 52/52 [01:13<00:00,  1.41s/it, loss=2.92, v_num=0, train_loss=2.890, valid_loss=2.560]\n","Epoch 10: 100% 52/52 [01:00<00:00,  1.16s/it, loss=2.88, v_num=0, train_loss=2.900, valid_loss=2.560]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/6 [00:00<?, ?it/s]\u001b[A\n","Epoch 10: 100% 52/52 [01:13<00:00,  1.41s/it, loss=2.89, v_num=0, train_loss=2.970, valid_loss=2.550]\n","Epoch 11: 100% 52/52 [00:59<00:00,  1.15s/it, loss=2.88, v_num=0, train_loss=2.850, valid_loss=2.550]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/6 [00:00<?, ?it/s]\u001b[A\n","Epoch 11: 100% 52/52 [01:12<00:00,  1.39s/it, loss=2.88, v_num=0, train_loss=2.880, valid_loss=2.550]\n","Epoch 12: 100% 52/52 [00:59<00:00,  1.15s/it, loss=2.88, v_num=0, train_loss=2.860, valid_loss=2.550]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/6 [00:00<?, ?it/s]\u001b[A\n","Epoch 12: 100% 52/52 [01:10<00:00,  1.36s/it, loss=2.89, v_num=0, train_loss=2.920, valid_loss=2.550]\n","Epoch 13: 100% 52/52 [00:59<00:00,  1.15s/it, loss=2.87, v_num=0, train_loss=2.840, valid_loss=2.550]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/6 [00:00<?, ?it/s]\u001b[A\n","Epoch 13: 100% 52/52 [01:10<00:00,  1.36s/it, loss=2.88, v_num=0, train_loss=2.940, valid_loss=2.540]\n","Epoch 14: 100% 52/52 [01:00<00:00,  1.16s/it, loss=2.88, v_num=0, train_loss=2.880, valid_loss=2.540]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/6 [00:00<?, ?it/s]\u001b[A\n","Epoch 14: 100% 52/52 [01:13<00:00,  1.41s/it, loss=2.88, v_num=0, train_loss=2.830, valid_loss=2.540]\n","Epoch 14: 100% 52/52 [01:13<00:00,  1.41s/it, loss=2.88, v_num=0, train_loss=2.830, valid_loss=2.540]\n","End training normal feature distribution from neighbrohood information\n","Start training normal feature distribution from position information\n","Using cache found in /root/.cache/torch/hub/pytorch_vision_v0.10.0\n","Using cache found in /root/.cache/torch/hub/pytorch_vision_v0.10.0\n","End training normal feature distribution from position information\n","Start evaluating anomaly score\n","GPU available: True, used: True\n","TPU available: False, using: 0 TPU cores\n","IPU available: False, using: 0 IPUs\n","Using cache found in /root/.cache/torch/hub/pytorch_vision_v0.10.0\n","LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n","/usr/local/lib/python3.8/dist-packages/pytorch_lightning/trainer/data_loading.py:132: UserWarning: The dataloader, test_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n","  rank_zero_warn(\n","Testing: 100% 19/19 [02:16<00:00,  7.21s/it]--------------------------------------------------------------------------------\n","DATALOADER:0 TEST RESULTS\n","{'pixel_auc_coor': 0.903856218884914,\n"," 'pixel_auc_nb': 0.8760679693014182,\n"," 'pixel_auc_nb_coor': 0.8816161347537668,\n"," 'pixel_auc_patchcore': 0.9089237499533249}\n","--------------------------------------------------------------------------------\n","Testing: 100% 19/19 [02:16<00:00,  7.21s/it]\n","End evaluating anomaly score\n","Global seed set to 23\n","14\n","14\n","['../dataset/UAD/mass/test/anomaly_images/file_brain_AXT1_202_2020041.png', '../dataset/UAD/mass/test/anomaly_images/file_brain_AXT1_202_2020080.png', '../dataset/UAD/mass/test/anomaly_images/file_brain_AXT1_202_2020181.png', '../dataset/UAD/mass/test/anomaly_images/file_brain_AXT1_202_2020283.png', '../dataset/UAD/mass/test/anomaly_images/file_brain_AXT1_202_2020318.png', '../dataset/UAD/mass/test/anomaly_images/file_brain_AXT1_202_2020336.png', '../dataset/UAD/mass/test/anomaly_images/file_brain_AXT1_202_2020530.png', '../dataset/UAD/mass/test/anomaly_images/file_brain_AXT1_202_2020576.png', '../dataset/UAD/mass/test/anomaly_images/file_brain_AXT1_202_2020581.png', '../dataset/UAD/mass/test/anomaly_images/file_brain_AXT1_202_2020584.png', '../dataset/UAD/mass/test/anomaly_images/file_brain_AXT1_202_2020585.png', '../dataset/UAD/mass/test/anomaly_images/file_brain_AXT1_202_6000264.png', '../dataset/UAD/mass/test/anomaly_images/file_brain_AXT1_202_6000465.png', '../dataset/UAD/mass/test/anomaly_images/file_brain_AXT1_202_6000577.png']\n","['../dataset/UAD/mass/ground_truth/anomaly_images/file_brain_AXT1_202_2020041_mass_1.png', '../dataset/UAD/mass/ground_truth/anomaly_images/file_brain_AXT1_202_2020080_combined.png', '../dataset/UAD/mass/ground_truth/anomaly_images/file_brain_AXT1_202_2020181_combined.png', '../dataset/UAD/mass/ground_truth/anomaly_images/file_brain_AXT1_202_2020283_mass_0.png', '../dataset/UAD/mass/ground_truth/anomaly_images/file_brain_AXT1_202_2020318_combined.png', '../dataset/UAD/mass/ground_truth/anomaly_images/file_brain_AXT1_202_2020336_mass_0.png', '../dataset/UAD/mass/ground_truth/anomaly_images/file_brain_AXT1_202_2020530_mass_0.png', '../dataset/UAD/mass/ground_truth/anomaly_images/file_brain_AXT1_202_2020576_mass_1.png', '../dataset/UAD/mass/ground_truth/anomaly_images/file_brain_AXT1_202_2020581_combined.png', '../dataset/UAD/mass/ground_truth/anomaly_images/file_brain_AXT1_202_2020584_mass_0.png', '../dataset/UAD/mass/ground_truth/anomaly_images/file_brain_AXT1_202_2020585_combined.png', '../dataset/UAD/mass/ground_truth/anomaly_images/file_brain_AXT1_202_6000264_mass_2.png', '../dataset/UAD/mass/ground_truth/anomaly_images/file_brain_AXT1_202_6000465_mass_1.png', '../dataset/UAD/mass/ground_truth/anomaly_images/file_brain_AXT1_202_6000577_mass_1.png']\n","Start generating embeeding coreset and distribution coreset\n","GPU available: True, used: True\n","TPU available: False, using: 0 TPU cores\n","IPU available: False, using: 0 IPUs\n","coreset_generator_trainer完了\n","Using cache found in /root/.cache/torch/hub/pytorch_vision_v0.10.0\n","/usr/local/lib/python3.8/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and will be removed in 0.15, please use 'weights' instead.\n","  warnings.warn(\n","/usr/local/lib/python3.8/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and will be removed in 0.15. The current behavior is equivalent to passing `weights=Wide_ResNet101_2_Weights.IMAGENET1K_V1`. You can also use `weights=Wide_ResNet101_2_Weights.DEFAULT` to get the most up-to-date weights.\n","  warnings.warn(msg)\n","coreset_generator完了\n","LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n","/usr/local/lib/python3.8/dist-packages/pytorch_lightning/trainer/optimizers.py:37: UserWarning: `LightningModule.configure_optimizers` returned `None`, this fit will run with no optimizer\n","  rank_zero_warn(\n","Missing logger folder: ./result/mass/WR101/coreset/lightning_logs\n","\n","  | Name     | Type   | Params\n","------------------------------------\n","0 | backbone | ResNet | 126 M \n","------------------------------------\n","126 M     Trainable params\n","0         Non-trainable params\n","126 M     Total params\n","507.547   Total estimated model params size (MB)\n","/usr/local/lib/python3.8/dist-packages/pytorch_lightning/trainer/data_loading.py:132: UserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n","  rank_zero_warn(\n","Epoch 0:   0% 0/726 [00:00<?, ?it/s] /usr/local/lib/python3.8/dist-packages/pytorch_lightning/loops/optimization/optimizer_loop.py:145: UserWarning: `training_step` returned `None`. If this was on purpose, ignore this warning...\n","  self.warning_cache.warn(\n","Epoch 0:  99% 720/726 [00:36<00:00, 19.97it/s, loss=nan, v_num=0]Size of embedding coreset without edge feature :  (1045, 1024)\n","Size of distribution coreset without edge feature :  (2048, 1024)\n","Epoch 0:  99% 720/726 [00:50<00:00, 14.40it/s, loss=nan, v_num=0]Size of embedding coreset with edge feature :  (1858, 1024)\n","Epoch 0: 100% 726/726 [01:01<00:00, 11.74it/s, loss=nan, v_num=0]\n","End generating embeeding coreset and distribution coreset\n","Using cache found in /root/.cache/torch/hub/pytorch_vision_v0.10.0\n","/usr/local/lib/python3.8/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and will be removed in 0.15, please use 'weights' instead.\n","  warnings.warn(\n","/usr/local/lib/python3.8/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and will be removed in 0.15. The current behavior is equivalent to passing `weights=Wide_ResNet101_2_Weights.IMAGENET1K_V1`. You can also use `weights=Wide_ResNet101_2_Weights.DEFAULT` to get the most up-to-date weights.\n","  warnings.warn(msg)\n","Start training normal feature distribution from neighbrohood information\n","GPU available: True, used: True\n","TPU available: False, using: 0 TPU cores\n","IPU available: False, using: 0 IPUs\n","LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n","Missing logger folder: ./result/mass/WR101/distribution\n","\n","  | Name  | Type               | Params\n","---------------------------------------------\n","0 | model | Distribution_Model | 205 M \n","---------------------------------------------\n","205 M     Trainable params\n","0         Non-trainable params\n","205 M     Total params\n","822.313   Total estimated model params size (MB)\n","Validation sanity check: 0it [00:00, ?it/s]/usr/local/lib/python3.8/dist-packages/pytorch_lightning/trainer/data_loading.py:132: UserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n","  rank_zero_warn(\n","Global seed set to 23\n","/usr/local/lib/python3.8/dist-packages/pytorch_lightning/trainer/data_loading.py:132: UserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n","  rank_zero_warn(\n","Epoch 0: 100% 52/52 [00:57<00:00,  1.10s/it, loss=6.32, v_num=0, train_loss=5.890]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/6 [00:00<?, ?it/s]\u001b[A\n","Epoch 0: 100% 52/52 [01:07<00:00,  1.31s/it, loss=6.04, v_num=0, train_loss=5.670, valid_loss=5.780]\n","Epoch 1: 100% 52/52 [00:59<00:00,  1.15s/it, loss=4.68, v_num=0, train_loss=4.470, valid_loss=5.780]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/6 [00:00<?, ?it/s]\u001b[A\n","Epoch 1: 100% 52/52 [01:11<00:00,  1.37s/it, loss=4.53, v_num=0, train_loss=4.410, valid_loss=4.090]\n","Epoch 2: 100% 52/52 [00:56<00:00,  1.10s/it, loss=3.93, v_num=0, train_loss=3.830, valid_loss=4.090]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/6 [00:00<?, ?it/s]\u001b[A\n","Epoch 2: 100% 52/52 [01:10<00:00,  1.36s/it, loss=3.86, v_num=0, train_loss=3.790, valid_loss=3.530]\n","Epoch 3: 100% 52/52 [00:58<00:00,  1.12s/it, loss=3.55, v_num=0, train_loss=3.520, valid_loss=3.530]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/6 [00:00<?, ?it/s]\u001b[A\n","Epoch 3: 100% 52/52 [01:09<00:00,  1.33s/it, loss=3.51, v_num=0, train_loss=3.500, valid_loss=3.160]\n","Epoch 4: 100% 52/52 [00:58<00:00,  1.13s/it, loss=3.29, v_num=0, train_loss=3.250, valid_loss=3.160]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/6 [00:00<?, ?it/s]\u001b[A\n","Epoch 4: 100% 52/52 [01:09<00:00,  1.33s/it, loss=3.27, v_num=0, train_loss=3.260, valid_loss=2.840]\n","Epoch 5: 100% 52/52 [00:57<00:00,  1.10s/it, loss=3.04, v_num=0, train_loss=2.970, valid_loss=2.840]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/6 [00:00<?, ?it/s]\u001b[A\n","Epoch 5: 100% 52/52 [01:10<00:00,  1.35s/it, loss=3.04, v_num=0, train_loss=3.020, valid_loss=2.660]\n","Epoch 6: 100% 52/52 [00:57<00:00,  1.11s/it, loss=2.99, v_num=0, train_loss=2.950, valid_loss=2.660]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/6 [00:00<?, ?it/s]\u001b[A\n","Epoch 6: 100% 52/52 [01:08<00:00,  1.31s/it, loss=2.99, v_num=0, train_loss=3.020, valid_loss=2.640]\n","Epoch 7: 100% 52/52 [00:57<00:00,  1.10s/it, loss=2.97, v_num=0, train_loss=2.990, valid_loss=2.640]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/6 [00:00<?, ?it/s]\u001b[A\n","Epoch 7: 100% 52/52 [01:10<00:00,  1.36s/it, loss=2.97, v_num=0, train_loss=2.990, valid_loss=2.600]\n","Epoch 8: 100% 52/52 [00:57<00:00,  1.11s/it, loss=2.95, v_num=0, train_loss=2.900, valid_loss=2.600]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/6 [00:00<?, ?it/s]\u001b[A\n","Epoch 8: 100% 52/52 [01:08<00:00,  1.32s/it, loss=2.94, v_num=0, train_loss=2.880, valid_loss=2.580]\n","Epoch 9: 100% 52/52 [00:58<00:00,  1.12s/it, loss=2.91, v_num=0, train_loss=2.900, valid_loss=2.580]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/6 [00:00<?, ?it/s]\u001b[A\n","Epoch 9: 100% 52/52 [01:08<00:00,  1.32s/it, loss=2.92, v_num=0, train_loss=2.890, valid_loss=2.560]\n","Epoch 10: 100% 52/52 [00:57<00:00,  1.11s/it, loss=2.88, v_num=0, train_loss=2.900, valid_loss=2.560]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/6 [00:00<?, ?it/s]\u001b[A\n","Epoch 10: 100% 52/52 [01:08<00:00,  1.32s/it, loss=2.89, v_num=0, train_loss=2.970, valid_loss=2.550]\n","Epoch 11: 100% 52/52 [00:58<00:00,  1.12s/it, loss=2.88, v_num=0, train_loss=2.850, valid_loss=2.550]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/6 [00:00<?, ?it/s]\u001b[A\n","Epoch 11: 100% 52/52 [01:08<00:00,  1.32s/it, loss=2.88, v_num=0, train_loss=2.880, valid_loss=2.550]\n","Epoch 12: 100% 52/52 [00:57<00:00,  1.10s/it, loss=2.88, v_num=0, train_loss=2.860, valid_loss=2.550]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/6 [00:00<?, ?it/s]\u001b[A\n","Epoch 12: 100% 52/52 [01:10<00:00,  1.36s/it, loss=2.89, v_num=0, train_loss=2.920, valid_loss=2.550]\n","Epoch 13: 100% 52/52 [00:58<00:00,  1.12s/it, loss=2.87, v_num=0, train_loss=2.840, valid_loss=2.550]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/6 [00:00<?, ?it/s]\u001b[A\n","Epoch 13: 100% 52/52 [01:08<00:00,  1.32s/it, loss=2.88, v_num=0, train_loss=2.940, valid_loss=2.540]\n","Epoch 14: 100% 52/52 [00:57<00:00,  1.11s/it, loss=2.88, v_num=0, train_loss=2.880, valid_loss=2.540]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/6 [00:00<?, ?it/s]\u001b[A\n","Epoch 14: 100% 52/52 [01:09<00:00,  1.34s/it, loss=2.88, v_num=0, train_loss=2.830, valid_loss=2.540]\n","Epoch 14: 100% 52/52 [01:09<00:00,  1.34s/it, loss=2.88, v_num=0, train_loss=2.830, valid_loss=2.540]\n","End training normal feature distribution from neighbrohood information\n","Start training normal feature distribution from position information\n","Using cache found in /root/.cache/torch/hub/pytorch_vision_v0.10.0\n","Using cache found in /root/.cache/torch/hub/pytorch_vision_v0.10.0\n","End training normal feature distribution from position information\n","Start evaluating anomaly score\n","GPU available: True, used: True\n","TPU available: False, using: 0 TPU cores\n","IPU available: False, using: 0 IPUs\n","Using cache found in /root/.cache/torch/hub/pytorch_vision_v0.10.0\n","LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n","/usr/local/lib/python3.8/dist-packages/pytorch_lightning/trainer/data_loading.py:132: UserWarning: The dataloader, test_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n","  rank_zero_warn(\n","Testing: 100% 14/14 [01:49<00:00,  7.84s/it]--------------------------------------------------------------------------------\n","DATALOADER:0 TEST RESULTS\n","{'pixel_auc_coor': 0.9660812281259598,\n"," 'pixel_auc_nb': 0.965485426691374,\n"," 'pixel_auc_nb_coor': 0.9638251233733077,\n"," 'pixel_auc_patchcore': 0.9700262276457199}\n","--------------------------------------------------------------------------------\n","Testing: 100% 14/14 [01:49<00:00,  7.84s/it]\n","End evaluating anomaly score\n","Global seed set to 23\n","28\n","28\n","['../dataset/UAD/posttreatment/test/anomaly_images/file_brain_AXT1_201_6002708.png', '../dataset/UAD/posttreatment/test/anomaly_images/file_brain_AXT1_202_2020026.png', '../dataset/UAD/posttreatment/test/anomaly_images/file_brain_AXT1_202_2020068.png', '../dataset/UAD/posttreatment/test/anomaly_images/file_brain_AXT1_202_2020089.png', '../dataset/UAD/posttreatment/test/anomaly_images/file_brain_AXT1_202_2020120.png', '../dataset/UAD/posttreatment/test/anomaly_images/file_brain_AXT1_202_2020143.png', '../dataset/UAD/posttreatment/test/anomaly_images/file_brain_AXT1_202_2020156.png', '../dataset/UAD/posttreatment/test/anomaly_images/file_brain_AXT1_202_2020203.png', '../dataset/UAD/posttreatment/test/anomaly_images/file_brain_AXT1_202_2020256.png', '../dataset/UAD/posttreatment/test/anomaly_images/file_brain_AXT1_202_2020313.png', '../dataset/UAD/posttreatment/test/anomaly_images/file_brain_AXT1_202_2020315.png', '../dataset/UAD/posttreatment/test/anomaly_images/file_brain_AXT1_202_2020334.png', '../dataset/UAD/posttreatment/test/anomaly_images/file_brain_AXT1_202_2020389.png', '../dataset/UAD/posttreatment/test/anomaly_images/file_brain_AXT1_202_2020400.png', '../dataset/UAD/posttreatment/test/anomaly_images/file_brain_AXT1_202_2020418.png', '../dataset/UAD/posttreatment/test/anomaly_images/file_brain_AXT1_202_2020482.png', '../dataset/UAD/posttreatment/test/anomaly_images/file_brain_AXT1_202_2020486.png', '../dataset/UAD/posttreatment/test/anomaly_images/file_brain_AXT1_202_2020523.png', '../dataset/UAD/posttreatment/test/anomaly_images/file_brain_AXT1_202_2020532.png', '../dataset/UAD/posttreatment/test/anomaly_images/file_brain_AXT1_202_2020551.png', '../dataset/UAD/posttreatment/test/anomaly_images/file_brain_AXT1_202_2020589.png', '../dataset/UAD/posttreatment/test/anomaly_images/file_brain_AXT1_202_6000324.png', '../dataset/UAD/posttreatment/test/anomaly_images/file_brain_AXT1_202_6000391.png', '../dataset/UAD/posttreatment/test/anomaly_images/file_brain_AXT1_202_6000408.png', '../dataset/UAD/posttreatment/test/anomaly_images/file_brain_AXT1_202_6000448.png', '../dataset/UAD/posttreatment/test/anomaly_images/file_brain_AXT1_202_6000505.png', '../dataset/UAD/posttreatment/test/anomaly_images/file_brain_AXT1_202_6000518.png', '../dataset/UAD/posttreatment/test/anomaly_images/file_brain_AXT1_206_2060085.png']\n","['../dataset/UAD/posttreatment/ground_truth/anomaly_images/file_brain_AXT1_201_6002708_posttreatment_change_0.png', '../dataset/UAD/posttreatment/ground_truth/anomaly_images/file_brain_AXT1_202_2020026_posttreatment_change_1.png', '../dataset/UAD/posttreatment/ground_truth/anomaly_images/file_brain_AXT1_202_2020068_posttreatment_change_0.png', '../dataset/UAD/posttreatment/ground_truth/anomaly_images/file_brain_AXT1_202_2020089_combined.png', '../dataset/UAD/posttreatment/ground_truth/anomaly_images/file_brain_AXT1_202_2020120_posttreatment_change_0.png', '../dataset/UAD/posttreatment/ground_truth/anomaly_images/file_brain_AXT1_202_2020143_combined.png', '../dataset/UAD/posttreatment/ground_truth/anomaly_images/file_brain_AXT1_202_2020156_posttreatment_change_2.png', '../dataset/UAD/posttreatment/ground_truth/anomaly_images/file_brain_AXT1_202_2020203_posttreatment_change_1.png', '../dataset/UAD/posttreatment/ground_truth/anomaly_images/file_brain_AXT1_202_2020256_combined.png', '../dataset/UAD/posttreatment/ground_truth/anomaly_images/file_brain_AXT1_202_2020313_combined.png', '../dataset/UAD/posttreatment/ground_truth/anomaly_images/file_brain_AXT1_202_2020315_posttreatment_change_0.png', '../dataset/UAD/posttreatment/ground_truth/anomaly_images/file_brain_AXT1_202_2020334_combined.png', '../dataset/UAD/posttreatment/ground_truth/anomaly_images/file_brain_AXT1_202_2020389_posttreatment_change_0.png', '../dataset/UAD/posttreatment/ground_truth/anomaly_images/file_brain_AXT1_202_2020400_combined.png', '../dataset/UAD/posttreatment/ground_truth/anomaly_images/file_brain_AXT1_202_2020418_posttreatment_change_0.png', '../dataset/UAD/posttreatment/ground_truth/anomaly_images/file_brain_AXT1_202_2020482_posttreatment_change_0.png', '../dataset/UAD/posttreatment/ground_truth/anomaly_images/file_brain_AXT1_202_2020486_combined.png', '../dataset/UAD/posttreatment/ground_truth/anomaly_images/file_brain_AXT1_202_2020523_combined.png', '../dataset/UAD/posttreatment/ground_truth/anomaly_images/file_brain_AXT1_202_2020532_combined.png', '../dataset/UAD/posttreatment/ground_truth/anomaly_images/file_brain_AXT1_202_2020551_posttreatment_change_0.png', '../dataset/UAD/posttreatment/ground_truth/anomaly_images/file_brain_AXT1_202_2020589_posttreatment_change_1.png', '../dataset/UAD/posttreatment/ground_truth/anomaly_images/file_brain_AXT1_202_6000324_posttreatment_change_0.png', '../dataset/UAD/posttreatment/ground_truth/anomaly_images/file_brain_AXT1_202_6000391_combined.png', '../dataset/UAD/posttreatment/ground_truth/anomaly_images/file_brain_AXT1_202_6000408_posttreatment_change_0.png', '../dataset/UAD/posttreatment/ground_truth/anomaly_images/file_brain_AXT1_202_6000448_posttreatment_change_3.png', '../dataset/UAD/posttreatment/ground_truth/anomaly_images/file_brain_AXT1_202_6000505_combined.png', '../dataset/UAD/posttreatment/ground_truth/anomaly_images/file_brain_AXT1_202_6000518_combined.png', '../dataset/UAD/posttreatment/ground_truth/anomaly_images/file_brain_AXT1_206_2060085_posttreatment_change_2.png']\n","Start generating embeeding coreset and distribution coreset\n","GPU available: True, used: True\n","TPU available: False, using: 0 TPU cores\n","IPU available: False, using: 0 IPUs\n","coreset_generator_trainer完了\n","Using cache found in /root/.cache/torch/hub/pytorch_vision_v0.10.0\n","/usr/local/lib/python3.8/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and will be removed in 0.15, please use 'weights' instead.\n","  warnings.warn(\n","/usr/local/lib/python3.8/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and will be removed in 0.15. The current behavior is equivalent to passing `weights=Wide_ResNet101_2_Weights.IMAGENET1K_V1`. You can also use `weights=Wide_ResNet101_2_Weights.DEFAULT` to get the most up-to-date weights.\n","  warnings.warn(msg)\n","coreset_generator完了\n","LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n","/usr/local/lib/python3.8/dist-packages/pytorch_lightning/trainer/optimizers.py:37: UserWarning: `LightningModule.configure_optimizers` returned `None`, this fit will run with no optimizer\n","  rank_zero_warn(\n","Missing logger folder: ./result/posttreatment/WR101/coreset/lightning_logs\n","\n","  | Name     | Type   | Params\n","------------------------------------\n","0 | backbone | ResNet | 126 M \n","------------------------------------\n","126 M     Trainable params\n","0         Non-trainable params\n","126 M     Total params\n","507.547   Total estimated model params size (MB)\n","/usr/local/lib/python3.8/dist-packages/pytorch_lightning/trainer/data_loading.py:132: UserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n","  rank_zero_warn(\n","Epoch 0:   0% 0/726 [00:00<?, ?it/s] /usr/local/lib/python3.8/dist-packages/pytorch_lightning/loops/optimization/optimizer_loop.py:145: UserWarning: `training_step` returned `None`. If this was on purpose, ignore this warning...\n","  self.warning_cache.warn(\n","Epoch 0:  99% 720/726 [00:35<00:00, 20.08it/s, loss=nan, v_num=0]Size of embedding coreset without edge feature :  (1045, 1024)\n","Size of distribution coreset without edge feature :  (2048, 1024)\n","Epoch 0:  99% 720/726 [00:49<00:00, 14.40it/s, loss=nan, v_num=0]Size of embedding coreset with edge feature :  (1858, 1024)\n","Epoch 0: 100% 726/726 [01:01<00:00, 11.74it/s, loss=nan, v_num=0]\n","End generating embeeding coreset and distribution coreset\n","Using cache found in /root/.cache/torch/hub/pytorch_vision_v0.10.0\n","/usr/local/lib/python3.8/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and will be removed in 0.15, please use 'weights' instead.\n","  warnings.warn(\n","/usr/local/lib/python3.8/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and will be removed in 0.15. The current behavior is equivalent to passing `weights=Wide_ResNet101_2_Weights.IMAGENET1K_V1`. You can also use `weights=Wide_ResNet101_2_Weights.DEFAULT` to get the most up-to-date weights.\n","  warnings.warn(msg)\n","Start training normal feature distribution from neighbrohood information\n","GPU available: True, used: True\n","TPU available: False, using: 0 TPU cores\n","IPU available: False, using: 0 IPUs\n","LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n","Missing logger folder: ./result/posttreatment/WR101/distribution\n","\n","  | Name  | Type               | Params\n","---------------------------------------------\n","0 | model | Distribution_Model | 205 M \n","---------------------------------------------\n","205 M     Trainable params\n","0         Non-trainable params\n","205 M     Total params\n","822.313   Total estimated model params size (MB)\n","Validation sanity check: 0it [00:00, ?it/s]/usr/local/lib/python3.8/dist-packages/pytorch_lightning/trainer/data_loading.py:132: UserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n","  rank_zero_warn(\n","Global seed set to 23\n","/usr/local/lib/python3.8/dist-packages/pytorch_lightning/trainer/data_loading.py:132: UserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n","  rank_zero_warn(\n","Epoch 0: 100% 52/52 [01:07<00:00,  1.29s/it, loss=6.32, v_num=0, train_loss=5.890]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/6 [00:00<?, ?it/s]\u001b[A\n","Epoch 0: 100% 52/52 [01:18<00:00,  1.52s/it, loss=6.04, v_num=0, train_loss=5.670, valid_loss=5.780]\n","Epoch 1: 100% 52/52 [00:56<00:00,  1.09s/it, loss=4.68, v_num=0, train_loss=4.470, valid_loss=5.780]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/6 [00:00<?, ?it/s]\u001b[A\n","Epoch 1: 100% 52/52 [01:06<00:00,  1.29s/it, loss=4.53, v_num=0, train_loss=4.410, valid_loss=4.090]\n","Epoch 2: 100% 52/52 [00:55<00:00,  1.07s/it, loss=3.93, v_num=0, train_loss=3.830, valid_loss=4.090]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 2: 100% 52/52 [01:06<00:00,  1.28s/it, loss=3.93, v_num=0, train_loss=3.830, valid_loss=4.090]\n","Epoch 2: 100% 52/52 [01:08<00:00,  1.31s/it, loss=3.86, v_num=0, train_loss=3.790, valid_loss=3.530]\n","Epoch 3: 100% 52/52 [00:56<00:00,  1.08s/it, loss=3.55, v_num=0, train_loss=3.520, valid_loss=3.530]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/6 [00:00<?, ?it/s]\u001b[A\n","Epoch 3: 100% 52/52 [01:07<00:00,  1.30s/it, loss=3.51, v_num=0, train_loss=3.500, valid_loss=3.160]\n","Epoch 4: 100% 52/52 [00:55<00:00,  1.08s/it, loss=3.29, v_num=0, train_loss=3.250, valid_loss=3.160]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/6 [00:00<?, ?it/s]\u001b[A\n","Epoch 4: 100% 52/52 [01:07<00:00,  1.30s/it, loss=3.27, v_num=0, train_loss=3.260, valid_loss=2.840]\n","Epoch 5: 100% 52/52 [00:55<00:00,  1.08s/it, loss=3.04, v_num=0, train_loss=2.970, valid_loss=2.840]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/6 [00:00<?, ?it/s]\u001b[A\n","Epoch 5: 100% 52/52 [01:07<00:00,  1.30s/it, loss=3.04, v_num=0, train_loss=3.020, valid_loss=2.660]\n","Epoch 6: 100% 52/52 [00:55<00:00,  1.07s/it, loss=2.99, v_num=0, train_loss=2.950, valid_loss=2.660]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 6: 100% 52/52 [01:06<00:00,  1.27s/it, loss=2.99, v_num=0, train_loss=2.950, valid_loss=2.660]\n","Epoch 6: 100% 52/52 [01:07<00:00,  1.30s/it, loss=2.99, v_num=0, train_loss=3.020, valid_loss=2.640]\n","Epoch 7: 100% 52/52 [00:56<00:00,  1.08s/it, loss=2.97, v_num=0, train_loss=2.990, valid_loss=2.640]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/6 [00:00<?, ?it/s]\u001b[A\n","Epoch 7: 100% 52/52 [01:06<00:00,  1.27s/it, loss=2.97, v_num=0, train_loss=2.990, valid_loss=2.600]\n","Epoch 8: 100% 52/52 [00:57<00:00,  1.10s/it, loss=2.95, v_num=0, train_loss=2.900, valid_loss=2.600]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/6 [00:00<?, ?it/s]\u001b[A\n","Epoch 8: 100% 52/52 [01:08<00:00,  1.31s/it, loss=2.94, v_num=0, train_loss=2.880, valid_loss=2.580]\n","Epoch 9: 100% 52/52 [00:56<00:00,  1.08s/it, loss=2.91, v_num=0, train_loss=2.900, valid_loss=2.580]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/6 [00:00<?, ?it/s]\u001b[A\n","Epoch 9: 100% 52/52 [01:07<00:00,  1.30s/it, loss=2.92, v_num=0, train_loss=2.890, valid_loss=2.560]\n","Epoch 10: 100% 52/52 [00:55<00:00,  1.06s/it, loss=2.88, v_num=0, train_loss=2.900, valid_loss=2.560]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/6 [00:00<?, ?it/s]\u001b[A\n","Epoch 10: 100% 52/52 [01:05<00:00,  1.27s/it, loss=2.89, v_num=0, train_loss=2.970, valid_loss=2.550]\n","Epoch 11: 100% 52/52 [00:56<00:00,  1.08s/it, loss=2.88, v_num=0, train_loss=2.850, valid_loss=2.550]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/6 [00:00<?, ?it/s]\u001b[A\n","Epoch 11: 100% 52/52 [01:07<00:00,  1.30s/it, loss=2.88, v_num=0, train_loss=2.880, valid_loss=2.550]\n","Epoch 12: 100% 52/52 [00:56<00:00,  1.08s/it, loss=2.88, v_num=0, train_loss=2.860, valid_loss=2.550]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/6 [00:00<?, ?it/s]\u001b[A\n","Epoch 12: 100% 52/52 [01:06<00:00,  1.28s/it, loss=2.89, v_num=0, train_loss=2.920, valid_loss=2.550]\n","Epoch 13: 100% 52/52 [00:56<00:00,  1.08s/it, loss=2.87, v_num=0, train_loss=2.840, valid_loss=2.550]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/6 [00:00<?, ?it/s]\u001b[A\n","Epoch 13: 100% 52/52 [01:06<00:00,  1.28s/it, loss=2.88, v_num=0, train_loss=2.940, valid_loss=2.540]\n","Epoch 14: 100% 52/52 [00:55<00:00,  1.06s/it, loss=2.88, v_num=0, train_loss=2.880, valid_loss=2.540]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/6 [00:00<?, ?it/s]\u001b[A\n","Epoch 14: 100% 52/52 [01:06<00:00,  1.29s/it, loss=2.88, v_num=0, train_loss=2.830, valid_loss=2.540]\n","Epoch 14: 100% 52/52 [01:06<00:00,  1.29s/it, loss=2.88, v_num=0, train_loss=2.830, valid_loss=2.540]\n","End training normal feature distribution from neighbrohood information\n","Start training normal feature distribution from position information\n","Using cache found in /root/.cache/torch/hub/pytorch_vision_v0.10.0\n","Using cache found in /root/.cache/torch/hub/pytorch_vision_v0.10.0\n","End training normal feature distribution from position information\n","Start evaluating anomaly score\n","GPU available: True, used: True\n","TPU available: False, using: 0 TPU cores\n","IPU available: False, using: 0 IPUs\n","Using cache found in /root/.cache/torch/hub/pytorch_vision_v0.10.0\n","LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n","/usr/local/lib/python3.8/dist-packages/pytorch_lightning/trainer/data_loading.py:132: UserWarning: The dataloader, test_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n","  rank_zero_warn(\n","Testing: 100% 28/28 [02:43<00:00,  5.85s/it]--------------------------------------------------------------------------------\n","DATALOADER:0 TEST RESULTS\n","{'pixel_auc_coor': 0.7924110884678551,\n"," 'pixel_auc_nb': 0.7522575958368326,\n"," 'pixel_auc_nb_coor': 0.7534998410203793,\n"," 'pixel_auc_patchcore': 0.7974615459161386}\n","--------------------------------------------------------------------------------\n","Testing: 100% 28/28 [02:43<00:00,  5.86s/it]\n","End evaluating anomaly score\n","Global seed set to 23\n","8\n","8\n","['../dataset/UAD/resection/test/anomaly_images/file_brain_AXT1_202_2020009.png', '../dataset/UAD/resection/test/anomaly_images/file_brain_AXT1_202_2020068.png', '../dataset/UAD/resection/test/anomaly_images/file_brain_AXT1_202_2020143.png', '../dataset/UAD/resection/test/anomaly_images/file_brain_AXT1_202_2020156.png', '../dataset/UAD/resection/test/anomaly_images/file_brain_AXT1_202_2020291.png', '../dataset/UAD/resection/test/anomaly_images/file_brain_AXT1_202_6000382.png', '../dataset/UAD/resection/test/anomaly_images/file_brain_AXT1_202_6000448.png', '../dataset/UAD/resection/test/anomaly_images/file_brain_AXT1_202_6000518.png']\n","['../dataset/UAD/resection/ground_truth/anomaly_images/file_brain_AXT1_202_2020009_resection_cavity_2.png', '../dataset/UAD/resection/ground_truth/anomaly_images/file_brain_AXT1_202_2020068_resection_cavity_1.png', '../dataset/UAD/resection/ground_truth/anomaly_images/file_brain_AXT1_202_2020143_resection_cavity_3.png', '../dataset/UAD/resection/ground_truth/anomaly_images/file_brain_AXT1_202_2020156_resection_cavity_3.png', '../dataset/UAD/resection/ground_truth/anomaly_images/file_brain_AXT1_202_2020291_resection_cavity_0.png', '../dataset/UAD/resection/ground_truth/anomaly_images/file_brain_AXT1_202_6000382_combined.png', '../dataset/UAD/resection/ground_truth/anomaly_images/file_brain_AXT1_202_6000448_resection_cavity_4.png', '../dataset/UAD/resection/ground_truth/anomaly_images/file_brain_AXT1_202_6000518_combined.png']\n","Start generating embeeding coreset and distribution coreset\n","GPU available: True, used: True\n","TPU available: False, using: 0 TPU cores\n","IPU available: False, using: 0 IPUs\n","coreset_generator_trainer完了\n","Using cache found in /root/.cache/torch/hub/pytorch_vision_v0.10.0\n","/usr/local/lib/python3.8/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and will be removed in 0.15, please use 'weights' instead.\n","  warnings.warn(\n","/usr/local/lib/python3.8/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and will be removed in 0.15. The current behavior is equivalent to passing `weights=Wide_ResNet101_2_Weights.IMAGENET1K_V1`. You can also use `weights=Wide_ResNet101_2_Weights.DEFAULT` to get the most up-to-date weights.\n","  warnings.warn(msg)\n","coreset_generator完了\n","LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n","/usr/local/lib/python3.8/dist-packages/pytorch_lightning/trainer/optimizers.py:37: UserWarning: `LightningModule.configure_optimizers` returned `None`, this fit will run with no optimizer\n","  rank_zero_warn(\n","Missing logger folder: ./result/resection/WR101/coreset/lightning_logs\n","\n","  | Name     | Type   | Params\n","------------------------------------\n","0 | backbone | ResNet | 126 M \n","------------------------------------\n","126 M     Trainable params\n","0         Non-trainable params\n","126 M     Total params\n","507.547   Total estimated model params size (MB)\n","/usr/local/lib/python3.8/dist-packages/pytorch_lightning/trainer/data_loading.py:132: UserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n","  rank_zero_warn(\n","Epoch 0:   0% 0/726 [00:00<?, ?it/s] /usr/local/lib/python3.8/dist-packages/pytorch_lightning/loops/optimization/optimizer_loop.py:145: UserWarning: `training_step` returned `None`. If this was on purpose, ignore this warning...\n","  self.warning_cache.warn(\n","Epoch 0:  99% 720/726 [00:34<00:00, 20.66it/s, loss=nan, v_num=0]Size of embedding coreset without edge feature :  (1045, 1024)\n","Size of distribution coreset without edge feature :  (2048, 1024)\n","Epoch 0:  99% 720/726 [00:50<00:00, 14.40it/s, loss=nan, v_num=0]Size of embedding coreset with edge feature :  (1858, 1024)\n","Epoch 0: 100% 726/726 [01:00<00:00, 11.95it/s, loss=nan, v_num=0]\n","End generating embeeding coreset and distribution coreset\n","Using cache found in /root/.cache/torch/hub/pytorch_vision_v0.10.0\n","/usr/local/lib/python3.8/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and will be removed in 0.15, please use 'weights' instead.\n","  warnings.warn(\n","/usr/local/lib/python3.8/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and will be removed in 0.15. The current behavior is equivalent to passing `weights=Wide_ResNet101_2_Weights.IMAGENET1K_V1`. You can also use `weights=Wide_ResNet101_2_Weights.DEFAULT` to get the most up-to-date weights.\n","  warnings.warn(msg)\n","Start training normal feature distribution from neighbrohood information\n","GPU available: True, used: True\n","TPU available: False, using: 0 TPU cores\n","IPU available: False, using: 0 IPUs\n","LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n","Missing logger folder: ./result/resection/WR101/distribution\n","\n","  | Name  | Type               | Params\n","---------------------------------------------\n","0 | model | Distribution_Model | 205 M \n","---------------------------------------------\n","205 M     Trainable params\n","0         Non-trainable params\n","205 M     Total params\n","822.313   Total estimated model params size (MB)\n","Validation sanity check: 0it [00:00, ?it/s]/usr/local/lib/python3.8/dist-packages/pytorch_lightning/trainer/data_loading.py:132: UserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n","  rank_zero_warn(\n","Global seed set to 23\n","/usr/local/lib/python3.8/dist-packages/pytorch_lightning/trainer/data_loading.py:132: UserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n","  rank_zero_warn(\n","Epoch 0: 100% 52/52 [01:08<00:00,  1.32s/it, loss=6.32, v_num=0, train_loss=5.890]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/6 [00:00<?, ?it/s]\u001b[A\n","Epoch 0: 100% 52/52 [01:23<00:00,  1.60s/it, loss=6.04, v_num=0, train_loss=5.670, valid_loss=5.780]\n","Epoch 1: 100% 52/52 [01:15<00:00,  1.45s/it, loss=4.68, v_num=0, train_loss=4.470, valid_loss=5.780]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/6 [00:00<?, ?it/s]\u001b[A\n","Epoch 1: 100% 52/52 [01:30<00:00,  1.73s/it, loss=4.53, v_num=0, train_loss=4.410, valid_loss=4.090]\n","Epoch 2: 100% 52/52 [01:19<00:00,  1.52s/it, loss=3.93, v_num=0, train_loss=3.830, valid_loss=4.090]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 2: 100% 52/52 [01:33<00:00,  1.80s/it, loss=3.93, v_num=0, train_loss=3.830, valid_loss=4.090]\n","Epoch 2: 100% 52/52 [01:34<00:00,  1.82s/it, loss=3.86, v_num=0, train_loss=3.790, valid_loss=3.530]\n","Epoch 3: 100% 52/52 [01:57<00:00,  2.26s/it, loss=3.55, v_num=0, train_loss=3.520, valid_loss=3.530]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/6 [00:00<?, ?it/s]\u001b[A\n","Epoch 3: 100% 52/52 [02:12<00:00,  2.55s/it, loss=3.51, v_num=0, train_loss=3.500, valid_loss=3.160]\n","Epoch 4: 100% 52/52 [01:15<00:00,  1.46s/it, loss=3.29, v_num=0, train_loss=3.250, valid_loss=3.160]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/6 [00:00<?, ?it/s]\u001b[A\n","Epoch 4: 100% 52/52 [01:30<00:00,  1.74s/it, loss=3.27, v_num=0, train_loss=3.260, valid_loss=2.840]\n","Epoch 5: 100% 52/52 [01:24<00:00,  1.62s/it, loss=3.04, v_num=0, train_loss=2.970, valid_loss=2.840]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 5: 100% 52/52 [01:35<00:00,  1.83s/it, loss=3.04, v_num=0, train_loss=2.970, valid_loss=2.840]\n","Epoch 5: 100% 52/52 [01:40<00:00,  1.93s/it, loss=3.04, v_num=0, train_loss=3.020, valid_loss=2.660]\n","Epoch 6: 100% 52/52 [01:24<00:00,  1.63s/it, loss=2.99, v_num=0, train_loss=2.950, valid_loss=2.660]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 6: 100% 52/52 [01:35<00:00,  1.83s/it, loss=2.99, v_num=0, train_loss=2.950, valid_loss=2.660]\n","Epoch 6: 100% 52/52 [01:39<00:00,  1.91s/it, loss=2.99, v_num=0, train_loss=3.020, valid_loss=2.640]\n","Epoch 7: 100% 52/52 [01:25<00:00,  1.65s/it, loss=2.97, v_num=0, train_loss=2.990, valid_loss=2.640]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 7: 100% 52/52 [01:35<00:00,  1.85s/it, loss=2.97, v_num=0, train_loss=2.990, valid_loss=2.640]\n","Epoch 7: 100% 52/52 [01:37<00:00,  1.87s/it, loss=2.97, v_num=0, train_loss=2.990, valid_loss=2.600]\n","Epoch 8: 100% 52/52 [01:33<00:00,  1.80s/it, loss=2.95, v_num=0, train_loss=2.900, valid_loss=2.600]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/6 [00:00<?, ?it/s]\u001b[A\n","Epoch 8: 100% 52/52 [01:48<00:00,  2.08s/it, loss=2.94, v_num=0, train_loss=2.880, valid_loss=2.580]\n","Epoch 9: 100% 52/52 [01:15<00:00,  1.45s/it, loss=2.91, v_num=0, train_loss=2.900, valid_loss=2.580]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/6 [00:00<?, ?it/s]\u001b[A\n","Epoch 9: 100% 52/52 [01:30<00:00,  1.74s/it, loss=2.92, v_num=0, train_loss=2.890, valid_loss=2.560]\n","Epoch 10: 100% 52/52 [01:19<00:00,  1.53s/it, loss=2.88, v_num=0, train_loss=2.900, valid_loss=2.560]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 10: 100% 52/52 [01:32<00:00,  1.77s/it, loss=2.88, v_num=0, train_loss=2.900, valid_loss=2.560]\n","Epoch 10: 100% 52/52 [01:34<00:00,  1.82s/it, loss=2.89, v_num=0, train_loss=2.970, valid_loss=2.550]\n","Epoch 11: 100% 52/52 [01:13<00:00,  1.42s/it, loss=2.88, v_num=0, train_loss=2.850, valid_loss=2.550]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 11: 100% 52/52 [01:27<00:00,  1.68s/it, loss=2.88, v_num=0, train_loss=2.850, valid_loss=2.550]\n","Epoch 11: 100% 52/52 [01:28<00:00,  1.70s/it, loss=2.88, v_num=0, train_loss=2.880, valid_loss=2.550]\n","Epoch 12: 100% 52/52 [01:16<00:00,  1.47s/it, loss=2.88, v_num=0, train_loss=2.860, valid_loss=2.550]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 12: 100% 52/52 [01:29<00:00,  1.71s/it, loss=2.88, v_num=0, train_loss=2.860, valid_loss=2.550]\n","Epoch 12: 100% 52/52 [01:31<00:00,  1.76s/it, loss=2.89, v_num=0, train_loss=2.920, valid_loss=2.550]\n","Epoch 13: 100% 52/52 [01:08<00:00,  1.32s/it, loss=2.87, v_num=0, train_loss=2.840, valid_loss=2.550]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/6 [00:00<?, ?it/s]\u001b[A\n","Epoch 13: 100% 52/52 [01:23<00:00,  1.61s/it, loss=2.88, v_num=0, train_loss=2.940, valid_loss=2.540]\n","Epoch 14: 100% 52/52 [01:09<00:00,  1.33s/it, loss=2.88, v_num=0, train_loss=2.880, valid_loss=2.540]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/6 [00:00<?, ?it/s]\u001b[A\n","Epoch 14: 100% 52/52 [01:25<00:00,  1.64s/it, loss=2.88, v_num=0, train_loss=2.830, valid_loss=2.540]\n","Epoch 14: 100% 52/52 [01:25<00:00,  1.64s/it, loss=2.88, v_num=0, train_loss=2.830, valid_loss=2.540]\n","End training normal feature distribution from neighbrohood information\n","Start training normal feature distribution from position information\n","Using cache found in /root/.cache/torch/hub/pytorch_vision_v0.10.0\n","Using cache found in /root/.cache/torch/hub/pytorch_vision_v0.10.0\n","End training normal feature distribution from position information\n","Start evaluating anomaly score\n","GPU available: True, used: True\n","TPU available: False, using: 0 TPU cores\n","IPU available: False, using: 0 IPUs\n","Using cache found in /root/.cache/torch/hub/pytorch_vision_v0.10.0\n","LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n","/usr/local/lib/python3.8/dist-packages/pytorch_lightning/trainer/data_loading.py:132: UserWarning: The dataloader, test_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n","  rank_zero_warn(\n","Testing: 100% 8/8 [01:05<00:00,  8.21s/it]--------------------------------------------------------------------------------\n","DATALOADER:0 TEST RESULTS\n","{'pixel_auc_coor': 0.8944607533433953,\n"," 'pixel_auc_nb': 0.8895753880381602,\n"," 'pixel_auc_nb_coor': 0.889150618865503,\n"," 'pixel_auc_patchcore': 0.8959889123607518}\n","--------------------------------------------------------------------------------\n","Testing: 100% 8/8 [01:05<00:00,  8.21s/it]\n","End evaluating anomaly score\n","Global seed set to 23\n","2\n","2\n","['../dataset/UAD/sinus/test/anomaly_images/file_brain_AXT1_202_2020486.png', '../dataset/UAD/sinus/test/anomaly_images/file_brain_AXT1_202_2020517.png']\n","['../dataset/UAD/sinus/ground_truth/anomaly_images/file_brain_AXT1_202_2020486_paranasal_sinus_opacification_2.png', '../dataset/UAD/sinus/ground_truth/anomaly_images/file_brain_AXT1_202_2020517_paranasal_sinus_opacification_1.png']\n","Start generating embeeding coreset and distribution coreset\n","GPU available: True, used: True\n","TPU available: False, using: 0 TPU cores\n","IPU available: False, using: 0 IPUs\n","coreset_generator_trainer完了\n","Using cache found in /root/.cache/torch/hub/pytorch_vision_v0.10.0\n","/usr/local/lib/python3.8/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and will be removed in 0.15, please use 'weights' instead.\n","  warnings.warn(\n","/usr/local/lib/python3.8/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and will be removed in 0.15. The current behavior is equivalent to passing `weights=Wide_ResNet101_2_Weights.IMAGENET1K_V1`. You can also use `weights=Wide_ResNet101_2_Weights.DEFAULT` to get the most up-to-date weights.\n","  warnings.warn(msg)\n","coreset_generator完了\n","LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n","/usr/local/lib/python3.8/dist-packages/pytorch_lightning/trainer/optimizers.py:37: UserWarning: `LightningModule.configure_optimizers` returned `None`, this fit will run with no optimizer\n","  rank_zero_warn(\n","Missing logger folder: ./result/sinus/WR101/coreset/lightning_logs\n","\n","  | Name     | Type   | Params\n","------------------------------------\n","0 | backbone | ResNet | 126 M \n","------------------------------------\n","126 M     Trainable params\n","0         Non-trainable params\n","126 M     Total params\n","507.547   Total estimated model params size (MB)\n","/usr/local/lib/python3.8/dist-packages/pytorch_lightning/trainer/data_loading.py:132: UserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n","  rank_zero_warn(\n","Epoch 0:   0% 0/726 [00:00<?, ?it/s] /usr/local/lib/python3.8/dist-packages/pytorch_lightning/loops/optimization/optimizer_loop.py:145: UserWarning: `training_step` returned `None`. If this was on purpose, ignore this warning...\n","  self.warning_cache.warn(\n","Epoch 0:  99% 720/726 [00:34<00:00, 20.81it/s, loss=nan, v_num=0]Size of embedding coreset without edge feature :  (1045, 1024)\n","Size of distribution coreset without edge feature :  (2048, 1024)\n","Epoch 0:  99% 720/726 [00:49<00:00, 14.40it/s, loss=nan, v_num=0]Size of embedding coreset with edge feature :  (1858, 1024)\n","Epoch 0: 100% 726/726 [01:00<00:00, 12.04it/s, loss=nan, v_num=0]\n","End generating embeeding coreset and distribution coreset\n","Using cache found in /root/.cache/torch/hub/pytorch_vision_v0.10.0\n","/usr/local/lib/python3.8/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and will be removed in 0.15, please use 'weights' instead.\n","  warnings.warn(\n","/usr/local/lib/python3.8/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and will be removed in 0.15. The current behavior is equivalent to passing `weights=Wide_ResNet101_2_Weights.IMAGENET1K_V1`. You can also use `weights=Wide_ResNet101_2_Weights.DEFAULT` to get the most up-to-date weights.\n","  warnings.warn(msg)\n","Start training normal feature distribution from neighbrohood information\n","GPU available: True, used: True\n","TPU available: False, using: 0 TPU cores\n","IPU available: False, using: 0 IPUs\n","LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n","Missing logger folder: ./result/sinus/WR101/distribution\n","\n","  | Name  | Type               | Params\n","---------------------------------------------\n","0 | model | Distribution_Model | 205 M \n","---------------------------------------------\n","205 M     Trainable params\n","0         Non-trainable params\n","205 M     Total params\n","822.313   Total estimated model params size (MB)\n","Validation sanity check: 0it [00:00, ?it/s]/usr/local/lib/python3.8/dist-packages/pytorch_lightning/trainer/data_loading.py:132: UserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n","  rank_zero_warn(\n","Global seed set to 23\n","/usr/local/lib/python3.8/dist-packages/pytorch_lightning/trainer/data_loading.py:132: UserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n","  rank_zero_warn(\n","Epoch 0: 100% 52/52 [00:56<00:00,  1.09s/it, loss=6.32, v_num=0, train_loss=5.890]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/6 [00:00<?, ?it/s]\u001b[A\n","Epoch 0: 100% 52/52 [01:07<00:00,  1.31s/it, loss=6.04, v_num=0, train_loss=5.670, valid_loss=5.780]\n","Epoch 1: 100% 52/52 [00:56<00:00,  1.09s/it, loss=4.68, v_num=0, train_loss=4.470, valid_loss=5.780]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/6 [00:00<?, ?it/s]\u001b[A\n","Epoch 1: 100% 52/52 [01:06<00:00,  1.29s/it, loss=4.53, v_num=0, train_loss=4.410, valid_loss=4.090]\n","Epoch 2: 100% 52/52 [00:58<00:00,  1.12s/it, loss=3.93, v_num=0, train_loss=3.830, valid_loss=4.090]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/6 [00:00<?, ?it/s]\u001b[A\n","Epoch 2: 100% 52/52 [01:08<00:00,  1.32s/it, loss=3.86, v_num=0, train_loss=3.790, valid_loss=3.530]\n","Epoch 3: 100% 52/52 [00:57<00:00,  1.11s/it, loss=3.55, v_num=0, train_loss=3.520, valid_loss=3.530]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/6 [00:00<?, ?it/s]\u001b[A\n","Epoch 3: 100% 52/52 [01:08<00:00,  1.32s/it, loss=3.51, v_num=0, train_loss=3.500, valid_loss=3.160]\n","Epoch 4: 100% 52/52 [00:57<00:00,  1.10s/it, loss=3.29, v_num=0, train_loss=3.250, valid_loss=3.160]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/6 [00:00<?, ?it/s]\u001b[A\n","Epoch 4: 100% 52/52 [01:11<00:00,  1.37s/it, loss=3.27, v_num=0, train_loss=3.260, valid_loss=2.840]\n","Epoch 5: 100% 52/52 [00:57<00:00,  1.11s/it, loss=3.04, v_num=0, train_loss=2.970, valid_loss=2.840]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/6 [00:00<?, ?it/s]\u001b[A\n","Epoch 5: 100% 52/52 [01:08<00:00,  1.32s/it, loss=3.04, v_num=0, train_loss=3.020, valid_loss=2.660]\n","Epoch 6: 100% 52/52 [00:58<00:00,  1.12s/it, loss=2.99, v_num=0, train_loss=2.950, valid_loss=2.660]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/6 [00:00<?, ?it/s]\u001b[A\n","Epoch 6: 100% 52/52 [01:09<00:00,  1.33s/it, loss=2.99, v_num=0, train_loss=3.020, valid_loss=2.640]\n","Epoch 7: 100% 52/52 [00:56<00:00,  1.09s/it, loss=2.97, v_num=0, train_loss=2.990, valid_loss=2.640]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/6 [00:00<?, ?it/s]\u001b[A\n","Epoch 7: 100% 52/52 [01:09<00:00,  1.34s/it, loss=2.97, v_num=0, train_loss=2.990, valid_loss=2.600]\n","Epoch 8: 100% 52/52 [00:57<00:00,  1.11s/it, loss=2.95, v_num=0, train_loss=2.900, valid_loss=2.600]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/6 [00:00<?, ?it/s]\u001b[A\n","Epoch 8: 100% 52/52 [01:08<00:00,  1.32s/it, loss=2.94, v_num=0, train_loss=2.880, valid_loss=2.580]\n","Epoch 9: 100% 52/52 [00:57<00:00,  1.11s/it, loss=2.91, v_num=0, train_loss=2.900, valid_loss=2.580]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/6 [00:00<?, ?it/s]\u001b[A\n","Epoch 9: 100% 52/52 [01:10<00:00,  1.36s/it, loss=2.92, v_num=0, train_loss=2.890, valid_loss=2.560]\n","Epoch 10: 100% 52/52 [00:57<00:00,  1.11s/it, loss=2.88, v_num=0, train_loss=2.900, valid_loss=2.560]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/6 [00:00<?, ?it/s]\u001b[A\n","Epoch 10: 100% 52/52 [01:10<00:00,  1.35s/it, loss=2.89, v_num=0, train_loss=2.970, valid_loss=2.550]\n","Epoch 11: 100% 52/52 [00:57<00:00,  1.11s/it, loss=2.88, v_num=0, train_loss=2.850, valid_loss=2.550]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/6 [00:00<?, ?it/s]\u001b[A\n","Epoch 11: 100% 52/52 [01:09<00:00,  1.34s/it, loss=2.88, v_num=0, train_loss=2.880, valid_loss=2.550]\n","Epoch 12: 100% 52/52 [00:58<00:00,  1.12s/it, loss=2.88, v_num=0, train_loss=2.860, valid_loss=2.550]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/6 [00:00<?, ?it/s]\u001b[A\n","Epoch 12: 100% 52/52 [01:09<00:00,  1.34s/it, loss=2.89, v_num=0, train_loss=2.920, valid_loss=2.550]\n","Epoch 13: 100% 52/52 [00:57<00:00,  1.10s/it, loss=2.87, v_num=0, train_loss=2.840, valid_loss=2.550]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/6 [00:00<?, ?it/s]\u001b[A\n","Epoch 13: 100% 52/52 [01:08<00:00,  1.32s/it, loss=2.88, v_num=0, train_loss=2.940, valid_loss=2.540]\n","Epoch 14: 100% 52/52 [00:58<00:00,  1.12s/it, loss=2.88, v_num=0, train_loss=2.880, valid_loss=2.540]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/6 [00:00<?, ?it/s]\u001b[A\n","Epoch 14: 100% 52/52 [01:09<00:00,  1.34s/it, loss=2.88, v_num=0, train_loss=2.830, valid_loss=2.540]\n","Epoch 14: 100% 52/52 [01:09<00:00,  1.34s/it, loss=2.88, v_num=0, train_loss=2.830, valid_loss=2.540]\n","End training normal feature distribution from neighbrohood information\n","Start training normal feature distribution from position information\n","Using cache found in /root/.cache/torch/hub/pytorch_vision_v0.10.0\n","Using cache found in /root/.cache/torch/hub/pytorch_vision_v0.10.0\n","End training normal feature distribution from position information\n","Start evaluating anomaly score\n","GPU available: True, used: True\n","TPU available: False, using: 0 TPU cores\n","IPU available: False, using: 0 IPUs\n","Using cache found in /root/.cache/torch/hub/pytorch_vision_v0.10.0\n","LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n","/usr/local/lib/python3.8/dist-packages/pytorch_lightning/trainer/data_loading.py:132: UserWarning: The dataloader, test_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n","  rank_zero_warn(\n","Testing: 100% 2/2 [00:20<00:00, 10.18s/it]--------------------------------------------------------------------------------\n","DATALOADER:0 TEST RESULTS\n","{'pixel_auc_coor': 0.7500386763196478,\n"," 'pixel_auc_nb': 0.7414145769020045,\n"," 'pixel_auc_nb_coor': 0.7334518360061223,\n"," 'pixel_auc_patchcore': 0.7608293040592204}\n","--------------------------------------------------------------------------------\n","Testing: 100% 2/2 [00:20<00:00, 10.19s/it]\n","End evaluating anomaly score\n","Global seed set to 23\n","3\n","3\n","['../dataset/UAD/wml/test/anomaly_images/file_brain_AXT1_201_6002688.png', '../dataset/UAD/wml/test/anomaly_images/file_brain_AXT1_202_2020388.png', '../dataset/UAD/wml/test/anomaly_images/file_brain_AXT1_202_6000345.png']\n","['../dataset/UAD/wml/ground_truth/anomaly_images/file_brain_AXT1_201_6002688_nonspecific_white_matter_lesion_0.png', '../dataset/UAD/wml/ground_truth/anomaly_images/file_brain_AXT1_202_2020388_combined.png', '../dataset/UAD/wml/ground_truth/anomaly_images/file_brain_AXT1_202_6000345_combined.png']\n","Start generating embeeding coreset and distribution coreset\n","GPU available: True, used: True\n","TPU available: False, using: 0 TPU cores\n","IPU available: False, using: 0 IPUs\n","coreset_generator_trainer完了\n","Using cache found in /root/.cache/torch/hub/pytorch_vision_v0.10.0\n","/usr/local/lib/python3.8/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and will be removed in 0.15, please use 'weights' instead.\n","  warnings.warn(\n","/usr/local/lib/python3.8/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and will be removed in 0.15. The current behavior is equivalent to passing `weights=Wide_ResNet101_2_Weights.IMAGENET1K_V1`. You can also use `weights=Wide_ResNet101_2_Weights.DEFAULT` to get the most up-to-date weights.\n","  warnings.warn(msg)\n","coreset_generator完了\n","LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n","/usr/local/lib/python3.8/dist-packages/pytorch_lightning/trainer/optimizers.py:37: UserWarning: `LightningModule.configure_optimizers` returned `None`, this fit will run with no optimizer\n","  rank_zero_warn(\n","Missing logger folder: ./result/wml/WR101/coreset/lightning_logs\n","\n","  | Name     | Type   | Params\n","------------------------------------\n","0 | backbone | ResNet | 126 M \n","------------------------------------\n","126 M     Trainable params\n","0         Non-trainable params\n","126 M     Total params\n","507.547   Total estimated model params size (MB)\n","/usr/local/lib/python3.8/dist-packages/pytorch_lightning/trainer/data_loading.py:132: UserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n","  rank_zero_warn(\n","Epoch 0:   0% 0/726 [00:00<?, ?it/s] /usr/local/lib/python3.8/dist-packages/pytorch_lightning/loops/optimization/optimizer_loop.py:145: UserWarning: `training_step` returned `None`. If this was on purpose, ignore this warning...\n","  self.warning_cache.warn(\n","Epoch 0:  99% 720/726 [00:34<00:00, 20.63it/s, loss=nan, v_num=0]Size of embedding coreset without edge feature :  (1045, 1024)\n","Size of distribution coreset without edge feature :  (2048, 1024)\n","Epoch 0:  99% 720/726 [00:49<00:00, 14.40it/s, loss=nan, v_num=0]Size of embedding coreset with edge feature :  (1858, 1024)\n","Epoch 0: 100% 726/726 [01:00<00:00, 11.97it/s, loss=nan, v_num=0]\n","End generating embeeding coreset and distribution coreset\n","Using cache found in /root/.cache/torch/hub/pytorch_vision_v0.10.0\n","/usr/local/lib/python3.8/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and will be removed in 0.15, please use 'weights' instead.\n","  warnings.warn(\n","/usr/local/lib/python3.8/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and will be removed in 0.15. The current behavior is equivalent to passing `weights=Wide_ResNet101_2_Weights.IMAGENET1K_V1`. You can also use `weights=Wide_ResNet101_2_Weights.DEFAULT` to get the most up-to-date weights.\n","  warnings.warn(msg)\n","Start training normal feature distribution from neighbrohood information\n","GPU available: True, used: True\n","TPU available: False, using: 0 TPU cores\n","IPU available: False, using: 0 IPUs\n","LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n","Missing logger folder: ./result/wml/WR101/distribution\n","\n","  | Name  | Type               | Params\n","---------------------------------------------\n","0 | model | Distribution_Model | 205 M \n","---------------------------------------------\n","205 M     Trainable params\n","0         Non-trainable params\n","205 M     Total params\n","822.313   Total estimated model params size (MB)\n","Validation sanity check: 0it [00:00, ?it/s]/usr/local/lib/python3.8/dist-packages/pytorch_lightning/trainer/data_loading.py:132: UserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n","  rank_zero_warn(\n","Global seed set to 23\n","/usr/local/lib/python3.8/dist-packages/pytorch_lightning/trainer/data_loading.py:132: UserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n","  rank_zero_warn(\n","Epoch 0: 100% 52/52 [01:09<00:00,  1.34s/it, loss=6.32, v_num=0, train_loss=5.890]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/6 [00:00<?, ?it/s]\u001b[A\n","Epoch 0: 100% 52/52 [01:21<00:00,  1.57s/it, loss=6.04, v_num=0, train_loss=5.670, valid_loss=5.780]\n","Epoch 1: 100% 52/52 [01:11<00:00,  1.38s/it, loss=4.68, v_num=0, train_loss=4.470, valid_loss=5.780]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 1: 100% 52/52 [01:22<00:00,  1.58s/it, loss=4.68, v_num=0, train_loss=4.470, valid_loss=5.780]\n","Epoch 1: 100% 52/52 [01:24<00:00,  1.62s/it, loss=4.53, v_num=0, train_loss=4.410, valid_loss=4.090]\n","Epoch 2: 100% 52/52 [01:12<00:00,  1.39s/it, loss=3.93, v_num=0, train_loss=3.830, valid_loss=4.090]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/6 [00:00<?, ?it/s]\u001b[A\n","Epoch 2: 100% 52/52 [01:26<00:00,  1.66s/it, loss=3.86, v_num=0, train_loss=3.790, valid_loss=3.530]\n","Epoch 3: 100% 52/52 [01:11<00:00,  1.38s/it, loss=3.55, v_num=0, train_loss=3.520, valid_loss=3.530]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 3: 100% 52/52 [01:22<00:00,  1.58s/it, loss=3.55, v_num=0, train_loss=3.520, valid_loss=3.530]\n","Epoch 3: 100% 52/52 [01:25<00:00,  1.65s/it, loss=3.51, v_num=0, train_loss=3.500, valid_loss=3.160]\n","Epoch 4: 100% 52/52 [01:11<00:00,  1.38s/it, loss=3.29, v_num=0, train_loss=3.250, valid_loss=3.160]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/6 [00:00<?, ?it/s]\u001b[A\n","Epoch 4: 100% 52/52 [01:23<00:00,  1.60s/it, loss=3.27, v_num=0, train_loss=3.260, valid_loss=2.840]\n","Epoch 5: 100% 52/52 [01:11<00:00,  1.37s/it, loss=3.04, v_num=0, train_loss=2.970, valid_loss=2.840]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 5: 100% 52/52 [01:23<00:00,  1.60s/it, loss=3.04, v_num=0, train_loss=2.970, valid_loss=2.840]\n","Epoch 5: 100% 52/52 [01:25<00:00,  1.64s/it, loss=3.04, v_num=0, train_loss=3.020, valid_loss=2.660]\n","Epoch 6: 100% 52/52 [01:10<00:00,  1.36s/it, loss=2.99, v_num=0, train_loss=2.950, valid_loss=2.660]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/6 [00:00<?, ?it/s]\u001b[A\n","Epoch 6: 100% 52/52 [01:24<00:00,  1.63s/it, loss=2.99, v_num=0, train_loss=3.020, valid_loss=2.640]\n","Epoch 7: 100% 52/52 [01:11<00:00,  1.38s/it, loss=2.97, v_num=0, train_loss=2.990, valid_loss=2.640]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 7: 100% 52/52 [01:24<00:00,  1.62s/it, loss=2.97, v_num=0, train_loss=2.990, valid_loss=2.640]\n","Epoch 7: 100% 52/52 [01:26<00:00,  1.66s/it, loss=2.97, v_num=0, train_loss=2.990, valid_loss=2.600]\n","Epoch 8: 100% 52/52 [01:12<00:00,  1.39s/it, loss=2.95, v_num=0, train_loss=2.900, valid_loss=2.600]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/6 [00:00<?, ?it/s]\u001b[A\n","Epoch 8: 100% 52/52 [01:24<00:00,  1.63s/it, loss=2.94, v_num=0, train_loss=2.880, valid_loss=2.580]\n","Epoch 9: 100% 52/52 [01:12<00:00,  1.40s/it, loss=2.91, v_num=0, train_loss=2.900, valid_loss=2.580]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 9: 100% 52/52 [01:23<00:00,  1.60s/it, loss=2.91, v_num=0, train_loss=2.900, valid_loss=2.580]\n","Epoch 9: 100% 52/52 [01:25<00:00,  1.64s/it, loss=2.92, v_num=0, train_loss=2.890, valid_loss=2.560]\n","Epoch 10: 100% 52/52 [01:10<00:00,  1.35s/it, loss=2.88, v_num=0, train_loss=2.900, valid_loss=2.560]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/6 [00:00<?, ?it/s]\u001b[A\n","Epoch 10: 100% 52/52 [01:23<00:00,  1.60s/it, loss=2.89, v_num=0, train_loss=2.970, valid_loss=2.550]\n","Epoch 11: 100% 52/52 [01:12<00:00,  1.40s/it, loss=2.88, v_num=0, train_loss=2.850, valid_loss=2.550]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 11: 100% 52/52 [01:24<00:00,  1.62s/it, loss=2.88, v_num=0, train_loss=2.850, valid_loss=2.550]\n","Epoch 11: 100% 52/52 [01:27<00:00,  1.67s/it, loss=2.88, v_num=0, train_loss=2.880, valid_loss=2.550]\n","Epoch 12: 100% 52/52 [01:09<00:00,  1.34s/it, loss=2.88, v_num=0, train_loss=2.860, valid_loss=2.550]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/6 [00:00<?, ?it/s]\u001b[A\n","Epoch 12: 100% 52/52 [01:22<00:00,  1.58s/it, loss=2.89, v_num=0, train_loss=2.920, valid_loss=2.550]\n","Epoch 13: 100% 52/52 [01:09<00:00,  1.34s/it, loss=2.87, v_num=0, train_loss=2.840, valid_loss=2.550]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/6 [00:00<?, ?it/s]\u001b[A\n","Epoch 13: 100% 52/52 [01:21<00:00,  1.57s/it, loss=2.88, v_num=0, train_loss=2.940, valid_loss=2.540]\n","Epoch 14: 100% 52/52 [01:13<00:00,  1.42s/it, loss=2.88, v_num=0, train_loss=2.880, valid_loss=2.540]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/6 [00:00<?, ?it/s]\u001b[A\n","Epoch 14: 100% 52/52 [01:25<00:00,  1.64s/it, loss=2.88, v_num=0, train_loss=2.830, valid_loss=2.540]\n","Epoch 14: 100% 52/52 [01:25<00:00,  1.64s/it, loss=2.88, v_num=0, train_loss=2.830, valid_loss=2.540]\n","End training normal feature distribution from neighbrohood information\n","Start training normal feature distribution from position information\n","Using cache found in /root/.cache/torch/hub/pytorch_vision_v0.10.0\n","Using cache found in /root/.cache/torch/hub/pytorch_vision_v0.10.0\n","End training normal feature distribution from position information\n","Start evaluating anomaly score\n","GPU available: True, used: True\n","TPU available: False, using: 0 TPU cores\n","IPU available: False, using: 0 IPUs\n","Using cache found in /root/.cache/torch/hub/pytorch_vision_v0.10.0\n","LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n","/usr/local/lib/python3.8/dist-packages/pytorch_lightning/trainer/data_loading.py:132: UserWarning: The dataloader, test_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n","  rank_zero_warn(\n","Testing: 100% 3/3 [00:24<00:00,  8.11s/it]--------------------------------------------------------------------------------\n","DATALOADER:0 TEST RESULTS\n","{'pixel_auc_coor': 0.7817379793113833,\n"," 'pixel_auc_nb': 0.7232591369646911,\n"," 'pixel_auc_nb_coor': 0.7315930775840126,\n"," 'pixel_auc_patchcore': 0.7819138114291425}\n","--------------------------------------------------------------------------------\n","Testing: 100% 3/3 [00:24<00:00,  8.12s/it]\n","End evaluating anomaly score\n","Global seed set to 23\n","5\n","5\n","['../dataset/UAD/other/test/anomaly_images/file_brain_AXT1_202_2020377.png', '../dataset/UAD/other/test/anomaly_images/file_brain_AXT1_202_2020486.png', '../dataset/UAD/other/test/anomaly_images/file_brain_AXT1_202_2020517.png', '../dataset/UAD/other/test/anomaly_images/file_brain_AXT1_202_6000391.png', '../dataset/UAD/other/test/anomaly_images/file_brain_AXT1_202_6000392.png']\n","['../dataset/UAD/other/ground_truth/anomaly_images/file_brain_AXT1_202_2020377_encephalomalacia_0.png', '../dataset/UAD/other/ground_truth/anomaly_images/file_brain_AXT1_202_2020486_paranasal_sinus_opacification_2.png', '../dataset/UAD/other/ground_truth/anomaly_images/file_brain_AXT1_202_2020517_paranasal_sinus_opacification_1.png', '../dataset/UAD/other/ground_truth/anomaly_images/file_brain_AXT1_202_6000391_intraventricular_substance_0.png', '../dataset/UAD/other/ground_truth/anomaly_images/file_brain_AXT1_202_6000392_absent_septum_pellucidum_0.png']\n","Start generating embeeding coreset and distribution coreset\n","GPU available: True, used: True\n","TPU available: False, using: 0 TPU cores\n","IPU available: False, using: 0 IPUs\n","coreset_generator_trainer完了\n","Using cache found in /root/.cache/torch/hub/pytorch_vision_v0.10.0\n","/usr/local/lib/python3.8/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and will be removed in 0.15, please use 'weights' instead.\n","  warnings.warn(\n","/usr/local/lib/python3.8/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and will be removed in 0.15. The current behavior is equivalent to passing `weights=Wide_ResNet101_2_Weights.IMAGENET1K_V1`. You can also use `weights=Wide_ResNet101_2_Weights.DEFAULT` to get the most up-to-date weights.\n","  warnings.warn(msg)\n","coreset_generator完了\n","LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n","/usr/local/lib/python3.8/dist-packages/pytorch_lightning/trainer/optimizers.py:37: UserWarning: `LightningModule.configure_optimizers` returned `None`, this fit will run with no optimizer\n","  rank_zero_warn(\n","Missing logger folder: ./result/other/WR101/coreset/lightning_logs\n","\n","  | Name     | Type   | Params\n","------------------------------------\n","0 | backbone | ResNet | 126 M \n","------------------------------------\n","126 M     Trainable params\n","0         Non-trainable params\n","126 M     Total params\n","507.547   Total estimated model params size (MB)\n","/usr/local/lib/python3.8/dist-packages/pytorch_lightning/trainer/data_loading.py:132: UserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n","  rank_zero_warn(\n","Epoch 0:   0% 0/726 [00:00<?, ?it/s] /usr/local/lib/python3.8/dist-packages/pytorch_lightning/loops/optimization/optimizer_loop.py:145: UserWarning: `training_step` returned `None`. If this was on purpose, ignore this warning...\n","  self.warning_cache.warn(\n","Epoch 0:  99% 720/726 [00:34<00:00, 20.76it/s, loss=nan, v_num=0]Size of embedding coreset without edge feature :  (1045, 1024)\n","Size of distribution coreset without edge feature :  (2048, 1024)\n","Epoch 0:  99% 720/726 [00:49<00:00, 14.40it/s, loss=nan, v_num=0]Size of embedding coreset with edge feature :  (1858, 1024)\n","Epoch 0: 100% 726/726 [01:00<00:00, 12.02it/s, loss=nan, v_num=0]\n","End generating embeeding coreset and distribution coreset\n","Using cache found in /root/.cache/torch/hub/pytorch_vision_v0.10.0\n","/usr/local/lib/python3.8/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and will be removed in 0.15, please use 'weights' instead.\n","  warnings.warn(\n","/usr/local/lib/python3.8/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and will be removed in 0.15. The current behavior is equivalent to passing `weights=Wide_ResNet101_2_Weights.IMAGENET1K_V1`. You can also use `weights=Wide_ResNet101_2_Weights.DEFAULT` to get the most up-to-date weights.\n","  warnings.warn(msg)\n","Start training normal feature distribution from neighbrohood information\n","GPU available: True, used: True\n","TPU available: False, using: 0 TPU cores\n","IPU available: False, using: 0 IPUs\n","LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n","Missing logger folder: ./result/other/WR101/distribution\n","\n","  | Name  | Type               | Params\n","---------------------------------------------\n","0 | model | Distribution_Model | 205 M \n","---------------------------------------------\n","205 M     Trainable params\n","0         Non-trainable params\n","205 M     Total params\n","822.313   Total estimated model params size (MB)\n","Validation sanity check: 0it [00:00, ?it/s]/usr/local/lib/python3.8/dist-packages/pytorch_lightning/trainer/data_loading.py:132: UserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n","  rank_zero_warn(\n","Global seed set to 23\n","/usr/local/lib/python3.8/dist-packages/pytorch_lightning/trainer/data_loading.py:132: UserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n","  rank_zero_warn(\n","Epoch 0: 100% 52/52 [01:02<00:00,  1.21s/it, loss=6.32, v_num=0, train_loss=5.890]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/6 [00:00<?, ?it/s]\u001b[A\n","Epoch 0: 100% 52/52 [01:14<00:00,  1.43s/it, loss=6.04, v_num=0, train_loss=5.670, valid_loss=5.780]\n","Epoch 1: 100% 52/52 [01:04<00:00,  1.25s/it, loss=4.68, v_num=0, train_loss=4.470, valid_loss=5.780]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/6 [00:00<?, ?it/s]\u001b[A\n","Epoch 1: 100% 52/52 [01:15<00:00,  1.46s/it, loss=4.53, v_num=0, train_loss=4.410, valid_loss=4.090]\n","Epoch 2: 100% 52/52 [01:06<00:00,  1.28s/it, loss=3.93, v_num=0, train_loss=3.830, valid_loss=4.090]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/6 [00:00<?, ?it/s]\u001b[A\n","Epoch 2: 100% 52/52 [01:18<00:00,  1.51s/it, loss=3.86, v_num=0, train_loss=3.790, valid_loss=3.530]\n","Epoch 3: 100% 52/52 [01:11<00:00,  1.37s/it, loss=3.55, v_num=0, train_loss=3.520, valid_loss=3.530]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/6 [00:00<?, ?it/s]\u001b[A\n","Epoch 3: 100% 52/52 [01:24<00:00,  1.63s/it, loss=3.51, v_num=0, train_loss=3.500, valid_loss=3.160]\n","Epoch 4: 100% 52/52 [01:09<00:00,  1.34s/it, loss=3.29, v_num=0, train_loss=3.250, valid_loss=3.160]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 4: 100% 52/52 [01:21<00:00,  1.57s/it, loss=3.29, v_num=0, train_loss=3.250, valid_loss=3.160]\n","Epoch 4: 100% 52/52 [01:23<00:00,  1.61s/it, loss=3.27, v_num=0, train_loss=3.260, valid_loss=2.840]\n","Epoch 5: 100% 52/52 [01:04<00:00,  1.24s/it, loss=3.04, v_num=0, train_loss=2.970, valid_loss=2.840]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 5: 100% 52/52 [01:18<00:00,  1.51s/it, loss=3.04, v_num=0, train_loss=2.970, valid_loss=2.840]\n","Epoch 5: 100% 52/52 [01:18<00:00,  1.51s/it, loss=3.04, v_num=0, train_loss=3.020, valid_loss=2.660]\n","Epoch 6: 100% 52/52 [01:02<00:00,  1.21s/it, loss=2.99, v_num=0, train_loss=2.950, valid_loss=2.660]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/6 [00:00<?, ?it/s]\u001b[A\n","Epoch 6: 100% 52/52 [01:13<00:00,  1.42s/it, loss=2.99, v_num=0, train_loss=3.020, valid_loss=2.640]\n","Epoch 7: 100% 52/52 [01:12<00:00,  1.40s/it, loss=2.97, v_num=0, train_loss=2.990, valid_loss=2.640]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 7: 100% 52/52 [01:26<00:00,  1.66s/it, loss=2.97, v_num=0, train_loss=2.990, valid_loss=2.640]\n","Epoch 7: 100% 52/52 [01:26<00:00,  1.66s/it, loss=2.97, v_num=0, train_loss=2.990, valid_loss=2.600]\n","Epoch 8: 100% 52/52 [01:13<00:00,  1.41s/it, loss=2.95, v_num=0, train_loss=2.900, valid_loss=2.600]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/6 [00:00<?, ?it/s]\u001b[A\n","Epoch 8: 100% 52/52 [01:24<00:00,  1.63s/it, loss=2.94, v_num=0, train_loss=2.880, valid_loss=2.580]\n","Epoch 9: 100% 52/52 [01:13<00:00,  1.41s/it, loss=2.91, v_num=0, train_loss=2.900, valid_loss=2.580]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 9: 100% 52/52 [01:24<00:00,  1.63s/it, loss=2.91, v_num=0, train_loss=2.900, valid_loss=2.580]\n","Epoch 9: 100% 52/52 [01:26<00:00,  1.66s/it, loss=2.92, v_num=0, train_loss=2.890, valid_loss=2.560]\n","Epoch 10: 100% 52/52 [01:07<00:00,  1.30s/it, loss=2.88, v_num=0, train_loss=2.900, valid_loss=2.560]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 10: 100% 52/52 [01:18<00:00,  1.51s/it, loss=2.88, v_num=0, train_loss=2.900, valid_loss=2.560]\n","Epoch 10: 100% 52/52 [01:19<00:00,  1.53s/it, loss=2.89, v_num=0, train_loss=2.970, valid_loss=2.550]\n","Epoch 11: 100% 52/52 [01:09<00:00,  1.34s/it, loss=2.88, v_num=0, train_loss=2.850, valid_loss=2.550]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/6 [00:00<?, ?it/s]\u001b[A\n","Epoch 11: 100% 52/52 [01:24<00:00,  1.62s/it, loss=2.88, v_num=0, train_loss=2.880, valid_loss=2.550]\n","Epoch 12: 100% 52/52 [01:09<00:00,  1.34s/it, loss=2.88, v_num=0, train_loss=2.860, valid_loss=2.550]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/6 [00:00<?, ?it/s]\u001b[A\n","Epoch 12: 100% 52/52 [01:20<00:00,  1.55s/it, loss=2.89, v_num=0, train_loss=2.920, valid_loss=2.550]\n","Epoch 13: 100% 52/52 [01:07<00:00,  1.30s/it, loss=2.87, v_num=0, train_loss=2.840, valid_loss=2.550]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/6 [00:00<?, ?it/s]\u001b[A\n","Epoch 13: 100% 52/52 [01:19<00:00,  1.52s/it, loss=2.88, v_num=0, train_loss=2.940, valid_loss=2.540]\n","Epoch 14: 100% 52/52 [01:06<00:00,  1.28s/it, loss=2.88, v_num=0, train_loss=2.880, valid_loss=2.540]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/6 [00:00<?, ?it/s]\u001b[A\n","Epoch 14: 100% 52/52 [01:19<00:00,  1.53s/it, loss=2.88, v_num=0, train_loss=2.830, valid_loss=2.540]\n","Epoch 14: 100% 52/52 [01:19<00:00,  1.53s/it, loss=2.88, v_num=0, train_loss=2.830, valid_loss=2.540]\n","End training normal feature distribution from neighbrohood information\n","Start training normal feature distribution from position information\n","Using cache found in /root/.cache/torch/hub/pytorch_vision_v0.10.0\n","Using cache found in /root/.cache/torch/hub/pytorch_vision_v0.10.0\n","End training normal feature distribution from position information\n","Start evaluating anomaly score\n","GPU available: True, used: True\n","TPU available: False, using: 0 TPU cores\n","IPU available: False, using: 0 IPUs\n","Using cache found in /root/.cache/torch/hub/pytorch_vision_v0.10.0\n","LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n","/usr/local/lib/python3.8/dist-packages/pytorch_lightning/trainer/data_loading.py:132: UserWarning: The dataloader, test_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n","  rank_zero_warn(\n","Testing: 100% 5/5 [00:34<00:00,  6.80s/it]--------------------------------------------------------------------------------\n","DATALOADER:0 TEST RESULTS\n","{'pixel_auc_coor': 0.9034367997670437,\n"," 'pixel_auc_nb': 0.931528395767867,\n"," 'pixel_auc_nb_coor': 0.9231685716965804,\n"," 'pixel_auc_patchcore': 0.9056492014955965}\n","--------------------------------------------------------------------------------\n","Testing: 100% 5/5 [00:34<00:00,  6.80s/it]\n","End evaluating anomaly score\n"]}]},{"cell_type":"code","source":["# make ensemble score for each category and save the result in \"./result/ensemble_result\" repository.\n","#\"--backbone_list\" argument is list of pretrained networks which are to ensemble. You can change category with \"--category\" path.\n","!python3 analysis_code/calc_ensemble_score.py --category absent_septum --backbone_list WR101 --project_root_path ./result --ensemble_root_path ./result/WR101_result\n","!python3 analysis_code/calc_ensemble_score.py --category artefacts --backbone_list WR101 --project_root_path ./result --ensemble_root_path ./result/WR101_result\n","!python3 analysis_code/calc_ensemble_score.py --category craniatomy --backbone_list WR101 --project_root_path ./result --ensemble_root_path ./result/WR101_result\n","!python3 analysis_code/calc_ensemble_score.py --category dural --backbone_list WR101 --project_root_path ./result --ensemble_root_path ./result/WR101_result\n","!python3 analysis_code/calc_ensemble_score.py --category ea_mass --backbone_list WR101 --project_root_path ./result --ensemble_root_path ./result/WR101_result\n","!python3 analysis_code/calc_ensemble_score.py --category edema --backbone_list WR101 --project_root_path ./result --ensemble_root_path ./result/WR101_result\n","!python3 analysis_code/calc_ensemble_score.py --category encephalomalacia --backbone_list WR101 --project_root_path ./result --ensemble_root_path ./result/WR101_result\n","!python3 analysis_code/calc_ensemble_score.py --category enlarged_ventricles --backbone_list WR101 --project_root_path ./result --ensemble_root_path ./result/WR101_result\n","!python3 analysis_code/calc_ensemble_score.py --category intraventricular --backbone_list WR101 --project_root_path ./result --ensemble_root_path ./result/WR101_result\n","!python3 analysis_code/calc_ensemble_score.py --category lesions --backbone_list WR101 --project_root_path ./result --ensemble_root_path ./result/WR101_result\n","!python3 analysis_code/calc_ensemble_score.py --category mass --backbone_list WR101 --project_root_path ./result --ensemble_root_path ./result/WR101_result\n","!python3 analysis_code/calc_ensemble_score.py --category posttreatment --backbone_list WR101 --project_root_path ./result --ensemble_root_path ./result/WR101_result\n","!python3 analysis_code/calc_ensemble_score.py --category resection --backbone_list WR101 --project_root_path ./result --ensemble_root_path ./result/WR101_result\n","!python3 analysis_code/calc_ensemble_score.py --category sinus --backbone_list WR101 --project_root_path ./result --ensemble_root_path ./result/WR101_result\n","!python3 analysis_code/calc_ensemble_score.py --category wml --backbone_list WR101 --project_root_path ./result --ensemble_root_path ./result/WR101_result\n","!python3 analysis_code/calc_ensemble_score.py --category other --backbone_list WR101 --project_root_path ./result --ensemble_root_path ./result/WR101_result\n"],"metadata":{"id":"LMkmeg0bqsB9","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1706432852541,"user_tz":-60,"elapsed":57901,"user":{"displayName":"田代勇希","userId":"12989083184923368445"}},"outputId":"0e1ac1e6-bbda-4932-cd43-f03c9ab16efe"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Start ensemble absent_septum!\n","['/result/absent_septum/WR101/anomaly/lightning_logs/version_0/sample/anomaly_images_file_brain_AXT1_202_6000392']\n","End ensemble absent_septum!\n","Start save ensemble absent_septum!\n","fname: anomaly_images_file_brain_AXT1_202_6000392, fnum: 6000392, ftype: anomaly_images_file_brain_AXT1\n","0, anomaly_images_file_brain_AXT1_202_6000392, ディレクトリ作成完了\n","1\n","Start ensemble artefacts!\n","['/result/artefacts/WR101/anomaly/lightning_logs/version_0/sample/anomaly_images_file_brain_AXT1_201_6002783', '/result/artefacts/WR101/anomaly/lightning_logs/version_0/sample/anomaly_images_file_brain_AXT1_201_6002796', '/result/artefacts/WR101/anomaly/lightning_logs/version_0/sample/anomaly_images_file_brain_AXT1_201_6002811', '/result/artefacts/WR101/anomaly/lightning_logs/version_0/sample/anomaly_images_file_brain_AXT1_201_6002847', '/result/artefacts/WR101/anomaly/lightning_logs/version_0/sample/anomaly_images_file_brain_AXT1_201_6002897', '/result/artefacts/WR101/anomaly/lightning_logs/version_0/sample/anomaly_images_file_brain_AXT1_202_2020086', '/result/artefacts/WR101/anomaly/lightning_logs/version_0/sample/anomaly_images_file_brain_AXT1_202_2020111', '/result/artefacts/WR101/anomaly/lightning_logs/version_0/sample/anomaly_images_file_brain_AXT1_202_2020129', '/result/artefacts/WR101/anomaly/lightning_logs/version_0/sample/anomaly_images_file_brain_AXT1_202_2020156', '/result/artefacts/WR101/anomaly/lightning_logs/version_0/sample/anomaly_images_file_brain_AXT1_202_2020264', '/result/artefacts/WR101/anomaly/lightning_logs/version_0/sample/anomaly_images_file_brain_AXT1_202_2020570', '/result/artefacts/WR101/anomaly/lightning_logs/version_0/sample/anomaly_images_file_brain_AXT1_202_6000416']\n","End ensemble artefacts!\n","Start save ensemble artefacts!\n","fname: anomaly_images_file_brain_AXT1_201_6002783, fnum: 6002783, ftype: anomaly_images_file_brain_AXT1_201\n","0, anomaly_images_file_brain_AXT1_201_6002783, ディレクトリ作成完了\n","12\n","fname: anomaly_images_file_brain_AXT1_201_6002796, fnum: 6002796, ftype: anomaly_images_file_brain_AXT1_201\n","1, anomaly_images_file_brain_AXT1_201_6002796, ディレクトリ作成完了\n","12\n","fname: anomaly_images_file_brain_AXT1_201_6002811, fnum: 6002811, ftype: anomaly_images_file_brain_AXT\n","2, anomaly_images_file_brain_AXT1_201_6002811, ディレクトリ作成完了\n","12\n","fname: anomaly_images_file_brain_AXT1_201_6002847, fnum: 6002847, ftype: anomaly_images_file_brain_AXT1_201\n","3, anomaly_images_file_brain_AXT1_201_6002847, ディレクトリ作成完了\n","12\n","fname: anomaly_images_file_brain_AXT1_201_6002897, fnum: 6002897, ftype: anomaly_images_file_brain_AXT1_201\n","4, anomaly_images_file_brain_AXT1_201_6002897, ディレクトリ作成完了\n","12\n","fname: anomaly_images_file_brain_AXT1_202_2020086, fnum: 2020086, ftype: anomaly_images_file_brain_AXT1\n","5, anomaly_images_file_brain_AXT1_202_2020086, ディレクトリ作成完了\n","12\n","fname: anomaly_images_file_brain_AXT1_202_2020111, fnum: 2020111, ftype: anomaly_images_file_brain_AXT\n","6, anomaly_images_file_brain_AXT1_202_2020111, ディレクトリ作成完了\n","12\n","fname: anomaly_images_file_brain_AXT1_202_2020129, fnum: 2020129, ftype: anomaly_images_file_brain_AXT\n","7, anomaly_images_file_brain_AXT1_202_2020129, ディレクトリ作成完了\n","12\n","fname: anomaly_images_file_brain_AXT1_202_2020156, fnum: 2020156, ftype: anomaly_images_file_brain_AXT\n","8, anomaly_images_file_brain_AXT1_202_2020156, ディレクトリ作成完了\n","12\n","fname: anomaly_images_file_brain_AXT1_202_2020264, fnum: 2020264, ftype: anomaly_images_file_brain_AXT1\n","9, anomaly_images_file_brain_AXT1_202_2020264, ディレクトリ作成完了\n","12\n","fname: anomaly_images_file_brain_AXT1_202_2020570, fnum: 2020570, ftype: anomaly_images_file_brain_AXT1\n","10, anomaly_images_file_brain_AXT1_202_2020570, ディレクトリ作成完了\n","12\n","fname: anomaly_images_file_brain_AXT1_202_6000416, fnum: 6000416, ftype: anomaly_images_file_brain_AXT1_202\n","11, anomaly_images_file_brain_AXT1_202_6000416, ディレクトリ作成完了\n","12\n","Start ensemble craniatomy!\n","['/result/craniatomy/WR101/anomaly/lightning_logs/version_0/sample/anomaly_images_file_brain_AXT1_201_6002796', '/result/craniatomy/WR101/anomaly/lightning_logs/version_0/sample/anomaly_images_file_brain_AXT1_201_6002885', '/result/craniatomy/WR101/anomaly/lightning_logs/version_0/sample/anomaly_images_file_brain_AXT1_202_2020009', '/result/craniatomy/WR101/anomaly/lightning_logs/version_0/sample/anomaly_images_file_brain_AXT1_202_2020026', '/result/craniatomy/WR101/anomaly/lightning_logs/version_0/sample/anomaly_images_file_brain_AXT1_202_2020066', '/result/craniatomy/WR101/anomaly/lightning_logs/version_0/sample/anomaly_images_file_brain_AXT1_202_2020203', '/result/craniatomy/WR101/anomaly/lightning_logs/version_0/sample/anomaly_images_file_brain_AXT1_202_2020334', '/result/craniatomy/WR101/anomaly/lightning_logs/version_0/sample/anomaly_images_file_brain_AXT1_202_2020486', '/result/craniatomy/WR101/anomaly/lightning_logs/version_0/sample/anomaly_images_file_brain_AXT1_202_2020518', '/result/craniatomy/WR101/anomaly/lightning_logs/version_0/sample/anomaly_images_file_brain_AXT1_202_6000382', '/result/craniatomy/WR101/anomaly/lightning_logs/version_0/sample/anomaly_images_file_brain_AXT1_202_6000448', '/result/craniatomy/WR101/anomaly/lightning_logs/version_0/sample/anomaly_images_file_brain_AXT1_202_6000505', '/result/craniatomy/WR101/anomaly/lightning_logs/version_0/sample/anomaly_images_file_brain_AXT1_202_6000518', '/result/craniatomy/WR101/anomaly/lightning_logs/version_0/sample/anomaly_images_file_brain_AXT1_206_2060085']\n","End ensemble craniatomy!\n","Start save ensemble craniatomy!\n","fname: anomaly_images_file_brain_AXT1_201_6002796, fnum: 6002796, ftype: anomaly_images_file_brain_AXT1_201\n","0, anomaly_images_file_brain_AXT1_201_6002796, ディレクトリ作成完了\n","14\n","fname: anomaly_images_file_brain_AXT1_201_6002885, fnum: 6002885, ftype: anomaly_images_file_brain_AXT1_201\n","1, anomaly_images_file_brain_AXT1_201_6002885, ディレクトリ作成完了\n","14\n","fname: anomaly_images_file_brain_AXT1_202_2020009, fnum: 2020009, ftype: anomaly_images_file_brain_AXT1\n","2, anomaly_images_file_brain_AXT1_202_2020009, ディレクトリ作成完了\n","14\n","fname: anomaly_images_file_brain_AXT1_202_2020026, fnum: 2020026, ftype: anomaly_images_file_brain_AXT1\n","3, anomaly_images_file_brain_AXT1_202_2020026, ディレクトリ作成完了\n","14\n","fname: anomaly_images_file_brain_AXT1_202_2020066, fnum: 2020066, ftype: anomaly_images_file_brain_AXT1\n","4, anomaly_images_file_brain_AXT1_202_2020066, ディレクトリ作成完了\n","14\n","fname: anomaly_images_file_brain_AXT1_202_2020203, fnum: 2020203, ftype: anomaly_images_file_brain_AXT1\n","5, anomaly_images_file_brain_AXT1_202_2020203, ディレクトリ作成完了\n","14\n","fname: anomaly_images_file_brain_AXT1_202_2020334, fnum: 2020334, ftype: anomaly_images_file_brain_AXT1\n","6, anomaly_images_file_brain_AXT1_202_2020334, ディレクトリ作成完了\n","14\n","fname: anomaly_images_file_brain_AXT1_202_2020486, fnum: 2020486, ftype: anomaly_images_file_brain_AXT1\n","7, anomaly_images_file_brain_AXT1_202_2020486, ディレクトリ作成完了\n","14\n","fname: anomaly_images_file_brain_AXT1_202_2020518, fnum: 2020518, ftype: anomaly_images_file_brain_AXT\n","8, anomaly_images_file_brain_AXT1_202_2020518, ディレクトリ作成完了\n","14\n","fname: anomaly_images_file_brain_AXT1_202_6000382, fnum: 6000382, ftype: anomaly_images_file_brain_AXT1\n","9, anomaly_images_file_brain_AXT1_202_6000382, ディレクトリ作成完了\n","14\n","fname: anomaly_images_file_brain_AXT1_202_6000448, fnum: 6000448, ftype: anomaly_images_file_brain_AXT1_202\n","10, anomaly_images_file_brain_AXT1_202_6000448, ディレクトリ作成完了\n","14\n","fname: anomaly_images_file_brain_AXT1_202_6000505, fnum: 6000505, ftype: anomaly_images_file_brain_AXT1_202\n","11, anomaly_images_file_brain_AXT1_202_6000505, ディレクトリ作成完了\n","14\n","fname: anomaly_images_file_brain_AXT1_202_6000518, fnum: 6000518, ftype: anomaly_images_file_brain_AXT1_202\n","12, anomaly_images_file_brain_AXT1_202_6000518, ディレクトリ作成完了\n","14\n","fname: anomaly_images_file_brain_AXT1_206_2060085, fnum: 2060085, ftype: anomaly_images_file_brain_AXT1\n","13, anomaly_images_file_brain_AXT1_206_2060085, ディレクトリ作成完了\n","14\n","Start ensemble dural!\n","['/result/dural/WR101/anomaly/lightning_logs/version_0/sample/anomaly_images_file_brain_AXT1_202_2020156', '/result/dural/WR101/anomaly/lightning_logs/version_0/sample/anomaly_images_file_brain_AXT1_202_2020256', '/result/dural/WR101/anomaly/lightning_logs/version_0/sample/anomaly_images_file_brain_AXT1_202_2020264', '/result/dural/WR101/anomaly/lightning_logs/version_0/sample/anomaly_images_file_brain_AXT1_202_2020334', '/result/dural/WR101/anomaly/lightning_logs/version_0/sample/anomaly_images_file_brain_AXT1_206_2060066', '/result/dural/WR101/anomaly/lightning_logs/version_0/sample/anomaly_images_file_brain_AXT1_206_2060085']\n","End ensemble dural!\n","Start save ensemble dural!\n","fname: anomaly_images_file_brain_AXT1_202_2020156, fnum: 2020156, ftype: anomaly_images_file_brain_AXT\n","0, anomaly_images_file_brain_AXT1_202_2020156, ディレクトリ作成完了\n","6\n","fname: anomaly_images_file_brain_AXT1_202_2020256, fnum: 2020256, ftype: anomaly_images_file_brain_AXT1\n","1, anomaly_images_file_brain_AXT1_202_2020256, ディレクトリ作成完了\n","6\n","fname: anomaly_images_file_brain_AXT1_202_2020264, fnum: 2020264, ftype: anomaly_images_file_brain_AXT1\n","2, anomaly_images_file_brain_AXT1_202_2020264, ディレクトリ作成完了\n","6\n","fname: anomaly_images_file_brain_AXT1_202_2020334, fnum: 2020334, ftype: anomaly_images_file_brain_AXT1\n","3, anomaly_images_file_brain_AXT1_202_2020334, ディレクトリ作成完了\n","6\n","fname: anomaly_images_file_brain_AXT1_206_2060066, fnum: 2060066, ftype: anomaly_images_file_brain_AXT1\n","4, anomaly_images_file_brain_AXT1_206_2060066, ディレクトリ作成完了\n","6\n","fname: anomaly_images_file_brain_AXT1_206_2060085, fnum: 2060085, ftype: anomaly_images_file_brain_AXT1\n","5, anomaly_images_file_brain_AXT1_206_2060085, ディレクトリ作成完了\n","6\n","Start ensemble ea_mass!\n","['/result/ea_mass/WR101/anomaly/lightning_logs/version_0/sample/anomaly_images_file_brain_AXT1_201_6002841', '/result/ea_mass/WR101/anomaly/lightning_logs/version_0/sample/anomaly_images_file_brain_AXT1_202_2020129', '/result/ea_mass/WR101/anomaly/lightning_logs/version_0/sample/anomaly_images_file_brain_AXT1_206_2060066']\n","End ensemble ea_mass!\n","Start save ensemble ea_mass!\n","fname: anomaly_images_file_brain_AXT1_201_6002841, fnum: 6002841, ftype: anomaly_images_file_brain_AXT\n","0, anomaly_images_file_brain_AXT1_201_6002841, ディレクトリ作成完了\n","3\n","fname: anomaly_images_file_brain_AXT1_202_2020129, fnum: 2020129, ftype: anomaly_images_file_brain_AXT\n","1, anomaly_images_file_brain_AXT1_202_2020129, ディレクトリ作成完了\n","3\n","fname: anomaly_images_file_brain_AXT1_206_2060066, fnum: 2060066, ftype: anomaly_images_file_brain_AXT1\n","2, anomaly_images_file_brain_AXT1_206_2060066, ディレクトリ作成完了\n","3\n","Start ensemble edema!\n","['/result/edema/WR101/anomaly/lightning_logs/version_0/sample/anomaly_images_file_brain_AXT1_202_2020041', '/result/edema/WR101/anomaly/lightning_logs/version_0/sample/anomaly_images_file_brain_AXT1_202_2020064', '/result/edema/WR101/anomaly/lightning_logs/version_0/sample/anomaly_images_file_brain_AXT1_202_2020103', '/result/edema/WR101/anomaly/lightning_logs/version_0/sample/anomaly_images_file_brain_AXT1_202_2020318', '/result/edema/WR101/anomaly/lightning_logs/version_0/sample/anomaly_images_file_brain_AXT1_202_2020455', '/result/edema/WR101/anomaly/lightning_logs/version_0/sample/anomaly_images_file_brain_AXT1_202_2020486', '/result/edema/WR101/anomaly/lightning_logs/version_0/sample/anomaly_images_file_brain_AXT1_202_2020570', '/result/edema/WR101/anomaly/lightning_logs/version_0/sample/anomaly_images_file_brain_AXT1_202_2020576', '/result/edema/WR101/anomaly/lightning_logs/version_0/sample/anomaly_images_file_brain_AXT1_202_2020581', '/result/edema/WR101/anomaly/lightning_logs/version_0/sample/anomaly_images_file_brain_AXT1_202_2020585', '/result/edema/WR101/anomaly/lightning_logs/version_0/sample/anomaly_images_file_brain_AXT1_202_2120021', '/result/edema/WR101/anomaly/lightning_logs/version_0/sample/anomaly_images_file_brain_AXT1_202_6000264', '/result/edema/WR101/anomaly/lightning_logs/version_0/sample/anomaly_images_file_brain_AXT1_202_6000319', '/result/edema/WR101/anomaly/lightning_logs/version_0/sample/anomaly_images_file_brain_AXT1_202_6000382', '/result/edema/WR101/anomaly/lightning_logs/version_0/sample/anomaly_images_file_brain_AXT1_202_6000465', '/result/edema/WR101/anomaly/lightning_logs/version_0/sample/anomaly_images_file_brain_AXT1_202_6000577', '/result/edema/WR101/anomaly/lightning_logs/version_0/sample/anomaly_images_file_brain_AXT1_206_2060078']\n","End ensemble edema!\n","Start save ensemble edema!\n","fname: anomaly_images_file_brain_AXT1_202_2020041, fnum: 2020041, ftype: anomaly_images_file_brain_AXT\n","0, anomaly_images_file_brain_AXT1_202_2020041, ディレクトリ作成完了\n","17\n","fname: anomaly_images_file_brain_AXT1_202_2020064, fnum: 2020064, ftype: anomaly_images_file_brain_AXT1\n","1, anomaly_images_file_brain_AXT1_202_2020064, ディレクトリ作成完了\n","17\n","fname: anomaly_images_file_brain_AXT1_202_2020103, fnum: 2020103, ftype: anomaly_images_file_brain_AXT\n","2, anomaly_images_file_brain_AXT1_202_2020103, ディレクトリ作成完了\n","17\n","fname: anomaly_images_file_brain_AXT1_202_2020318, fnum: 2020318, ftype: anomaly_images_file_brain_AXT\n","3, anomaly_images_file_brain_AXT1_202_2020318, ディレクトリ作成完了\n","17\n","fname: anomaly_images_file_brain_AXT1_202_2020455, fnum: 2020455, ftype: anomaly_images_file_brain_AXT1\n","4, anomaly_images_file_brain_AXT1_202_2020455, ディレクトリ作成完了\n","17\n","fname: anomaly_images_file_brain_AXT1_202_2020486, fnum: 2020486, ftype: anomaly_images_file_brain_AXT1\n","5, anomaly_images_file_brain_AXT1_202_2020486, ディレクトリ作成完了\n","17\n","fname: anomaly_images_file_brain_AXT1_202_2020570, fnum: 2020570, ftype: anomaly_images_file_brain_AXT1\n","6, anomaly_images_file_brain_AXT1_202_2020570, ディレクトリ作成完了\n","17\n","fname: anomaly_images_file_brain_AXT1_202_2020576, fnum: 2020576, ftype: anomaly_images_file_brain_AXT1\n","7, anomaly_images_file_brain_AXT1_202_2020576, ディレクトリ作成完了\n","17\n","fname: anomaly_images_file_brain_AXT1_202_2020581, fnum: 2020581, ftype: anomaly_images_file_brain_AXT\n","8, anomaly_images_file_brain_AXT1_202_2020581, ディレクトリ作成完了\n","17\n","fname: anomaly_images_file_brain_AXT1_202_2020585, fnum: 2020585, ftype: anomaly_images_file_brain_AXT1\n","9, anomaly_images_file_brain_AXT1_202_2020585, ディレクトリ作成完了\n","17\n","fname: anomaly_images_file_brain_AXT1_202_2120021, fnum: 2120021, ftype: anomaly_images_file_brain_AXT\n","10, anomaly_images_file_brain_AXT1_202_2120021, ディレクトリ作成完了\n","17\n","fname: anomaly_images_file_brain_AXT1_202_6000264, fnum: 6000264, ftype: anomaly_images_file_brain_AXT1\n","11, anomaly_images_file_brain_AXT1_202_6000264, ディレクトリ作成完了\n","17\n","fname: anomaly_images_file_brain_AXT1_202_6000319, fnum: 6000319, ftype: anomaly_images_file_brain_AXT1_202\n","12, anomaly_images_file_brain_AXT1_202_6000319, ディレクトリ作成完了\n","17\n","fname: anomaly_images_file_brain_AXT1_202_6000382, fnum: 6000382, ftype: anomaly_images_file_brain_AXT1\n","13, anomaly_images_file_brain_AXT1_202_6000382, ディレクトリ作成完了\n","17\n","fname: anomaly_images_file_brain_AXT1_202_6000465, fnum: 6000465, ftype: anomaly_images_file_brain_AXT1_202\n","14, anomaly_images_file_brain_AXT1_202_6000465, ディレクトリ作成完了\n","17\n","fname: anomaly_images_file_brain_AXT1_202_6000577, fnum: 6000577, ftype: anomaly_images_file_brain_AXT1_202\n","15, anomaly_images_file_brain_AXT1_202_6000577, ディレクトリ作成完了\n","17\n","fname: anomaly_images_file_brain_AXT1_206_2060078, fnum: 2060078, ftype: anomaly_images_file_brain_AXT1\n","16, anomaly_images_file_brain_AXT1_206_2060078, ディレクトリ作成完了\n","17\n","Start ensemble encephalomalacia!\n","['/result/encephalomalacia/WR101/anomaly/lightning_logs/version_0/sample/anomaly_images_file_brain_AXT1_202_2020377']\n","End ensemble encephalomalacia!\n","Start save ensemble encephalomalacia!\n","fname: anomaly_images_file_brain_AXT1_202_2020377, fnum: 2020377, ftype: anomaly_images_file_brain_AXT1\n","0, anomaly_images_file_brain_AXT1_202_2020377, ディレクトリ作成完了\n","1\n","Start ensemble enlarged_ventricles!\n","['/result/enlarged_ventricles/WR101/anomaly/lightning_logs/version_0/sample/anomaly_images_file_brain_AXT1_201_6002800', '/result/enlarged_ventricles/WR101/anomaly/lightning_logs/version_0/sample/anomaly_images_file_brain_AXT1_201_6002836', '/result/enlarged_ventricles/WR101/anomaly/lightning_logs/version_0/sample/anomaly_images_file_brain_AXT1_201_6002879', '/result/enlarged_ventricles/WR101/anomaly/lightning_logs/version_0/sample/anomaly_images_file_brain_AXT1_202_2020146', '/result/enlarged_ventricles/WR101/anomaly/lightning_logs/version_0/sample/anomaly_images_file_brain_AXT1_202_2020256', '/result/enlarged_ventricles/WR101/anomaly/lightning_logs/version_0/sample/anomaly_images_file_brain_AXT1_202_2020517', '/result/enlarged_ventricles/WR101/anomaly/lightning_logs/version_0/sample/anomaly_images_file_brain_AXT1_202_2020526', '/result/enlarged_ventricles/WR101/anomaly/lightning_logs/version_0/sample/anomaly_images_file_brain_AXT1_202_2020559', '/result/enlarged_ventricles/WR101/anomaly/lightning_logs/version_0/sample/anomaly_images_file_brain_AXT1_202_2020561', '/result/enlarged_ventricles/WR101/anomaly/lightning_logs/version_0/sample/anomaly_images_file_brain_AXT1_202_2020589', '/result/enlarged_ventricles/WR101/anomaly/lightning_logs/version_0/sample/anomaly_images_file_brain_AXT1_202_6000264', '/result/enlarged_ventricles/WR101/anomaly/lightning_logs/version_0/sample/anomaly_images_file_brain_AXT1_202_6000279', '/result/enlarged_ventricles/WR101/anomaly/lightning_logs/version_0/sample/anomaly_images_file_brain_AXT1_202_6000342', '/result/enlarged_ventricles/WR101/anomaly/lightning_logs/version_0/sample/anomaly_images_file_brain_AXT1_202_6000398', '/result/enlarged_ventricles/WR101/anomaly/lightning_logs/version_0/sample/anomaly_images_file_brain_AXT1_202_6000434', '/result/enlarged_ventricles/WR101/anomaly/lightning_logs/version_0/sample/anomaly_images_file_brain_AXT1_206_2060056']\n","End ensemble enlarged_ventricles!\n","Start save ensemble enlarged_ventricles!\n","fname: anomaly_images_file_brain_AXT1_201_6002800, fnum: 6002800, ftype: anomaly_images_file_brain_AXT1_201\n","0, anomaly_images_file_brain_AXT1_201_6002800, ディレクトリ作成完了\n","16\n","fname: anomaly_images_file_brain_AXT1_201_6002836, fnum: 6002836, ftype: anomaly_images_file_brain_AXT1_201\n","1, anomaly_images_file_brain_AXT1_201_6002836, ディレクトリ作成完了\n","16\n","fname: anomaly_images_file_brain_AXT1_201_6002879, fnum: 6002879, ftype: anomaly_images_file_brain_AXT1_201\n","2, anomaly_images_file_brain_AXT1_201_6002879, ディレクトリ作成完了\n","16\n","fname: anomaly_images_file_brain_AXT1_202_2020146, fnum: 2020146, ftype: anomaly_images_file_brain_AXT\n","3, anomaly_images_file_brain_AXT1_202_2020146, ディレクトリ作成完了\n","16\n","fname: anomaly_images_file_brain_AXT1_202_2020256, fnum: 2020256, ftype: anomaly_images_file_brain_AXT1\n","4, anomaly_images_file_brain_AXT1_202_2020256, ディレクトリ作成完了\n","16\n","fname: anomaly_images_file_brain_AXT1_202_2020517, fnum: 2020517, ftype: anomaly_images_file_brain_AXT\n","5, anomaly_images_file_brain_AXT1_202_2020517, ディレクトリ作成完了\n","16\n","fname: anomaly_images_file_brain_AXT1_202_2020526, fnum: 2020526, ftype: anomaly_images_file_brain_AXT1\n","6, anomaly_images_file_brain_AXT1_202_2020526, ディレクトリ作成完了\n","16\n","fname: anomaly_images_file_brain_AXT1_202_2020559, fnum: 2020559, ftype: anomaly_images_file_brain_AXT1\n","7, anomaly_images_file_brain_AXT1_202_2020559, ディレクトリ作成完了\n","16\n","fname: anomaly_images_file_brain_AXT1_202_2020561, fnum: 2020561, ftype: anomaly_images_file_brain_AXT\n","8, anomaly_images_file_brain_AXT1_202_2020561, ディレクトリ作成完了\n","16\n","fname: anomaly_images_file_brain_AXT1_202_2020589, fnum: 2020589, ftype: anomaly_images_file_brain_AXT1\n","9, anomaly_images_file_brain_AXT1_202_2020589, ディレクトリ作成完了\n","16\n","fname: anomaly_images_file_brain_AXT1_202_6000264, fnum: 6000264, ftype: anomaly_images_file_brain_AXT1\n","10, anomaly_images_file_brain_AXT1_202_6000264, ディレクトリ作成完了\n","16\n","fname: anomaly_images_file_brain_AXT1_202_6000279, fnum: 6000279, ftype: anomaly_images_file_brain_AXT1\n","11, anomaly_images_file_brain_AXT1_202_6000279, ディレクトリ作成完了\n","16\n","fname: anomaly_images_file_brain_AXT1_202_6000342, fnum: 6000342, ftype: anomaly_images_file_brain_AXT1\n","12, anomaly_images_file_brain_AXT1_202_6000342, ディレクトリ作成完了\n","16\n","fname: anomaly_images_file_brain_AXT1_202_6000398, fnum: 6000398, ftype: anomaly_images_file_brain_AXT1_202\n","13, anomaly_images_file_brain_AXT1_202_6000398, ディレクトリ作成完了\n","16\n","fname: anomaly_images_file_brain_AXT1_202_6000434, fnum: 6000434, ftype: anomaly_images_file_brain_AXT1_202\n","14, anomaly_images_file_brain_AXT1_202_6000434, ディレクトリ作成完了\n","16\n","fname: anomaly_images_file_brain_AXT1_206_2060056, fnum: 2060056, ftype: anomaly_images_file_brain_AXT1\n","15, anomaly_images_file_brain_AXT1_206_2060056, ディレクトリ作成完了\n","16\n","Start ensemble intraventricular!\n","['/result/intraventricular/WR101/anomaly/lightning_logs/version_0/sample/anomaly_images_file_brain_AXT1_202_6000391']\n","End ensemble intraventricular!\n","Start save ensemble intraventricular!\n","fname: anomaly_images_file_brain_AXT1_202_6000391, fnum: 6000391, ftype: anomaly_images_file_brain_AXT1_202\n","0, anomaly_images_file_brain_AXT1_202_6000391, ディレクトリ作成完了\n","1\n","Start ensemble lesions!\n","['/result/lesions/WR101/anomaly/lightning_logs/version_0/sample/anomaly_images_file_brain_AXT1_201_6002761', '/result/lesions/WR101/anomaly/lightning_logs/version_0/sample/anomaly_images_file_brain_AXT1_201_6002793', '/result/lesions/WR101/anomaly/lightning_logs/version_0/sample/anomaly_images_file_brain_AXT1_201_6002875', '/result/lesions/WR101/anomaly/lightning_logs/version_0/sample/anomaly_images_file_brain_AXT1_201_6002900', '/result/lesions/WR101/anomaly/lightning_logs/version_0/sample/anomaly_images_file_brain_AXT1_202_2020009', '/result/lesions/WR101/anomaly/lightning_logs/version_0/sample/anomaly_images_file_brain_AXT1_202_2020111', '/result/lesions/WR101/anomaly/lightning_logs/version_0/sample/anomaly_images_file_brain_AXT1_202_2020143', '/result/lesions/WR101/anomaly/lightning_logs/version_0/sample/anomaly_images_file_brain_AXT1_202_2020194', '/result/lesions/WR101/anomaly/lightning_logs/version_0/sample/anomaly_images_file_brain_AXT1_202_2020211', '/result/lesions/WR101/anomaly/lightning_logs/version_0/sample/anomaly_images_file_brain_AXT1_202_2020320', '/result/lesions/WR101/anomaly/lightning_logs/version_0/sample/anomaly_images_file_brain_AXT1_202_2020495', '/result/lesions/WR101/anomaly/lightning_logs/version_0/sample/anomaly_images_file_brain_AXT1_202_2020590', '/result/lesions/WR101/anomaly/lightning_logs/version_0/sample/anomaly_images_file_brain_AXT1_202_6000319', '/result/lesions/WR101/anomaly/lightning_logs/version_0/sample/anomaly_images_file_brain_AXT1_202_6000332', '/result/lesions/WR101/anomaly/lightning_logs/version_0/sample/anomaly_images_file_brain_AXT1_202_6000333', '/result/lesions/WR101/anomaly/lightning_logs/version_0/sample/anomaly_images_file_brain_AXT1_202_6000416', '/result/lesions/WR101/anomaly/lightning_logs/version_0/sample/anomaly_images_file_brain_AXT1_202_6000448', '/result/lesions/WR101/anomaly/lightning_logs/version_0/sample/anomaly_images_file_brain_AXT1_206_2060051', '/result/lesions/WR101/anomaly/lightning_logs/version_0/sample/anomaly_images_file_brain_AXT1_206_2060078']\n","End ensemble lesions!\n","Start save ensemble lesions!\n","fname: anomaly_images_file_brain_AXT1_201_6002761, fnum: 6002761, ftype: anomaly_images_file_brain_AXT\n","0, anomaly_images_file_brain_AXT1_201_6002761, ディレクトリ作成完了\n","19\n","fname: anomaly_images_file_brain_AXT1_201_6002793, fnum: 6002793, ftype: anomaly_images_file_brain_AXT1_201\n","1, anomaly_images_file_brain_AXT1_201_6002793, ディレクトリ作成完了\n","19\n","fname: anomaly_images_file_brain_AXT1_201_6002875, fnum: 6002875, ftype: anomaly_images_file_brain_AXT1_201\n","2, anomaly_images_file_brain_AXT1_201_6002875, ディレクトリ作成完了\n","19\n","fname: anomaly_images_file_brain_AXT1_201_6002900, fnum: 6002900, ftype: anomaly_images_file_brain_AXT1_201\n","3, anomaly_images_file_brain_AXT1_201_6002900, ディレクトリ作成完了\n","19\n","fname: anomaly_images_file_brain_AXT1_202_2020009, fnum: 2020009, ftype: anomaly_images_file_brain_AXT1\n","4, anomaly_images_file_brain_AXT1_202_2020009, ディレクトリ作成完了\n","19\n","fname: anomaly_images_file_brain_AXT1_202_2020111, fnum: 2020111, ftype: anomaly_images_file_brain_AXT\n","5, anomaly_images_file_brain_AXT1_202_2020111, ディレクトリ作成完了\n","19\n","fname: anomaly_images_file_brain_AXT1_202_2020143, fnum: 2020143, ftype: anomaly_images_file_brain_AXT\n","6, anomaly_images_file_brain_AXT1_202_2020143, ディレクトリ作成完了\n","19\n","fname: anomaly_images_file_brain_AXT1_202_2020194, fnum: 2020194, ftype: anomaly_images_file_brain_AXT\n","7, anomaly_images_file_brain_AXT1_202_2020194, ディレクトリ作成完了\n","19\n","fname: anomaly_images_file_brain_AXT1_202_2020211, fnum: 2020211, ftype: anomaly_images_file_brain_AXT\n","8, anomaly_images_file_brain_AXT1_202_2020211, ディレクトリ作成完了\n","19\n","fname: anomaly_images_file_brain_AXT1_202_2020320, fnum: 2020320, ftype: anomaly_images_file_brain_AXT1\n","9, anomaly_images_file_brain_AXT1_202_2020320, ディレクトリ作成完了\n","19\n","fname: anomaly_images_file_brain_AXT1_202_2020495, fnum: 2020495, ftype: anomaly_images_file_brain_AXT1\n","10, anomaly_images_file_brain_AXT1_202_2020495, ディレクトリ作成完了\n","19\n","fname: anomaly_images_file_brain_AXT1_202_2020590, fnum: 2020590, ftype: anomaly_images_file_brain_AXT1\n","11, anomaly_images_file_brain_AXT1_202_2020590, ディレクトリ作成完了\n","19\n","fname: anomaly_images_file_brain_AXT1_202_6000319, fnum: 6000319, ftype: anomaly_images_file_brain_AXT1_202\n","12, anomaly_images_file_brain_AXT1_202_6000319, ディレクトリ作成完了\n","19\n","fname: anomaly_images_file_brain_AXT1_202_6000332, fnum: 6000332, ftype: anomaly_images_file_brain_AXT1\n","13, anomaly_images_file_brain_AXT1_202_6000332, ディレクトリ作成完了\n","19\n","fname: anomaly_images_file_brain_AXT1_202_6000333, fnum: 6000333, ftype: anomaly_images_file_brain_AXT1_202\n","14, anomaly_images_file_brain_AXT1_202_6000333, ディレクトリ作成完了\n","19\n","fname: anomaly_images_file_brain_AXT1_202_6000416, fnum: 6000416, ftype: anomaly_images_file_brain_AXT1_202\n","15, anomaly_images_file_brain_AXT1_202_6000416, ディレクトリ作成完了\n","19\n","fname: anomaly_images_file_brain_AXT1_202_6000448, fnum: 6000448, ftype: anomaly_images_file_brain_AXT1_202\n","16, anomaly_images_file_brain_AXT1_202_6000448, ディレクトリ作成完了\n","19\n","fname: anomaly_images_file_brain_AXT1_206_2060051, fnum: 2060051, ftype: anomaly_images_file_brain_AXT\n","17, anomaly_images_file_brain_AXT1_206_2060051, ディレクトリ作成完了\n","19\n","fname: anomaly_images_file_brain_AXT1_206_2060078, fnum: 2060078, ftype: anomaly_images_file_brain_AXT1\n","18, anomaly_images_file_brain_AXT1_206_2060078, ディレクトリ作成完了\n","19\n","Start ensemble mass!\n","['/result/mass/WR101/anomaly/lightning_logs/version_0/sample/anomaly_images_file_brain_AXT1_202_2020041', '/result/mass/WR101/anomaly/lightning_logs/version_0/sample/anomaly_images_file_brain_AXT1_202_2020080', '/result/mass/WR101/anomaly/lightning_logs/version_0/sample/anomaly_images_file_brain_AXT1_202_2020181', '/result/mass/WR101/anomaly/lightning_logs/version_0/sample/anomaly_images_file_brain_AXT1_202_2020283', '/result/mass/WR101/anomaly/lightning_logs/version_0/sample/anomaly_images_file_brain_AXT1_202_2020318', '/result/mass/WR101/anomaly/lightning_logs/version_0/sample/anomaly_images_file_brain_AXT1_202_2020336', '/result/mass/WR101/anomaly/lightning_logs/version_0/sample/anomaly_images_file_brain_AXT1_202_2020530', '/result/mass/WR101/anomaly/lightning_logs/version_0/sample/anomaly_images_file_brain_AXT1_202_2020576', '/result/mass/WR101/anomaly/lightning_logs/version_0/sample/anomaly_images_file_brain_AXT1_202_2020581', '/result/mass/WR101/anomaly/lightning_logs/version_0/sample/anomaly_images_file_brain_AXT1_202_2020584', '/result/mass/WR101/anomaly/lightning_logs/version_0/sample/anomaly_images_file_brain_AXT1_202_2020585', '/result/mass/WR101/anomaly/lightning_logs/version_0/sample/anomaly_images_file_brain_AXT1_202_6000264', '/result/mass/WR101/anomaly/lightning_logs/version_0/sample/anomaly_images_file_brain_AXT1_202_6000465', '/result/mass/WR101/anomaly/lightning_logs/version_0/sample/anomaly_images_file_brain_AXT1_202_6000577']\n","End ensemble mass!\n","Start save ensemble mass!\n","fname: anomaly_images_file_brain_AXT1_202_2020041, fnum: 2020041, ftype: anomaly_images_file_brain_AXT\n","0, anomaly_images_file_brain_AXT1_202_2020041, ディレクトリ作成完了\n","14\n","fname: anomaly_images_file_brain_AXT1_202_2020080, fnum: 2020080, ftype: anomaly_images_file_brain_AXT1\n","1, anomaly_images_file_brain_AXT1_202_2020080, ディレクトリ作成完了\n","14\n","fname: anomaly_images_file_brain_AXT1_202_2020181, fnum: 2020181, ftype: anomaly_images_file_brain_AXT\n","2, anomaly_images_file_brain_AXT1_202_2020181, ディレクトリ作成完了\n","14\n","fname: anomaly_images_file_brain_AXT1_202_2020283, fnum: 2020283, ftype: anomaly_images_file_brain_AXT1\n","3, anomaly_images_file_brain_AXT1_202_2020283, ディレクトリ作成完了\n","14\n","fname: anomaly_images_file_brain_AXT1_202_2020318, fnum: 2020318, ftype: anomaly_images_file_brain_AXT\n","4, anomaly_images_file_brain_AXT1_202_2020318, ディレクトリ作成完了\n","14\n","fname: anomaly_images_file_brain_AXT1_202_2020336, fnum: 2020336, ftype: anomaly_images_file_brain_AXT1\n","5, anomaly_images_file_brain_AXT1_202_2020336, ディレクトリ作成完了\n","14\n","fname: anomaly_images_file_brain_AXT1_202_2020530, fnum: 2020530, ftype: anomaly_images_file_brain_AXT1\n","6, anomaly_images_file_brain_AXT1_202_2020530, ディレクトリ作成完了\n","14\n","fname: anomaly_images_file_brain_AXT1_202_2020576, fnum: 2020576, ftype: anomaly_images_file_brain_AXT1\n","7, anomaly_images_file_brain_AXT1_202_2020576, ディレクトリ作成完了\n","14\n","fname: anomaly_images_file_brain_AXT1_202_2020581, fnum: 2020581, ftype: anomaly_images_file_brain_AXT\n","8, anomaly_images_file_brain_AXT1_202_2020581, ディレクトリ作成完了\n","14\n","fname: anomaly_images_file_brain_AXT1_202_2020584, fnum: 2020584, ftype: anomaly_images_file_brain_AXT1\n","9, anomaly_images_file_brain_AXT1_202_2020584, ディレクトリ作成完了\n","14\n","fname: anomaly_images_file_brain_AXT1_202_2020585, fnum: 2020585, ftype: anomaly_images_file_brain_AXT1\n","10, anomaly_images_file_brain_AXT1_202_2020585, ディレクトリ作成完了\n","14\n","fname: anomaly_images_file_brain_AXT1_202_6000264, fnum: 6000264, ftype: anomaly_images_file_brain_AXT1\n","11, anomaly_images_file_brain_AXT1_202_6000264, ディレクトリ作成完了\n","14\n","fname: anomaly_images_file_brain_AXT1_202_6000465, fnum: 6000465, ftype: anomaly_images_file_brain_AXT1_202\n","12, anomaly_images_file_brain_AXT1_202_6000465, ディレクトリ作成完了\n","14\n","fname: anomaly_images_file_brain_AXT1_202_6000577, fnum: 6000577, ftype: anomaly_images_file_brain_AXT1_202\n","13, anomaly_images_file_brain_AXT1_202_6000577, ディレクトリ作成完了\n","14\n","Start ensemble posttreatment!\n","['/result/posttreatment/WR101/anomaly/lightning_logs/version_0/sample/anomaly_images_file_brain_AXT1_201_6002708', '/result/posttreatment/WR101/anomaly/lightning_logs/version_0/sample/anomaly_images_file_brain_AXT1_202_2020026', '/result/posttreatment/WR101/anomaly/lightning_logs/version_0/sample/anomaly_images_file_brain_AXT1_202_2020068', '/result/posttreatment/WR101/anomaly/lightning_logs/version_0/sample/anomaly_images_file_brain_AXT1_202_2020089', '/result/posttreatment/WR101/anomaly/lightning_logs/version_0/sample/anomaly_images_file_brain_AXT1_202_2020120', '/result/posttreatment/WR101/anomaly/lightning_logs/version_0/sample/anomaly_images_file_brain_AXT1_202_2020143', '/result/posttreatment/WR101/anomaly/lightning_logs/version_0/sample/anomaly_images_file_brain_AXT1_202_2020156', '/result/posttreatment/WR101/anomaly/lightning_logs/version_0/sample/anomaly_images_file_brain_AXT1_202_2020203', '/result/posttreatment/WR101/anomaly/lightning_logs/version_0/sample/anomaly_images_file_brain_AXT1_202_2020256', '/result/posttreatment/WR101/anomaly/lightning_logs/version_0/sample/anomaly_images_file_brain_AXT1_202_2020313', '/result/posttreatment/WR101/anomaly/lightning_logs/version_0/sample/anomaly_images_file_brain_AXT1_202_2020315', '/result/posttreatment/WR101/anomaly/lightning_logs/version_0/sample/anomaly_images_file_brain_AXT1_202_2020334', '/result/posttreatment/WR101/anomaly/lightning_logs/version_0/sample/anomaly_images_file_brain_AXT1_202_2020389', '/result/posttreatment/WR101/anomaly/lightning_logs/version_0/sample/anomaly_images_file_brain_AXT1_202_2020400', '/result/posttreatment/WR101/anomaly/lightning_logs/version_0/sample/anomaly_images_file_brain_AXT1_202_2020418', '/result/posttreatment/WR101/anomaly/lightning_logs/version_0/sample/anomaly_images_file_brain_AXT1_202_2020482', '/result/posttreatment/WR101/anomaly/lightning_logs/version_0/sample/anomaly_images_file_brain_AXT1_202_2020486', '/result/posttreatment/WR101/anomaly/lightning_logs/version_0/sample/anomaly_images_file_brain_AXT1_202_2020523', '/result/posttreatment/WR101/anomaly/lightning_logs/version_0/sample/anomaly_images_file_brain_AXT1_202_2020532', '/result/posttreatment/WR101/anomaly/lightning_logs/version_0/sample/anomaly_images_file_brain_AXT1_202_2020551', '/result/posttreatment/WR101/anomaly/lightning_logs/version_0/sample/anomaly_images_file_brain_AXT1_202_2020589', '/result/posttreatment/WR101/anomaly/lightning_logs/version_0/sample/anomaly_images_file_brain_AXT1_202_6000324', '/result/posttreatment/WR101/anomaly/lightning_logs/version_0/sample/anomaly_images_file_brain_AXT1_202_6000391', '/result/posttreatment/WR101/anomaly/lightning_logs/version_0/sample/anomaly_images_file_brain_AXT1_202_6000408', '/result/posttreatment/WR101/anomaly/lightning_logs/version_0/sample/anomaly_images_file_brain_AXT1_202_6000448', '/result/posttreatment/WR101/anomaly/lightning_logs/version_0/sample/anomaly_images_file_brain_AXT1_202_6000505', '/result/posttreatment/WR101/anomaly/lightning_logs/version_0/sample/anomaly_images_file_brain_AXT1_202_6000518', '/result/posttreatment/WR101/anomaly/lightning_logs/version_0/sample/anomaly_images_file_brain_AXT1_206_2060085']\n","End ensemble posttreatment!\n","Start save ensemble posttreatment!\n","fname: anomaly_images_file_brain_AXT1_201_6002708, fnum: 6002708, ftype: anomaly_images_file_brain_AXT1_201\n","0, anomaly_images_file_brain_AXT1_201_6002708, ディレクトリ作成完了\n","28\n","fname: anomaly_images_file_brain_AXT1_202_2020026, fnum: 2020026, ftype: anomaly_images_file_brain_AXT1\n","1, anomaly_images_file_brain_AXT1_202_2020026, ディレクトリ作成完了\n","28\n","fname: anomaly_images_file_brain_AXT1_202_2020068, fnum: 2020068, ftype: anomaly_images_file_brain_AXT1\n","2, anomaly_images_file_brain_AXT1_202_2020068, ディレクトリ作成完了\n","28\n","fname: anomaly_images_file_brain_AXT1_202_2020089, fnum: 2020089, ftype: anomaly_images_file_brain_AXT1\n","3, anomaly_images_file_brain_AXT1_202_2020089, ディレクトリ作成完了\n","28\n","fname: anomaly_images_file_brain_AXT1_202_2020120, fnum: 2020120, ftype: anomaly_images_file_brain_AXT\n","4, anomaly_images_file_brain_AXT1_202_2020120, ディレクトリ作成完了\n","28\n","fname: anomaly_images_file_brain_AXT1_202_2020143, fnum: 2020143, ftype: anomaly_images_file_brain_AXT\n","5, anomaly_images_file_brain_AXT1_202_2020143, ディレクトリ作成完了\n","28\n","fname: anomaly_images_file_brain_AXT1_202_2020156, fnum: 2020156, ftype: anomaly_images_file_brain_AXT\n","6, anomaly_images_file_brain_AXT1_202_2020156, ディレクトリ作成完了\n","28\n","fname: anomaly_images_file_brain_AXT1_202_2020203, fnum: 2020203, ftype: anomaly_images_file_brain_AXT1\n","7, anomaly_images_file_brain_AXT1_202_2020203, ディレクトリ作成完了\n","28\n","fname: anomaly_images_file_brain_AXT1_202_2020256, fnum: 2020256, ftype: anomaly_images_file_brain_AXT1\n","8, anomaly_images_file_brain_AXT1_202_2020256, ディレクトリ作成完了\n","28\n","fname: anomaly_images_file_brain_AXT1_202_2020313, fnum: 2020313, ftype: anomaly_images_file_brain_AXT\n","9, anomaly_images_file_brain_AXT1_202_2020313, ディレクトリ作成完了\n","28\n","fname: anomaly_images_file_brain_AXT1_202_2020315, fnum: 2020315, ftype: anomaly_images_file_brain_AXT\n","10, anomaly_images_file_brain_AXT1_202_2020315, ディレクトリ作成完了\n","28\n","fname: anomaly_images_file_brain_AXT1_202_2020334, fnum: 2020334, ftype: anomaly_images_file_brain_AXT1\n","11, anomaly_images_file_brain_AXT1_202_2020334, ディレクトリ作成完了\n","28\n","fname: anomaly_images_file_brain_AXT1_202_2020389, fnum: 2020389, ftype: anomaly_images_file_brain_AXT1\n","12, anomaly_images_file_brain_AXT1_202_2020389, ディレクトリ作成完了\n","28\n","fname: anomaly_images_file_brain_AXT1_202_2020400, fnum: 2020400, ftype: anomaly_images_file_brain_AXT1\n","13, anomaly_images_file_brain_AXT1_202_2020400, ディレクトリ作成完了\n","28\n","fname: anomaly_images_file_brain_AXT1_202_2020418, fnum: 2020418, ftype: anomaly_images_file_brain_AXT\n","14, anomaly_images_file_brain_AXT1_202_2020418, ディレクトリ作成完了\n","28\n","fname: anomaly_images_file_brain_AXT1_202_2020482, fnum: 2020482, ftype: anomaly_images_file_brain_AXT1\n","15, anomaly_images_file_brain_AXT1_202_2020482, ディレクトリ作成完了\n","28\n","fname: anomaly_images_file_brain_AXT1_202_2020486, fnum: 2020486, ftype: anomaly_images_file_brain_AXT1\n","16, anomaly_images_file_brain_AXT1_202_2020486, ディレクトリ作成完了\n","28\n","fname: anomaly_images_file_brain_AXT1_202_2020523, fnum: 2020523, ftype: anomaly_images_file_brain_AXT1\n","17, anomaly_images_file_brain_AXT1_202_2020523, ディレクトリ作成完了\n","28\n","fname: anomaly_images_file_brain_AXT1_202_2020532, fnum: 2020532, ftype: anomaly_images_file_brain_AXT1\n","18, anomaly_images_file_brain_AXT1_202_2020532, ディレクトリ作成完了\n","28\n","fname: anomaly_images_file_brain_AXT1_202_2020551, fnum: 2020551, ftype: anomaly_images_file_brain_AXT\n","19, anomaly_images_file_brain_AXT1_202_2020551, ディレクトリ作成完了\n","28\n","fname: anomaly_images_file_brain_AXT1_202_2020589, fnum: 2020589, ftype: anomaly_images_file_brain_AXT1\n","20, anomaly_images_file_brain_AXT1_202_2020589, ディレクトリ作成完了\n","28\n","fname: anomaly_images_file_brain_AXT1_202_6000324, fnum: 6000324, ftype: anomaly_images_file_brain_AXT1\n","21, anomaly_images_file_brain_AXT1_202_6000324, ディレクトリ作成完了\n","28\n","fname: anomaly_images_file_brain_AXT1_202_6000391, fnum: 6000391, ftype: anomaly_images_file_brain_AXT1_202\n","22, anomaly_images_file_brain_AXT1_202_6000391, ディレクトリ作成完了\n","28\n","fname: anomaly_images_file_brain_AXT1_202_6000408, fnum: 6000408, ftype: anomaly_images_file_brain_AXT1_202\n","23, anomaly_images_file_brain_AXT1_202_6000408, ディレクトリ作成完了\n","28\n","fname: anomaly_images_file_brain_AXT1_202_6000448, fnum: 6000448, ftype: anomaly_images_file_brain_AXT1_202\n","24, anomaly_images_file_brain_AXT1_202_6000448, ディレクトリ作成完了\n","28\n","fname: anomaly_images_file_brain_AXT1_202_6000505, fnum: 6000505, ftype: anomaly_images_file_brain_AXT1_202\n","25, anomaly_images_file_brain_AXT1_202_6000505, ディレクトリ作成完了\n","28\n","fname: anomaly_images_file_brain_AXT1_202_6000518, fnum: 6000518, ftype: anomaly_images_file_brain_AXT1_202\n","26, anomaly_images_file_brain_AXT1_202_6000518, ディレクトリ作成完了\n","28\n","fname: anomaly_images_file_brain_AXT1_206_2060085, fnum: 2060085, ftype: anomaly_images_file_brain_AXT1\n","27, anomaly_images_file_brain_AXT1_206_2060085, ディレクトリ作成完了\n","28\n","Start ensemble resection!\n","['/result/resection/WR101/anomaly/lightning_logs/version_0/sample/anomaly_images_file_brain_AXT1_202_2020009', '/result/resection/WR101/anomaly/lightning_logs/version_0/sample/anomaly_images_file_brain_AXT1_202_2020068', '/result/resection/WR101/anomaly/lightning_logs/version_0/sample/anomaly_images_file_brain_AXT1_202_2020143', '/result/resection/WR101/anomaly/lightning_logs/version_0/sample/anomaly_images_file_brain_AXT1_202_2020156', '/result/resection/WR101/anomaly/lightning_logs/version_0/sample/anomaly_images_file_brain_AXT1_202_2020291', '/result/resection/WR101/anomaly/lightning_logs/version_0/sample/anomaly_images_file_brain_AXT1_202_6000382', '/result/resection/WR101/anomaly/lightning_logs/version_0/sample/anomaly_images_file_brain_AXT1_202_6000448', '/result/resection/WR101/anomaly/lightning_logs/version_0/sample/anomaly_images_file_brain_AXT1_202_6000518']\n","End ensemble resection!\n","Start save ensemble resection!\n","fname: anomaly_images_file_brain_AXT1_202_2020009, fnum: 2020009, ftype: anomaly_images_file_brain_AXT1\n","0, anomaly_images_file_brain_AXT1_202_2020009, ディレクトリ作成完了\n","8\n","fname: anomaly_images_file_brain_AXT1_202_2020068, fnum: 2020068, ftype: anomaly_images_file_brain_AXT1\n","1, anomaly_images_file_brain_AXT1_202_2020068, ディレクトリ作成完了\n","8\n","fname: anomaly_images_file_brain_AXT1_202_2020143, fnum: 2020143, ftype: anomaly_images_file_brain_AXT\n","2, anomaly_images_file_brain_AXT1_202_2020143, ディレクトリ作成完了\n","8\n","fname: anomaly_images_file_brain_AXT1_202_2020156, fnum: 2020156, ftype: anomaly_images_file_brain_AXT\n","3, anomaly_images_file_brain_AXT1_202_2020156, ディレクトリ作成完了\n","8\n","fname: anomaly_images_file_brain_AXT1_202_2020291, fnum: 2020291, ftype: anomaly_images_file_brain_AXT\n","4, anomaly_images_file_brain_AXT1_202_2020291, ディレクトリ作成完了\n","8\n","fname: anomaly_images_file_brain_AXT1_202_6000382, fnum: 6000382, ftype: anomaly_images_file_brain_AXT1\n","5, anomaly_images_file_brain_AXT1_202_6000382, ディレクトリ作成完了\n","8\n","fname: anomaly_images_file_brain_AXT1_202_6000448, fnum: 6000448, ftype: anomaly_images_file_brain_AXT1_202\n","6, anomaly_images_file_brain_AXT1_202_6000448, ディレクトリ作成完了\n","8\n","fname: anomaly_images_file_brain_AXT1_202_6000518, fnum: 6000518, ftype: anomaly_images_file_brain_AXT1_202\n","7, anomaly_images_file_brain_AXT1_202_6000518, ディレクトリ作成完了\n","8\n","Start ensemble sinus!\n","['/result/sinus/WR101/anomaly/lightning_logs/version_0/sample/anomaly_images_file_brain_AXT1_202_2020486', '/result/sinus/WR101/anomaly/lightning_logs/version_0/sample/anomaly_images_file_brain_AXT1_202_2020517']\n","End ensemble sinus!\n","Start save ensemble sinus!\n","fname: anomaly_images_file_brain_AXT1_202_2020486, fnum: 2020486, ftype: anomaly_images_file_brain_AXT1\n","0, anomaly_images_file_brain_AXT1_202_2020486, ディレクトリ作成完了\n","2\n","fname: anomaly_images_file_brain_AXT1_202_2020517, fnum: 2020517, ftype: anomaly_images_file_brain_AXT\n","1, anomaly_images_file_brain_AXT1_202_2020517, ディレクトリ作成完了\n","2\n","Start ensemble wml!\n","['/result/wml/WR101/anomaly/lightning_logs/version_0/sample/anomaly_images_file_brain_AXT1_201_6002688', '/result/wml/WR101/anomaly/lightning_logs/version_0/sample/anomaly_images_file_brain_AXT1_202_2020388', '/result/wml/WR101/anomaly/lightning_logs/version_0/sample/anomaly_images_file_brain_AXT1_202_6000345']\n","End ensemble wml!\n","Start save ensemble wml!\n","fname: anomaly_images_file_brain_AXT1_201_6002688, fnum: 6002688, ftype: anomaly_images_file_brain_AXT1_201\n","0, anomaly_images_file_brain_AXT1_201_6002688, ディレクトリ作成完了\n","3\n","fname: anomaly_images_file_brain_AXT1_202_2020388, fnum: 2020388, ftype: anomaly_images_file_brain_AXT1\n","1, anomaly_images_file_brain_AXT1_202_2020388, ディレクトリ作成完了\n","3\n","fname: anomaly_images_file_brain_AXT1_202_6000345, fnum: 6000345, ftype: anomaly_images_file_brain_AXT1_202\n","2, anomaly_images_file_brain_AXT1_202_6000345, ディレクトリ作成完了\n","3\n","Start ensemble other!\n","['/result/other/WR101/anomaly/lightning_logs/version_0/sample/anomaly_images_file_brain_AXT1_202_2020377', '/result/other/WR101/anomaly/lightning_logs/version_0/sample/anomaly_images_file_brain_AXT1_202_2020486', '/result/other/WR101/anomaly/lightning_logs/version_0/sample/anomaly_images_file_brain_AXT1_202_2020517', '/result/other/WR101/anomaly/lightning_logs/version_0/sample/anomaly_images_file_brain_AXT1_202_6000391', '/result/other/WR101/anomaly/lightning_logs/version_0/sample/anomaly_images_file_brain_AXT1_202_6000392']\n","End ensemble other!\n","Start save ensemble other!\n","fname: anomaly_images_file_brain_AXT1_202_2020377, fnum: 2020377, ftype: anomaly_images_file_brain_AXT1\n","0, anomaly_images_file_brain_AXT1_202_2020377, ディレクトリ作成完了\n","5\n","fname: anomaly_images_file_brain_AXT1_202_2020486, fnum: 2020486, ftype: anomaly_images_file_brain_AXT1\n","1, anomaly_images_file_brain_AXT1_202_2020486, ディレクトリ作成完了\n","5\n","fname: anomaly_images_file_brain_AXT1_202_2020517, fnum: 2020517, ftype: anomaly_images_file_brain_AXT\n","2, anomaly_images_file_brain_AXT1_202_2020517, ディレクトリ作成完了\n","5\n","fname: anomaly_images_file_brain_AXT1_202_6000391, fnum: 6000391, ftype: anomaly_images_file_brain_AXT1_202\n","3, anomaly_images_file_brain_AXT1_202_6000391, ディレクトリ作成完了\n","5\n","fname: anomaly_images_file_brain_AXT1_202_6000392, fnum: 6000392, ftype: anomaly_images_file_brain_AXT1\n","4, anomaly_images_file_brain_AXT1_202_6000392, ディレクトリ作成完了\n","5\n"]}]},{"cell_type":"code","source":["# convert result format and save it into \"./result/ensemble_ravel\" repository.\n","# Add argument \"--is_BTAD\" if dataset is BTAD, and \"--is_MVtec_small\" if dataset is small version of MVTec which we provided.\n","# Default dataste is MVTec AD benchmark.\n","!python3 analysis_code/convert_result_format.py --before_result_root_dir ./result/WR101_result --after_result_root_dir ./result/WR101_ravel\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"GeExjm6qyhSO","executionInfo":{"status":"ok","timestamp":1706432870458,"user_tz":-60,"elapsed":17934,"user":{"displayName":"田代勇希","userId":"12989083184923368445"}},"outputId":"34e1e290-40a6-4d20-be84-08b43c7e3e75"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["100% 150/150 [00:17<00:00,  8.61it/s]\n"]}]},{"cell_type":"code","source":["# analysis anomaly map from \"./result/ensemble_ravel\" repository.\n","# Add argument \"--visualize\" to visualize anomaly map on \"./result/ensemble_ravel/viz\" repository.\n","# If you want to find misclassified images with trained model, add argument \"--calc_misclassified_sample\" and indices of false positive samples and false negative samples will be presented on \"./result/ensemble_ravel/misclassified_sample_list.csv\"\n","# In addition, add \"--calc_pro\" argument to additionally calculate AUPRO score. The result will presented on \"./result/ensemble_ravel/score_result.csv\".\n","!python3 analysis_code/analysis_amap.py --project_root_path ./result_05_FirstTrueTry/WR101_ravel --calc_pro # --visualize"],"metadata":{"id":"lsxTUa3RO1GE","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1706581391154,"user_tz":-60,"elapsed":6872,"user":{"displayName":"田代勇希","userId":"12989083184923368445"}},"outputId":"b9f444d6-114d-4aaa-be71-2518d97a73f4"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Start calculating anomaly score!\n","(1, 1, 128, 128)\n","(1, 16384)\n","[[[[0.]]]]\n","[[[[1.]]]]\n","Category : absent_septum, y_true: (16384,), y_score: (16384,), category_pxl_auc: 0.9841882606949768\n","Category : absent_septum\n","Best threshold:  0.8608987563897155\n","Best F1-Score:  0.44159503049120663\n","Best Img threshold:  1.0\n","Pixel AUROC:   0.984188\n","Pixel AUPRO:   0.271382\n","\n","(1, 12, 128, 128)\n","(1, 196608)\n","[[[[0.]]]]\n","[[[[1.]]]]\n","Category : artefacts, y_true: (196608,), y_score: (196608,), category_pxl_auc: 0.8710149543287717\n","Category : artefacts\n","Best threshold:  0.8102693217364767\n","Best F1-Score:  0.30442302256504106\n","Best Img threshold:  0.4555123216601816\n","Pixel AUROC:   0.871015\n","Pixel AUPRO:   0.294636\n","\n","(1, 14, 128, 128)\n","(1, 229376)\n","[[[[0.]]]]\n","[[[[1.]]]]\n","Category : craniatomy, y_true: (229376,), y_score: (229376,), category_pxl_auc: 0.8336169802328174\n","Category : craniatomy\n","Best threshold:  0.7481498435950256\n","Best F1-Score:  0.24720832107334811\n","Best Img threshold:  0.5057450217441062\n","Pixel AUROC:   0.833617\n","Pixel AUPRO:   0.141897\n","\n","(1, 6, 128, 128)\n","(1, 98304)\n","[[[[0.]]]]\n","[[[[1.]]]]\n","Category : dural, y_true: (98304,), y_score: (98304,), category_pxl_auc: 0.6750077372295606\n","Category : dural\n","Best threshold:  0.5528496223392081\n","Best F1-Score:  0.3552464210024518\n","Best Img threshold:  0.6917067215991455\n","Pixel AUROC:   0.675008\n","Pixel AUPRO:   0.187924\n","\n","(1, 3, 128, 128)\n","(1, 49152)\n","[[[[0.]]]]\n","[[[[1.]]]]\n","Category : ea_mass, y_true: (49152,), y_score: (49152,), category_pxl_auc: 0.8494879704166742\n","Category : ea_mass\n","Best threshold:  0.7504081788357366\n","Best F1-Score:  0.12235993576407292\n","Best Img threshold:  0.53847562371252\n","Pixel AUROC:   0.849488\n","Pixel AUPRO:   0.040748\n","\n","(1, 17, 128, 128)\n","(1, 278528)\n","[[[[0.]]]]\n","[[[[1.]]]]\n","Category : edema, y_true: (278528,), y_score: (278528,), category_pxl_auc: 0.879855020955834\n","Category : edema\n","Best threshold:  0.5322499427786679\n","Best F1-Score:  0.38566442082909175\n","Best Img threshold:  0.4619668879224842\n","Pixel AUROC:   0.879855\n","Pixel AUPRO:   0.300995\n","\n","(1, 1, 128, 128)\n","(1, 16384)\n","[[[[0.]]]]\n","[[[[1.]]]]\n","Category : encephalomalacia, y_true: (16384,), y_score: (16384,), category_pxl_auc: 0.9787444399521874\n","Category : encephalomalacia\n","Best threshold:  0.8525978484779125\n","Best F1-Score:  0.6507319666632552\n","Best Img threshold:  1.0\n","Pixel AUROC:   0.978744\n","Pixel AUPRO:   0.739462\n","\n","(1, 16, 128, 128)\n","(1, 262144)\n","[[[[0.]]]]\n","[[[[1.]]]]\n","Category : enlarged_ventricles, y_true: (262144,), y_score: (262144,), category_pxl_auc: 0.9309614906801913\n","Category : enlarged_ventricles\n","Best threshold:  0.6736095216296636\n","Best F1-Score:  0.6622155014636207\n","Best Img threshold:  0.455741206988632\n","Pixel AUROC:   0.930961\n","Pixel AUPRO:   0.726869\n","\n","(1, 1, 128, 128)\n","(1, 16384)\n","[[[[0.]]]]\n","[[[[1.]]]]\n","Category : intraventricular, y_true: (16384,), y_score: (16384,), category_pxl_auc: 0.9533004878713828\n","Category : intraventricular\n","Best threshold:  0.9500724803540093\n","Best F1-Score:  0.29942812981354927\n","Best Img threshold:  1.0\n","Pixel AUROC:   0.953300\n","Pixel AUPRO:   0.278350\n","\n","(1, 19, 128, 128)\n","(1, 311296)\n","[[[[0.]]]]\n","[[[[1.]]]]\n","Category : lesions, y_true: (311296,), y_score: (311296,), category_pxl_auc: 0.881616297974113\n","Category : lesions\n","Best threshold:  0.7253986419470512\n","Best F1-Score:  0.33316546945890485\n","Best Img threshold:  0.523750667582208\n","Pixel AUROC:   0.881616\n","Pixel AUPRO:   0.166248\n","\n","(1, 14, 128, 128)\n","(1, 229376)\n","[[[[0.]]]]\n","[[[[1.]]]]\n","Category : mass, y_true: (229376,), y_score: (229376,), category_pxl_auc: 0.9638250993650771\n","Category : mass\n","Best threshold:  0.6241397726405737\n","Best F1-Score:  0.5632513466448607\n","Best Img threshold:  0.35590142671854735\n","Pixel AUROC:   0.963825\n","Pixel AUPRO:   0.464130\n","\n","(1, 28, 128, 128)\n","(1, 458752)\n","[[[[0.]]]]\n","[[[[1.]]]]\n","Category : posttreatment, y_true: (458752,), y_score: (458752,), category_pxl_auc: 0.7534999708163913\n","Category : posttreatment\n","Best threshold:  0.5743190661478599\n","Best F1-Score:  0.12579868578974565\n","Best Img threshold:  0.481025406271458\n","Pixel AUROC:   0.753500\n","Pixel AUPRO:   0.086140\n","\n","(1, 8, 128, 128)\n","(1, 131072)\n","[[[[0.]]]]\n","[[[[1.]]]]\n","Category : resection, y_true: (131072,), y_score: (131072,), category_pxl_auc: 0.8891507959535502\n","Category : resection\n","Best threshold:  0.77778286411841\n","Best F1-Score:  0.33455352652231984\n","Best Img threshold:  0.6062867170214389\n","Pixel AUROC:   0.889151\n","Pixel AUPRO:   0.326424\n","\n","(1, 2, 128, 128)\n","(1, 32768)\n","[[[[0.]]]]\n","[[[[1.]]]]\n","Category : sinus, y_true: (32768,), y_score: (32768,), category_pxl_auc: 0.7334517051217918\n","Category : sinus\n","Best threshold:  0.7202716105897612\n","Best F1-Score:  0.023024693325014688\n","Best Img threshold:  0.781704432745861\n","Pixel AUROC:   0.733452\n","Pixel AUPRO:   0.006588\n","\n","(1, 3, 128, 128)\n","(1, 49152)\n","[[[[0.]]]]\n","[[[[1.]]]]\n","Category : wml, y_true: (49152,), y_score: (49152,), category_pxl_auc: 0.7315928697534035\n","Category : wml\n","Best threshold:  0.8355840390630961\n","Best F1-Score:  0.11752019011288446\n","Best Img threshold:  0.7401693751430534\n","Pixel AUROC:   0.731593\n","Pixel AUPRO:   0.053116\n","\n","(1, 5, 128, 128)\n","(1, 81920)\n","[[[[0.]]]]\n","[[[[1.]]]]\n","Category : other, y_true: (81920,), y_score: (81920,), category_pxl_auc: 0.9231683716997877\n","Category : other\n","Best threshold:  0.8349584191653315\n","Best F1-Score:  0.2654571445873748\n","Best Img threshold:  0.6606088349736782\n","Pixel AUROC:   0.923168\n","Pixel AUPRO:   0.168343\n","\n","(2457600,) {np_y_score_rocauc.shape}\n"]}]},{"cell_type":"code","source":["# category_list = ['absent_septum', 'artefacts', 'craniatomy', 'dural', 'ea_mass', 'edema', 'encephalomalacia', 'enlarged_ventricles', 'intraventricular', 'lesions', 'mass', 'posttreatment', 'resection', 'sinus', 'wml', 'other']\n","# values = [1, 16, 15, 7, 4, 18, 1, 19, 1, 22, 22, 44, 10, 2, 5, 5]\n","\n","# category_values_dict = dict(zip(category_list, values))\n","# category_values_dict\n","# \"\"\"\n","# {'absent_septum': 1,\n","#  'artefacts': 16,\n","#  'craniatomy': 15,\n","#  'dural': 7,\n","#  'ea_mass': 4,\n","#  'edema': 18,\n","#  'encephalomalacia': 1,\n","#  'enlarged_ventricles': 19,\n","#  'intraventricular': 1,\n","#  'lesions': 22,\n","#  'mass': 22,\n","#  'posttreatment': 44,\n","#  'resection': 10,\n","#  'sinus': 2,\n","#  'wml': 5,\n","#  'other': 5}\n","# \"\"\""],"metadata":{"id":"uFSP6EpjUN9w"},"execution_count":null,"outputs":[]}]}